[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2023.06.03

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#diffusion>diffusion</a></li>
    <li><a href=#text-generation>text generation</a></li>
  </ol>
</details>

## diffusion

|Publish Date|Title|Categories|Abstract|PDF|Code|
|---|---|---|---|---|
|**2023-06-01**|**Diffusion Self-Guidance for Controllable Image Generation**|['cs.CV', 'cs.LG', 'stat.ML']|Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images. For results and an interactive demo, see our project page at https://dave.ml/selfguidance/ |[2306.00986v1](http://arxiv.org/abs/2306.00986v1)|null|
|**2023-06-01**|**SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds**|['cs.CV', 'cs.AI', 'cs.LG']|Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers. However, these models are large, with complex network architectures and tens of denoising iterations, making them computationally expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run diffusion models at scale. This is costly and has privacy implications, especially when user data is sent to a third party. To overcome these challenges, we present a generic approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in less than $2$ seconds. We achieve so by introducing efficient network architecture and improving step distillation. Specifically, we propose an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, we enhance the step distillation by exploring training strategies and introducing regularization from classifier-free guidance. Our extensive experiments on MS-COCO show that our model with $8$ denoising steps achieves better FID and CLIP scores than Stable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creation by bringing powerful text-to-image diffusion models to the hands of users. |[2306.00980v1](http://arxiv.org/abs/2306.00980v1)|**[link](https://github.com/huggingface/diffusers)**|
|**2023-06-01**|**Intriguing Properties of Text-guided Diffusion Models**|['cs.CV']|Text-guided diffusion models (TDMs) are widely applied but can fail unexpectedly. Common failures include: (i) natural-looking text prompts generating images with the wrong content, or (ii) different random samples of the latent variables that generate vastly different, and even unrelated, outputs despite being conditioned on the same text prompt. In this work, we aim to study and understand the failure modes of TDMs in more detail. To achieve this, we propose SAGE, an adversarial attack on TDMs that uses image classifiers as surrogate loss functions, to search over the discrete prompt space and the high-dimensional latent space of TDMs to automatically discover unexpected behaviors and failure cases in the image generation. We make several technical contributions to ensure that SAGE finds failure cases of the diffusion model, rather than the classifier, and verify this in a human study. Our study reveals four intriguing properties of TDMs that have not been systematically studied before: (1) We find a variety of natural text prompts producing images that fail to capture the semantics of input texts. We categorize these failures into ten distinct types based on the underlying causes. (2) We find samples in the latent space (which are not outliers) that lead to distorted images independent of the text prompt, suggesting that parts of the latent space are not well-structured. (3) We also find latent samples that lead to natural-looking images which are unrelated to the text prompt, implying a potential misalignment between the latent and prompt spaces. (4) By appending a single adversarial token embedding to an input prompt we can generate a variety of specified target objects, while only minimally affecting the CLIP score. This demonstrates the fragility of language representations and raises potential safety concerns. |[2306.00974v1](http://arxiv.org/abs/2306.00974v1)|**[link](https://github.com/qihao067/sage)**|
|**2023-06-01**|**Intelligent Grimm -- Open-ended Visual Storytelling via Latent Diffusion Models**|['cs.CV']|Generative models have recently exhibited exceptional capabilities in various scenarios, for example, image generation based on text description. In this work, we focus on the task of generating a series of coherent image sequence based on a given storyline, denoted as open-ended visual storytelling. We make the following three contributions: (i) to fulfill the task of visual storytelling, we introduce two modules into a pre-trained stable diffusion model, and construct an auto-regressive image generator, termed as StoryGen, that enables to generate the current frame by conditioning on both a text prompt and a preceding frame; (ii) to train our proposed model, we collect paired image and text samples by sourcing from various online sources, such as videos, E-books, and establish a data processing pipeline for constructing a diverse dataset, named StorySalon, with a far larger vocabulary than existing animation-specific datasets; (iii) we adopt a three-stage curriculum training strategy, that enables style transfer, visual context conditioning, and human feedback alignment, respectively. Quantitative experiments and human evaluation have validated the superiority of our proposed model, in terms of image quality, style consistency, content consistency, and visual-language alignment. We will make the code, model, and dataset publicly available to the research community. |[2306.00973v1](http://arxiv.org/abs/2306.00973v1)|**[link](https://github.com/haoningwu3639/StoryGen)**|
|**2023-06-01**|**The Hidden Language of Diffusion Models**|['cs.CV']|Text-to-image diffusion models have demonstrated an unparalleled ability to generate high-quality, diverse images from a textual concept (e.g., "a doctor", "love"). However, the internal process of mapping text to a rich visual representation remains an enigma. In this work, we tackle the challenge of understanding concept representations in text-to-image models by decomposing an input text prompt into a small set of interpretable elements. This is achieved by learning a pseudo-token that is a sparse weighted combination of tokens from the model's vocabulary, with the objective of reconstructing the images generated for the given concept. Applied over the state-of-the-art Stable Diffusion model, this decomposition reveals non-trivial and surprising structures in the representations of concepts. For example, we find that some concepts such as "a president" or "a composer" are dominated by specific instances (e.g., "Obama", "Biden") and their interpolations. Other concepts, such as "happiness" combine associated terms that can be concrete ("family", "laughter") or abstract ("friendship", "emotion"). In addition to peering into the inner workings of Stable Diffusion, our method also enables applications such as single-image decomposition to tokens, bias detection and mitigation, and semantic image manipulation. Our code will be available at: https://hila-chefer.github.io/Conceptor/ |[2306.00966v1](http://arxiv.org/abs/2306.00966v1)|**[link](https://github.com/hila-chefer/Conceptor)**|
|**2023-06-01**|**Differential Diffusion: Giving Each Pixel Its Strength**|['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'I.3.3']|Text-based image editing has advanced significantly in recent years. With the rise of diffusion models, image editing via textual instructions has become ubiquitous. Unfortunately, current models lack the ability to customize the quantity of the change per pixel or per image fragment, resorting to changing the entire image in an equal amount, or editing a specific region using a binary mask. In this paper, we suggest a new framework which enables the user to customize the quantity of change for each image fragment, thereby enhancing the flexibility and verbosity of modern diffusion models. Our framework does not require model training or fine-tuning, but instead performs everything at inference time, making it easily applicable to an existing model. We show both qualitatively and quantitatively that our method allows better controllability and can produce results which are unattainable by existing models. Our code is available at: https://github.com/exx8/differential-diffusion |[2306.00950v1](http://arxiv.org/abs/2306.00950v1)|null|
|**2023-06-01**|**Inserting Anybody in Diffusion Models via Celeb Basis**|['cs.CV']|Exquisite demand exists for customizing the pretrained large text-to-image model, $\textit{e.g.}$, Stable Diffusion, to generate innovative concepts, such as the users themselves. However, the newly-added concept from previous customization methods often shows weaker combination abilities than the original ones even given several images during training. We thus propose a new personalization method that allows for the seamless integration of a unique individual into the pre-trained diffusion model using just $\textbf{one facial photograph}$ and only $\textbf{1024 learnable parameters}$ under $\textbf{3 minutes}$. So as we can effortlessly generate stunning images of this person in any pose or position, interacting with anyone and doing anything imaginable from text prompts. To achieve this, we first analyze and build a well-defined celeb basis from the embedding space of the pre-trained large text encoder. Then, given one facial photo as the target identity, we generate its own embedding by optimizing the weight of this basis and locking all other parameters. Empowered by the proposed celeb basis, the new identity in our customized model showcases a better concept combination ability than previous personalization methods. Besides, our model can also learn several new identities at once and interact with each other where the previous customization model fails to. The code will be released. |[2306.00926v1](http://arxiv.org/abs/2306.00926v1)|**[link](https://github.com/ygtxr1997/celebbasis)**|
|**2023-06-01**|**Conditioning Diffusion Models via Attributes and Semantic Masks for Face Generation**|['cs.CV']|Deep generative models have shown impressive results in generating realistic images of faces. GANs managed to generate high-quality, high-fidelity images when conditioned on semantic masks, but they still lack the ability to diversify their output. Diffusion models partially solve this problem and are able to generate diverse samples given the same condition. In this paper, we propose a multi-conditioning approach for diffusion models via cross-attention exploiting both attributes and semantic masks to generate high-quality and controllable face images. We also studied the impact of applying perceptual-focused loss weighting into the latent space instead of the pixel space. Our method extends the previous approaches by introducing conditioning on more than one set of features, guaranteeing a more fine-grained control over the generated face images. We evaluate our approach on the CelebA-HQ dataset, and we show that it can generate realistic and diverse samples while allowing for fine-grained control over multiple attributes and semantic regions. Additionally, we perform an ablation study to evaluate the impact of different conditioning strategies on the quality and diversity of the generated images. |[2306.00914v1](http://arxiv.org/abs/2306.00914v1)|null|
|**2023-06-01**|**Spatio-Angular Convolutions for Super-resolution in Diffusion MRI**|['eess.IV', 'cs.CV']|Diffusion MRI (dMRI) is a widely used imaging modality, but requires long scanning times to acquire high resolution datasets. By leveraging the unique geometry present within this domain, we present a novel approach to dMRI angular super-resolution that extends upon the parametric continuous convolution (PCConv) framework. We introduce several additions to the operation including a Fourier feature mapping, global coordinates, and domain specific context. Using this framework, we build a fully parametric continuous convolution network (PCCNN) and compare against existing models. We demonstrate the PCCNN performs competitively while using significantly less parameters. Moreover, we show that this formulation generalises well to clinically relevant downstream analyses such as fixel-based analysis, and neurite orientation dispersion and density imaging. |[2306.00854v1](http://arxiv.org/abs/2306.00854v1)|null|
|**2023-06-01**|**Diffuse Sources, Clustering and the Excess Anisotropy of the Radio Synchrotron Background**|['astro-ph.CO', 'astro-ph.IM']|We present the largest low frequency (120~MHz) arcminute resolution image of the radio synchrotron background (RSB) to date, and its corresponding angular power spectrum of anisotropies (APS) with angular scales ranging from $3^\circ$ to $0.3^\prime$. We show that the RSB around the North Celestial Pole has a significant excess anisotropy power at all scales over a model of unclustered point sources based on source counts of known source classes. This anisotropy excess, which does not seem attributable to the diffuse Galactic emission, could be linked to the surface brightness excess of the RSB. To better understand the information contained within the measured APS, we model the RSB varying the brightness distribution, size, and angular clustering of potential sources. We show that the observed APS could be produced by a population of faint clustered point sources only if the clustering is extreme and the size of the Gaussian clusters is $\lesssim 1'$. We also show that the observed APS could be produced by a population of faint diffuse sources with sizes $\lesssim 1'$, and this is supported by features present in our image. Both of these cases would also cause an associated surface brightness excess. These classes of sources are in a parameter space not well probed by even the deepest radio surveys to date. |[2306.00829v1](http://arxiv.org/abs/2306.00829v1)|null|
|**2023-06-01**|**FDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models**|['cs.CV']|The ability to create high-quality 3D faces from a single image has become increasingly important with wide applications in video conferencing, AR/VR, and advanced video editing in movie industries. In this paper, we propose Face Diffusion NeRF (FDNeRF), a new generative method to reconstruct high-quality Face NeRFs from single images, complete with semantic editing and relighting capabilities. FDNeRF utilizes high-resolution 3D GAN inversion and expertly trained 2D latent-diffusion model, allowing users to manipulate and construct Face NeRFs in zero-shot learning without the need for explicit 3D data. With carefully designed illumination and identity preserving loss, as well as multi-modal pre-training, FD-NeRF offers users unparalleled control over the editing process enabling them to create and edit face NeRFs using just single-view images, text prompts, and explicit target lighting. The advanced features of FDNeRF have been designed to produce more impressive results than existing 2D editing approaches that rely on 2D segmentation maps for editable attributes. Experiments show that our FDNeRF achieves exceptionally realistic results and unprecedented flexibility in editing compared with state-of-the-art 3D face reconstruction and editing methods. Our code will be available at https://github.com/BillyXYB/FDNeRF. |[2306.00783v1](http://arxiv.org/abs/2306.00783v1)|**[link](https://github.com/billyxyb/fdnerf)**|
|**2023-06-01**|**Inference and Sampling of Point Processes from Diffusion Excursions**|['stat.CO', 'stat.ME', 'stat.ML']|Point processes often have a natural interpretation with respect to a continuous process. We propose a point process construction that describes arrival time observations in terms of the state of a latent diffusion process. In this framework, we relate the return times of a diffusion in a continuous path space to new arrivals of the point process. This leads to a continuous sample path that is used to describe the underlying mechanism generating the arrival distribution. These models arise in many disciplines, such as financial settings where actions in a market are determined by a hidden continuous price or in neuroscience where a latent stimulus generates spike trains. Based on the developments in It\^o's excursion theory, we propose methods for inferring and sampling from the point process derived from the latent diffusion process. We illustrate the approach with numerical examples using both simulated and real data. The proposed methods and framework provide a basis for interpreting point processes through the lens of diffusions. |[2306.00762v1](http://arxiv.org/abs/2306.00762v1)|null|
|**2023-06-01**|**UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model**|['cs.SD', 'cs.AI', 'eess.AS']|This paper introduces UnDiff, a diffusion probabilistic model capable of solving various speech inverse tasks. Being once trained for speech waveform generation in an unconditional manner, it can be adapted to different tasks including degradation inversion, neural vocoding, and source separation. In this paper, we, first, tackle the challenging problem of unconditional waveform generation by comparing different neural architectures and preconditioning domains. After that, we demonstrate how the trained unconditional diffusion could be adapted to different tasks of speech processing by the means of recent developments in post-training conditioning of diffusion models. Finally, we demonstrate the performance of the proposed technique on the tasks of bandwidth extension, declipping, vocoding, and speech source separation and compare it to the baselines. The codes will be released soon. |[2306.00721v1](http://arxiv.org/abs/2306.00721v1)|null|
|**2023-06-01**|**Dissecting Arbitrary-scale Super-resolution Capability from Pre-trained Diffusion Generative Models**|['cs.CV', 'cs.LG', 'eess.IV']|Diffusion-based Generative Models (DGMs) have achieved unparalleled performance in synthesizing high-quality visual content, opening up the opportunity to improve image super-resolution (SR) tasks. Recent solutions for these tasks often train architecture-specific DGMs from scratch, or require iterative fine-tuning and distillation on pre-trained DGMs, both of which take considerable time and hardware investments. More seriously, since the DGMs are established with a discrete pre-defined upsampling scale, they cannot well match the emerging requirements of arbitrary-scale super-resolution (ASSR), where a unified model adapts to arbitrary upsampling scales, instead of preparing a series of distinct models for each case. These limitations beg an intriguing question: can we identify the ASSR capability of existing pre-trained DGMs without the need for distillation or fine-tuning? In this paper, we take a step towards resolving this matter by proposing Diff-SR, a first ASSR attempt based solely on pre-trained DGMs, without additional training efforts. It is motivated by an exciting finding that a simple methodology, which first injects a specific amount of noise into the low-resolution images before invoking a DGM's backward diffusion process, outperforms current leading solutions. The key insight is determining a suitable amount of noise to inject, i.e., small amounts lead to poor low-level fidelity, while over-large amounts degrade the high-level signature. Through a finely-grained theoretical analysis, we propose the Perceptual Recoverable Field (PRF), a metric that achieves the optimal trade-off between these two factors. Extensive experiments verify the effectiveness, flexibility, and adaptability of Diff-SR, demonstrating superior performance to state-of-the-art solutions under diverse ASSR environments. |[2306.00714v1](http://arxiv.org/abs/2306.00714v1)|null|
|**2023-06-01**|**Traveling Wave in a Ratio-dependent Holling-Tanner System with Nonlocal Diffusion and Strong Allee Effect**|['math.AP', 'math.DS']|In this paper, a ratio-dependent Holling-Tanner system with nonlocal diffusion is taken into account, where the prey is subject to a strong Allee effect. To be special, by applying Schauder's fixed point theorem and iterative technique, we provide a general theory on the existence of traveling waves for such system. Then appropriate upper and lower solutions and a novel sequence, similar to squeeze method, are constructed to demonstrate the existence of traveling waves for c>c*. Moreover, the existence of traveling wave for c=c* is also established by spreading speed theory and comparison principle. Finally, the nonexistence of traveling waves for c<c* is investigated, and the minimal wave speed then is determined. |[2306.00666v1](http://arxiv.org/abs/2306.00666v1)|null|
|**2023-06-01**|**EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis**|['cs.SD', 'eess.AS']|There has been significant progress in emotional Text-To-Speech (TTS) synthesis technology in recent years. However, existing methods primarily focus on the synthesis of a limited number of emotion types and have achieved unsatisfactory performance in intensity control. To address these limitations, we propose EmoMix, which can generate emotional speech with specified intensity or a mixture of emotions. Specifically, EmoMix is a controllable emotional TTS model based on a diffusion probabilistic model and a pre-trained speech emotion recognition (SER) model used to extract emotion embedding. Mixed emotion synthesis is achieved by combining the noises predicted by diffusion model conditioned on different emotions during only one sampling process at the run-time. We further apply the Neutral and specific primary emotion mixed in varying degrees to control intensity. Experimental results validate the effectiveness of EmoMix for synthesizing mixed emotion and intensity control. |[2306.00648v1](http://arxiv.org/abs/2306.00648v1)|null|
|**2023-06-01**|**General SIS diffusion process with indirect spreading pathways on a hypergraph**|['eess.SY', 'cs.SY', '05C65, 34D05, 34C12, 37N25, 92D30']|While conventional graphs only characterize pairwise interactions, higher-order networks (hypergraph, simplicial complex) capture multi-body interactions, which is a potentially more suitable modeling framework for a complex real system. However, the introduction of higher-order interactions brings new challenges for the rigorous analysis of such systems on a higher-order network. In this paper, we study a series of SIS-type diffusion processes with both indirect and direct pathways on a directed hypergraph. In a concrete case, the model we propose is based on a specific choice (polynomial) of interaction function (how several agents influence each other when they are in a hyperedge). Then, by the same choice of interaction function, we further extend the system and propose a bi-virus competing model on a directed hypergraph by coupling two single-virus models together. Finally, the most general model in this paper considers an abstract interaction function under single-virus and bi-virus settings. For the single-virus model, we provide the results regarding healthy state and endemic equilibrium. For the bi-virus setting, we further give an analysis of the existence and stability of the healthy state, dominant endemic equilibria, and coexisting equilibria. All theoretical results are finally supported by some numerical examples. |[2306.00619v1](http://arxiv.org/abs/2306.00619v1)|null|
|**2023-06-01**|**DiffRoom: Diffusion-based High-Quality 3D Room Reconstruction and Generation**|['cs.CV']|We present DiffRoom, a novel framework for tackling the problem of high-quality 3D indoor room reconstruction and generation, both of which are challenging due to the complexity and diversity of the room geometry. Although diffusion-based generative models have previously demonstrated impressive performance in image generation and object-level 3D generation, they have not yet been applied to room-level 3D generation due to their computationally intensive costs. In DiffRoom, we propose a sparse 3D diffusion network that is efficient and possesses strong generative performance for Truncated Signed Distance Field (TSDF), based on a rough occupancy prior. Inspired by KinectFusion's incremental alignment and fusion of local SDFs, we propose a diffusion-based TSDF fusion approach that iteratively diffuses and fuses TSDFs, facilitating the reconstruction and generation of an entire room environment. Additionally, to ease training, we introduce a curriculum diffusion learning paradigm that speeds up the training convergence process and enables high-quality reconstruction. According to the user study, the mesh quality generated by our DiffRoom can even outperform the ground truth mesh provided by ScanNet. |[2306.00519v1](http://arxiv.org/abs/2306.00519v1)|null|
|**2023-06-01**|**Image generation with shortest path diffusion**|['cs.CV', 'cs.AI', 'cs.LG']|The field of image generation has made significant progress thanks to the introduction of Diffusion Models, which learn to progressively reverse a given image corruption. Recently, a few studies introduced alternative ways of corrupting images in Diffusion Models, with an emphasis on blurring. However, these studies are purely empirical and it remains unclear what is the optimal procedure for corrupting an image. In this work, we hypothesize that the optimal procedure minimizes the length of the path taken when corrupting an image towards a given final state. We propose the Fisher metric for the path length, measured in the space of probability distributions. We compute the shortest path according to this metric, and we show that it corresponds to a combination of image sharpening, rather than blurring, and noise deblurring. While the corruption was chosen arbitrarily in previous work, our Shortest Path Diffusion (SPD) determines uniquely the entire spatiotemporal structure of the corruption. We show that SPD improves on strong baselines without any hyperparameter tuning, and outperforms all previous Diffusion Models based on image blurring. Furthermore, any small deviation from the shortest path leads to worse performance, suggesting that SPD provides the optimal procedure to corrupt images. Our work sheds new light on observations made in recent works and provides a new approach to improve diffusion models on images and other types of data. |[2306.00501v1](http://arxiv.org/abs/2306.00501v1)|null|
|**2023-06-01**|**Reconstructing Graph Diffusion History from a Single Snapshot**|['cs.LG', 'cs.SI']|Diffusion on graphs is ubiquitous with numerous high-impact applications. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot} (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) estimation error of diffusion parameters is unavoidable due to NP-hardness of diffusion parameter estimation, and (b) the MLE formulation is sensitive to estimation error of diffusion parameters. To overcome the inherent limitation of the MLE formulation, we propose a novel barycenter formulation: finding the barycenter of the posterior distribution of histories, which is provably stable against the estimation error of diffusion parameters. We further develop an effective solver named DIffusion hiTting Times with Optimal proposal (DITTO) by reducing the problem to estimating posterior expected hitting times via the Metropolis--Hastings Markov chain Monte Carlo method (M--H MCMC) and employing an unsupervised graph neural network to learn an optimal proposal to accelerate the convergence of M--H MCMC. We conduct extensive experiments to demonstrate the efficacy of the proposed method. |[2306.00488v1](http://arxiv.org/abs/2306.00488v1)|**[link](https://github.com/q-rz/kdd23-ditto)**|
|**2023-06-01**|**Random advection-diffusion models and their statistics**|['cond-mat.stat-mech', 'nlin.CD']|We study the statistics of a one-dimensional randomly advected field with diffusion. The motivation for this setup comes from a straightforward interpretation as advection of particles in one-dimensional turbulence, but it is also related to a problem of synchronization of dynamical systems driven by common noise. A general class of lattice models describing the joint effect of random advection and diffusion for an ensemble of particles is introduced. It consists of a general microscopic random update rule and encompasses as specific cases, some models studied in the literature, like the Kang-Redner, Kipnis-Marchioro-Presutti, Takatasu-Taguchi etc. For finite lattices, we study both the coagulation of an initially spread field (interpreted as roughening), and the statistical steady-state properties. We distinguish two main size-dependent regimes, depending on the strength of the advection term and on the lattice size. Using numerical simulations and mean-field approach, we study the statistics of the field. For weak diffusion, we unveil a characteristic hierarchical structure of the field. We also connect the model and the iterated function systems concept |[2306.00463v1](http://arxiv.org/abs/2306.00463v1)|null|
|**2023-06-01**|**Provably stable numerical method for the anisotropic diffusion equation in toroidally confined magnetic fields**|['math.NA', 'cs.NA', '65M06, 65M12', 'G.1.8']|We present a novel numerical method for solving the anisotropic diffusion equation in toroidally confined magnetic fields which is efficient, accurate and provably stable. The continuous problem is written in terms of a derivative operator for the perpendicular transport and a linear operator, obtained through field line tracing, for the parallel transport. We derive energy estimates of the solution of the continuous initial boundary value problem. A discrete formulation is presented using operator splitting in time with the summation by parts finite difference approximation of spatial derivatives for the perpendicular diffusion operator. Weak penalty procedures are derived for implementing both boundary conditions and parallel diffusion operator obtained by field line tracing. We prove that the fully-discrete approximation is unconditionally stable and asymptotic preserving. Discrete energy estimates are shown to match the continuous energy estimate given the correct choice of penalty parameters. Convergence tests are shown for the perpendicular operator by itself, and the ``NIMROD benchmark" problem is used as a manufactured solution to show the full scheme converges even in the case where the perpendicular diffusion is zero. Finally, we present a magnetic field with chaotic regions and islands and show the contours of the anisotropic diffusion equation reproduce key features in the field. |[2306.00423v1](http://arxiv.org/abs/2306.00423v1)|null|
|**2023-06-01**|**Controllable Motion Diffusion Model**|['cs.CV', 'cs.AI', 'cs.GR']|Generating realistic and controllable motions for virtual characters is a challenging task in computer animation, and its implications extend to games, simulations, and virtual reality. Recent studies have drawn inspiration from the success of diffusion models in image generation, demonstrating the potential for addressing this task. However, the majority of these studies have been limited to offline applications that target at sequence-level generation that generates all steps simultaneously. To enable real-time motion synthesis with diffusion models in response to time-varying control signals, we propose the framework of the Controllable Motion Diffusion Model (COMODO). Our framework begins with an auto-regressive motion diffusion model (A-MDM), which generates motion sequences step by step. In this way, simply using the standard DDPM algorithm without any additional complexity, our framework is able to generate high-fidelity motion sequences over extended periods with different types of control signals. Then, we propose our reinforcement learning-based controller and controlling strategies on top of the A-MDM model, so that our framework can steer the motion synthesis process across multiple tasks, including target reaching, joystick-based control, goal-oriented control, and trajectory following. The proposed framework enables the real-time generation of diverse motions that react adaptively to user commands on-the-fly, thereby enhancing the overall user experience. Besides, it is compatible with the inpainting-based editing methods and can predict much more diverse motions without additional fine-tuning of the basic motion generation models. We conduct comprehensive experiments to evaluate the effectiveness of our framework in performing various tasks and compare its performance against state-of-the-art methods. |[2306.00416v1](http://arxiv.org/abs/2306.00416v1)|null|
|**2023-06-01**|**On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diffusion Models, and Fokker-Planck Regularization**|['math.ST', 'cs.AI', 'cs.LG', 'stat.TH']|The emergence of various notions of ``consistency'' in diffusion models has garnered considerable attention and helped achieve improved sample quality, likelihood estimation, and accelerated sampling. Although similar concepts have been proposed in the literature, the precise relationships among them remain unclear. In this study, we establish theoretical connections between three recent ``consistency'' notions designed to enhance diffusion models for distinct objectives. Our insights offer the potential for a more comprehensive and encompassing framework for consistency-type models. |[2306.00367v1](http://arxiv.org/abs/2306.00367v1)|null|
|**2023-06-01**|**Addressing Negative Transfer in Diffusion Models**|['cs.CV', 'cs.AI', 'cs.LG']|Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL methods, but the presence of a huge number of denoising tasks makes this computationally expensive to calculate the necessary per-task loss or gradient. To address this challenge, we propose clustering the denoising tasks into small task clusters and applying MTL methods to them. Specifically, based on $\textbf{(O2)}$, we employ interval clustering to enforce temporal proximity among denoising tasks within clusters. We show that interval clustering can be solved with dynamic programming and utilize signal-to-noise ratio, timestep, and task affinity for clustering objectives. Through this, our approach addresses the issue of negative transfer in diffusion models by allowing for efficient computation of MTL methods. We validate the proposed clustering and its integration with MTL methods through various experiments, demonstrating improved sample quality of diffusion models. |[2306.00354v1](http://arxiv.org/abs/2306.00354v1)|null|
|**2023-06-01**|**Low-Light Image Enhancement with Wavelet-based Diffusion Models**|['cs.CV']|Diffusion models have achieved promising results in image restoration tasks, yet suffer from time-consuming, excessive computational resource consumption, and unstable restoration. To address these issues, we propose a robust and efficient Diffusion-based Low-Light image enhancement approach, dubbed DiffLL. Specifically, we present a wavelet-based conditional diffusion model (WCDM) that leverages the generative power of diffusion models to produce results with satisfactory perceptual fidelity. Additionally, it also takes advantage of the strengths of wavelet transformation to greatly accelerate inference and reduce computational resource usage without sacrificing information. To avoid chaotic content and diversity, we perform both forward diffusion and reverse denoising in the training phase of WCDM, enabling the model to achieve stable denoising and reduce randomness during inference. Moreover, we further design a high-frequency restoration module (HFRM) that utilizes the vertical and horizontal details of the image to complement the diagonal information for better fine-grained restoration. Extensive experiments on publicly available real-world benchmarks demonstrate that our method outperforms the existing state-of-the-art methods both quantitatively and visually, and it achieves remarkable improvements in efficiency compared to previous diffusion-based methods. In addition, we empirically show that the application for low-light face detection also reveals the latent practical values of our method. |[2306.00306v1](http://arxiv.org/abs/2306.00306v1)|null|
|**2023-06-01**|**The origin of MeV gamma-ray diffuse emission from the inner Galactic region**|['astro-ph.HE']|The origin of the inner Galactic emission, measured by COMPTEL with a flux of $\sim ~ 10^{-2}$ MeV cm$^{-2}$ s$^{-1}$ sr$^{-1}$ in the 1-30 MeV range, has remained unsettled since its discovery in 1994. We investigate the origin of this emission by taking into account individual sources which are not resolved by COMPTEL and the Galactic diffuse emission. The source contribution is estimated for sources crossmatched between the Swift-BAT and Fermi-LAT catalogs by interpolating the energy spectra in the hard X-ray and GeV gamma-ray ranges, as well as unmatched sources. This results in a flux of $\sim$20% of the COMPTEL excess. The Galactic diffuse emission is calculated by GALPROP to reconcile the cosmic-ray and gamma-ray spectra with observations by AMS-02, Voyager, and Fermi-LAT, resulting in a flux of $\sim$30-80% of the COMPTEL emission. Thus, we show that the COMPTEL emission could be roughly reproduced by a combination of the sources and the Galactic diffuse emission. Furthermore, combined with the extragalactic emission, we construct all-sky images in the MeV gamma-ray range to pinpoint some potential interesting targets for future missions, which would be critical for bridging the MeV gap in the spectra of gamma-ray sources. |[2306.00290v1](http://arxiv.org/abs/2306.00290v1)|null|
|**2023-05-31**|**Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images**|['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']|Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpainting techniques and editing software for fine-tuning AI-generated imagery. |[2306.00219v1](http://arxiv.org/abs/2306.00219v1)|null|
|**2023-05-31**|**Diffused Redundancy in Pre-trained Representations**|['cs.LG', 'cs.AI']|Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstream tasks taken from the VTAB benchmark. We find that the loss & dataset used during pre-training largely govern the degree of diffuse redundancy and the "critical mass" of neurons needed often depends on the downstream task, suggesting that there is a task-inherent redundancy-performance Pareto frontier. Our findings shed light on the nature of representations learned by pre-trained deep neural networks and suggest that entire layers might not be necessary to perform many downstream tasks. We investigate the potential for exploiting this redundancy to achieve efficient generalization for downstream tasks and also draw caution to certain possible unintended consequences. |[2306.00183v1](http://arxiv.org/abs/2306.00183v1)|null|
|**2023-05-31**|**eXtended Hybridizable Discontinous Galerkin (X-HDG) Method for Linear Convection-Diffusion Equations on Unfitted Domains**|['math.NA', 'cs.NA']|In this work, we propose a novel strategy for the numerical solution of linear convection diffusion equation (CDE) over unfitted domains. In the proposed numerical scheme, strategies from high order Hybridized Discontinuous Galerkin method and eXtended Finite Element method is combined with the level set definition of the boundaries. The proposed scheme and hence, is named as eXtended Hybridizable Discontinuous Galerkin (XHDG) method. In this regard, the Hybridizable Discontinuous Galerkin (HDG) method is eXtended to the unfitted domains; i.e, the computational mesh does not need to fit to the domain boundary; instead, the boundary is defined by a level set function and cuts through the background mesh arbitrarily. The original unknown structure of HDG and its hybrid nature ensuring the local conservation of fluxes is kept, while developing a modified bilinear form for the elements cut by the boundary. At every cut element, an auxiliary nodal trace variable on the boundary is introduced, which is eliminated afterwards while imposing the boundary conditions. Both stationary and time dependent CDEs are studied over a range of flow regimes from diffusion to convection dominated; using high order $(p \leq 4)$ XHDG through benchmark numerical examples over arbitrary unfitted domains. Results proved that XHDG inherits optimal $(p + 1)$ and super $(p + 2)$ convergence properties of HDG while removing the fitting mesh restriction. |[2306.00156v1](http://arxiv.org/abs/2306.00156v1)|null|
|**2023-05-31**|**SafeDiffuser: Safe Planning with Diffusion Probabilistic Models**|['cs.LG', 'cs.RO', 'cs.SY', 'eess.SY']|Diffusion model-based approaches have shown promise in data-driven planning, but there are no safety guarantees, thus making it hard to be applied for safety-critical applications. To address these challenges, we propose a new method, called SafeDiffuser, to ensure diffusion probabilistic models satisfy specifications by using a class of control barrier functions. The key idea of our approach is to embed the proposed finite-time diffusion invariance into the denoising diffusion procedure, which enables trustworthy diffusion data generation. Moreover, we demonstrate that our finite-time diffusion invariance method through generative models not only maintains generalization performance but also creates robustness in safe data generation. We test our method on a series of safe planning tasks, including maze path generation, legged robot locomotion, and 3D space manipulation, with results showing the advantages of robustness and guarantees over vanilla diffusion models. |[2306.00148v1](http://arxiv.org/abs/2306.00148v1)|null|
|**2023-05-31**|**Diffusive and Ballistic Transport in Ultra-thin InSb Nanowire Devices Using a Few-layer-Graphene-AlOx Gate**|['cond-mat.mes-hall', 'cond-mat.mtrl-sci']|Quantum devices based on InSb nanowires (NWs) are a prime candidate system for realizing and exploring topologically-protected quantum states and for electrically-controlled spin-based qubits. The influence of disorder on achieving reliable topological regimes has been studied theoretically, highlighting the importance of optimizing both growth and nanofabrication. In this work we investigate both aspects. We developed InSb nanowires with ultra-thin diameters, as well as a new gating approach, involving few-layer graphene (FLG) and Atomic Layer Deposition (ALD)-grown AlOx. Low-temperature electronic transport measurements of these devices reveal conductance plateaus and Fabry-P\'erot interference, evidencing phase-coherent transport in the regime of few quantum modes. The approaches developed in this work could help mitigate the role of material and fabrication-induced disorder in semiconductor-based quantum devices. |[2306.00117v1](http://arxiv.org/abs/2306.00117v1)|null|
|**2023-05-31**|**Diffuse Ultra-High-Energy Gamma-Ray Emission From TeV Halos**|['astro-ph.HE', 'astro-ph.CO', 'astro-ph.GA', 'hep-ph']|The LHAASO Collaboration has recently reported a measurement of the diffuse gamma-ray emission from the Galactic Plane at energies between 10 TeV and 1 PeV. While this emission is brighter than that expected from cosmic-ray interactions in the interstellar medium alone, we show that the intensity, spectrum, and morphology of this excess are in good agreement with that predicted from the "TeV halos" which surround the Milky Way's pulsar population. These results support the conclusion that TeV halos dominate the ultra-high-energy sky, and that these objects convert $\sim 5\%$ of their total spindown power into very-high and ultra-high-energy photons. |[2306.00051v1](http://arxiv.org/abs/2306.00051v1)|null|
|**2023-05-31**|**Understanding and Mitigating Copying in Diffusion Models**|['cs.LG', 'cs.CR', 'cs.CV']|Images generated by diffusion models like Stable Diffusion are increasingly widespread. Recent works and even lawsuits have shown that these models are prone to replicating their training data, unbeknownst to the user. In this paper, we first analyze this memorization problem in text-to-image diffusion models. While it is widely believed that duplicated images in the training set are responsible for content replication at inference time, we observe that the text conditioning of the model plays a similarly important role. In fact, we see in our experiments that data replication often does not happen for unconditional models, while it is common in the text-conditional case. Motivated by our findings, we then propose several techniques for reducing data replication at both training and inference time by randomizing and augmenting image captions in the training set. |[2305.20086v1](http://arxiv.org/abs/2305.20086v1)|**[link](https://github.com/somepago/dcr)**|
|**2023-05-31**|**Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D Diffusion-based Editor**|['cs.CV']|Recent years have witnessed considerable achievements in editing images with text instructions. When applying these editors to dynamic scene editing, the new-style scene tends to be temporally inconsistent due to the frame-by-frame nature of these 2D editors. To tackle this issue, we propose Control4D, a novel approach for high-fidelity and temporally consistent 4D portrait editing. Control4D is built upon an efficient 4D representation with a 2D diffusion-based editor. Instead of using direct supervisions from the editor, our method learns a 4D GAN from it and avoids the inconsistent supervision signals. Specifically, we employ a discriminator to learn the generation distribution based on the edited images and then update the generator with the discrimination signals. For more stable training, multi-level information is extracted from the edited images and used to facilitate the learning of the generator. Experimental results show that Control4D surpasses previous approaches and achieves more photo-realistic and consistent 4D editing performances. The link to our project website is https://control4darxiv.github.io. |[2305.20082v1](http://arxiv.org/abs/2305.20082v1)|null|
|**2023-05-31**|**Efficient Diffusion Policies for Offline Reinforcement Learning**|['cs.LG', 'cs.AI']|Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion policy training time from 5 days to 5 hours on gym-locomotion tasks. Moreover, we show that EDP is compatible with various offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on D4RL by large margins over previous methods. Our code is available at https://github.com/sail-sg/edp. |[2305.20081v1](http://arxiv.org/abs/2305.20081v1)|**[link](https://github.com/sail-sg/edp)**|
|**2023-05-31**|**A Unified Conditional Framework for Diffusion-based Image Restoration**|['cs.CV']|Diffusion Probabilistic Models (DPMs) have recently shown remarkable performance in image generation tasks, which are capable of generating highly realistic images. When adopting DPMs for image restoration tasks, the crucial aspect lies in how to integrate the conditional information to guide the DPMs to generate accurate and natural output, which has been largely overlooked in existing works. In this paper, we present a unified conditional framework based on diffusion models for image restoration. We leverage a lightweight UNet to predict initial guidance and the diffusion model to learn the residual of the guidance. By carefully designing the basic module and integration module for the diffusion model block, we integrate the guidance and other auxiliary conditional information into every block of the diffusion model to achieve spatially-adaptive generation conditioning. To handle high-resolution images, we propose a simple yet effective inter-step patch-splitting strategy to produce arbitrary-resolution images without grid artifacts. We evaluate our conditional framework on three challenging tasks: extreme low-light denoising, deblurring, and JPEG restoration, demonstrating its significant improvements in perceptual quality and the generalization to restoration tasks. |[2305.20049v1](http://arxiv.org/abs/2305.20049v1)|**[link](https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement)**|
|**2023-06-01**|**Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust**|['cs.LG', 'cs.CR', 'cs.CV']|Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed. Code is available at https://github.com/YuxinWenRick/tree-ring-watermark. |[2305.20030v2](http://arxiv.org/abs/2305.20030v2)|**[link](https://github.com/YuxinWenRick/tree-ring-watermark)**|
|**2023-05-31**|**Protein Design with Guided Discrete Diffusion**|['cs.LG', 'q-bio.BM']|A popular approach to protein design is to combine a generative model with a discriminative model for conditional sampling. The generative model samples plausible sequences while the discriminative model guides a search for sequences with high fitness. Given its broad success in conditional sampling, classifier-guided diffusion modeling is a promising foundation for protein design, leading many to develop guided diffusion models for structure with inverse folding to recover sequences. In this work, we propose diffusioN Optimized Sampling (NOS), a guidance method for discrete diffusion models that follows gradients in the hidden states of the denoising network. NOS makes it possible to perform design directly in sequence space, circumventing significant limitations of structure-based methods, including scarce data and challenging inverse design. Moreover, we use NOS to generalize LaMBO, a Bayesian optimization procedure for sequence design that facilitates multiple objectives and edit-based constraints. The resulting method, LaMBO-2, enables discrete diffusions and stronger performance with limited edits through a novel application of saliency maps. We apply LaMBO-2 to a real-world protein design task, optimizing antibodies for higher expression yield and binding affinity to a therapeutic target under locality and liability constraints, with 97% expression rate and 25% binding rate in exploratory in vitro experiments. |[2305.20009v1](http://arxiv.org/abs/2305.20009v1)|**[link](https://github.com/ngruver/nos)**|
|**2023-05-31**|**A Geometric Perspective on Diffusion Models**|['cs.CV', 'cs.LG', 'stat.ML']|Recent years have witnessed significant progress in developing efficient training and fast sampling approaches for diffusion models. A recent remarkable advancement is the use of stochastic differential equations (SDEs) to describe data perturbation and generative modeling in a unified mathematical framework. In this paper, we reveal several intriguing geometric structures of diffusion models and contribute a simple yet powerful interpretation to their sampling dynamics. Through carefully inspecting a popular variance-exploding SDE and its marginal-preserving ordinary differential equation (ODE) for sampling, we discover that the data distribution and the noise distribution are smoothly connected with an explicit, quasi-linear sampling trajectory, and another implicit denoising trajectory, which even converges faster in terms of visual quality. We also establish a theoretical relationship between the optimal ODE-based sampling and the classic mean-shift (mode-seeking) algorithm, with which we can characterize the asymptotic behavior of diffusion models and identify the score deviation. These new geometric observations enable us to improve previous sampling algorithms, re-examine latent interpolation, as well as re-explain the working principles of distillation-based fast sampling techniques. |[2305.19947v1](http://arxiv.org/abs/2305.19947v1)|null|
|**2023-05-31**|**MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL**|['cs.LG', 'cs.AI']|Recently, diffusion model shines as a promising backbone for the sequence modeling paradigm in offline reinforcement learning(RL). However, these works mostly lack the generalization ability across tasks with reward or dynamics change. To tackle this challenge, in this paper we propose a task-oriented conditioned diffusion planner for offline meta-RL(MetaDiffuser), which considers the generalization problem as conditional trajectory generation task with contextual representation. The key is to learn a context conditioned diffusion model which can generate task-oriented trajectories for planning across diverse tasks. To enhance the dynamics consistency of the generated trajectories while encouraging trajectories to achieve high returns, we further design a dual-guided module in the sampling process of the diffusion model. The proposed framework enjoys the robustness to the quality of collected warm-start data from the testing task and the flexibility to incorporate with different task representation method. The experiment results on MuJoCo benchmarks show that MetaDiffuser outperforms other strong offline meta-RL baselines, demonstrating the outstanding conditional generation ability of diffusion architecture. |[2305.19923v1](http://arxiv.org/abs/2305.19923v1)|null|
|**2023-05-31**|**Unsupervised Anomaly Detection in Medical Images Using Masked Diffusion Model**|['eess.IV', 'cs.CV']|It can be challenging to identify brain MRI anomalies using supervised deep-learning techniques due to anatomical heterogeneity and the requirement for pixel-level labeling. Unsupervised anomaly detection approaches provide an alternative solution by relying only on sample-level labels of healthy brains to generate a desired representation to identify abnormalities at the pixel level. Although, generative models are crucial for generating such anatomically consistent representations of healthy brains, accurately generating the intricate anatomy of the human brain remains a challenge. In this study, we present a method called masked-DDPM (mDPPM), which introduces masking-based regularization to reframe the generation task of diffusion models. Specifically, we introduce Masked Image Modeling (MIM) and Masked Frequency Modeling (MFM) in our self-supervised approach that enables models to learn visual representations from unlabeled data. To the best of our knowledge, this is the first attempt to apply MFM in DPPM models for medical applications. We evaluate our approach on datasets containing tumors and numerous sclerosis lesions and exhibit the superior performance of our unsupervised method as compared to the existing fully/weakly supervised baselines. Code is available at https://github.com/hasan1292/mDDPM. |[2305.19867v1](http://arxiv.org/abs/2305.19867v1)|**[link](https://github.com/hasan1292/mddpm)**|
|**2023-05-31**|**Inverse-design of nonlinear mechanical metamaterials via video denoising diffusion models**|['cs.CE']|The accelerated inverse design of complex material properties - such as identifying a material with a given stress-strain response over a nonlinear deformation path - holds great potential for addressing challenges from soft robotics to biomedical implants and impact mitigation. While machine learning models have provided such inverse mappings, they are typically restricted to linear target properties such as stiffness. To tailor the nonlinear response, we here show that video diffusion generative models trained on full-field data of periodic stochastic cellular structures can successfully predict and tune their nonlinear deformation and stress response under compression in the large-strain regime, including buckling and contact. Unlike commonly encountered black-box models, our framework intrinsically provides an estimate of the expected deformation path, including the full-field internal stress distribution closely agreeing with finite element simulations. This work has thus the potential to simplify and accelerate the identification of materials with complex target performance. |[2305.19836v1](http://arxiv.org/abs/2305.19836v1)|**[link](https://github.com/jhbastek/videometamaterials)**|
|**2023-05-31**|**Direct Diffusion Bridge using Data Consistency for Inverse Problems**|['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']|Diffusion model-based inverse problem solvers have shown impressive performance, but are limited in speed, mostly as they require reverse diffusion sampling starting from noise. Several recent works have tried to alleviate this problem by building a diffusion process, directly bridging the clean and the corrupted for specific inverse problems. In this paper, we first unify these existing works under the name Direct Diffusion Bridges (DDB), showing that while motivated by different theories, the resulting algorithms only differ in the choice of parameters. Then, we highlight a critical limitation of the current DDB framework, namely that it does not ensure data consistency. To address this problem, we propose a modified inference procedure that imposes data consistency without the need for fine-tuning. We term the resulting method data Consistent DDB (CDDB), which outperforms its inconsistent counterpart in terms of both perception and distortion metrics, thereby effectively pushing the Pareto-frontier toward the optimum. Our proposed method achieves state-of-the-art results on both evaluation criteria, showcasing its superiority over existing methods. |[2305.19809v1](http://arxiv.org/abs/2305.19809v1)|null|
|**2023-05-31**|**A hybridizable discontinuous Galerkin method for magnetic advection-diffusion problems**|['math.NA', 'cs.NA', '65N30, 65N12']|We propose and analyze a hybridizable discontinuous Galerkin (HDG) method for solving a mixed magnetic advection-diffusion problem within a more general Friedrichs system framework. With carefully constructed numerical traces, we introduce two distinct stabilization parameters: $\tau_t$ for the tangential trace and $\tau_n$ for the normal trace. These parameters are tailored to satisfy different requirements, ensuring the stability and convergence of the method. Furthermore, we incorporate a weight function to facilitate the establishment of stability conditions. We also investigate an elementwise postprocessing technique that proves to be effective for both two-dimensional and three-dimensional problems in terms of broken $H({\rm curl})$ semi-norm accuracy improvement. Extensive numerical examples are presented to showcase the performance and effectiveness of the HDG method and the postprocessing techniques. |[2305.19806v1](http://arxiv.org/abs/2305.19806v1)|null|
|**2023-05-30**|**RINGER: Rapid Conformer Generation for Macrocycles with Sequence-Conditioned Internal Coordinate Diffusion**|['q-bio.BM', 'cs.LG']|Macrocyclic peptides are an emerging therapeutic modality, yet computational approaches for accurately sampling their diverse 3D ensembles remain challenging due to their conformational diversity and geometric constraints. Here, we introduce RINGER, a diffusion-based transformer model for sequence-conditioned generation of macrocycle structures based on internal coordinates. RINGER provides fast backbone sampling while respecting key structural invariances of cyclic peptides. Through extensive benchmarking and analysis against gold-standard conformer ensembles of cyclic peptides generated with metadynamics, we demonstrate how RINGER generates both high-quality and diverse geometries at a fraction of the computational cost. Our work lays the foundation for improved sampling of cyclic geometries and the development of geometric learning methods for peptides. |[2305.19800v1](http://arxiv.org/abs/2305.19800v1)|**[link](https://github.com/genentech/ringer)**|
|**2023-05-31**|**Existence and Stability of Random Transition Waves for Nonautonomous Fisher-KPP Equations with Nonlocal Diffusion**|['math.AP', 'math.DS', '35K55, 35C07, 35B35, 45G10, 92D25']|In this paper, we study the existence and stability of random transition waves for time heterogeneous Fisher-KPP Equations with nonlocal diffusion. More specifically, we consider general time heterogeneities both for the nonlocal diffusion kernel and the reaction term. We use the comparison principle of the scalar equation and the method of upper and lower solutions to investigate the existence of random transition wave solution when the wave speed is large enough. In addition, we show the stability of random transition fronts for non-autonomous Fisher-KPP equations with nonlocal diffusion. |[2305.19762v1](http://arxiv.org/abs/2305.19762v1)|null|
|**2023-05-31**|**Multiplexed wavefront sensing with a thin diffuser**|['physics.optics', 'physics.ins-det']|In astronomy or biological imaging, refractive index inhomogeneities of e.g. atmosphere or tissues induce optical aberrations which degrade the desired information hidden behind the medium. A standard approach consists in measuring these aberrations with a wavefront sensor (e.g Shack-Hartmann) located in the pupil plane, and compensating them either digitally or by adaptive optics with a wavefront shaper. However, in its usual implementation this strategy can only extract aberrations within a single isoplanatic patch, i.e. a region where the aberrations remain correlated. This limitation severely reduces the effective field-of-view in which the correction can be performed. Here, we propose a new wavefront sensing method capable of measuring, in a single shot, various pupil aberrations corresponding to multiple isoplanatic patches. The method, based on a thin diffuser (i.e a random phase mask), exploits the dissimilarity between different speckle regions to multiplex several wavefronts incoming from various incidence angles. We present proof-of-concept experiments carried out in wide-field fluorescence microscopy. A digital deconvolution procedure in each isoplanatic patch yields accurate aberration correction within an extended field-of-view. This approach is of interest for adaptive optics applications as well as diffractive optical tomography. |[2305.19758v1](http://arxiv.org/abs/2305.19758v1)|null|
|**2023-05-31**|**Transportation cost inequalities for stochastic reaction diffusion equations on the whole line $\mathbb{R}$**|['math.PR', '60H15 (Primary) 35R60 (Secondary)']|In this paper, we established quadratic transportation cost inequalities for solutions of stochastic reaction diffusion equations driven by multiplicative space-time white noise on the whole line $\mathbb{R}$. Since the space variable is defined on the unbounded domain $\mathbb{R}$, the inequalities are proved under a weighted $L^2$-norm and a weighted uniform metric in the so called $L^2_{tem}$, $C_{tem}$ spaces. The new moments estimates of the stochastic convolution with respect to space-time white noise play an important role. In addition, the transportation cost inequalities are also obtained for the stochastic reaction diffusion equations with random initial values. |[2305.19739v1](http://arxiv.org/abs/2305.19739v1)|null|
|**2023-06-01**|**Spontaneous symmetry breaking in generative diffusion models**|['cs.LG', 'cs.CV']|Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking that divides the generative dynamics into two distinct phases: 1) A linear steady-state dynamics around a central fixed-point and 2) an attractor dynamics directed towards the data manifold. These two "phases" are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3x FID improvements on fast samplers, while also increasing sample diversity (e.g., racial composition of generated CelebA images). Our work offers a new way to understand the generative dynamics of diffusion models that has the potential to bring about higher performance and less biased fast-samplers. |[2305.19693v2](http://arxiv.org/abs/2305.19693v2)|**[link](https://github.com/gabrielraya/symmetry_breaking_diffusion_models)**|
|**2023-05-31**|**Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models**|['cs.CV', 'cs.AI', 'eess.IV']|The introduction of diffusion models in anomaly detection has paved the way for more effective and accurate image reconstruction in pathologies. However, the current limitations in controlling noise granularity hinder diffusion models' ability to generalize across diverse anomaly types and compromise the restoration of healthy tissues. To overcome these challenges, we propose AutoDDPM, a novel approach that enhances the robustness of diffusion models. AutoDDPM utilizes diffusion models to generate initial likelihood maps of potential anomalies and seamlessly integrates them with the original image. Through joint noised distribution re-sampling, AutoDDPM achieves harmonization and in-painting effects. Our study demonstrates the efficacy of AutoDDPM in replacing anomalous regions while preserving healthy tissues, considerably surpassing diffusion models' limitations. It also contributes valuable insights and analysis on the limitations of current diffusion models, promoting robust and interpretable anomaly detection in medical imaging - an essential aspect of building autonomous clinical decision systems with higher interpretability. |[2305.19643v1](http://arxiv.org/abs/2305.19643v1)|null|
|**2023-06-01**|**Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards**|['cs.CV', 'cs.AI']|Recent advances in text-to-image diffusion models have achieved remarkable success in generating high-quality, realistic images from given text prompts. However, previous methods fail to perform accurate modality alignment between text concepts and generated images due to the lack of fine-level semantic guidance that successfully diagnoses the modality discrepancy. In this paper, we propose FineRewards to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward. From the global semantic view, the caption reward generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt. From the local semantic view, the SAM reward segments the generated images into local parts with category labels, and scores the segmented parts by measuring the likelihood of each category appearing in the prompted scene via a large language model, i.e., Vicuna-7B. Additionally, we adopt an assemble reward-ranked learning strategy to enable the integration of multiple reward functions to jointly guide the model training. Adapting results of text-to-image models on the MS-COCO benchmark show that the proposed semantic reward outperforms other baseline reward functions with a considerable margin on both visual quality and semantic similarity with the input prompt. Moreover, by adopting the assemble reward-ranked learning strategy, we further demonstrate that model performance is further improved when adapting under the unifying of the proposed semantic reward with the current image rewards. |[2305.19599v2](http://arxiv.org/abs/2305.19599v2)|**[link](https://github.com/enderfga/finerewards)**|
|**2023-05-31**|**Improving Handwritten OCR with Training Samples Generated by Glyph Conditional Denoising Diffusion Probabilistic Model**|['cs.CV']|Constructing a highly accurate handwritten OCR system requires large amounts of representative training data, which is both time-consuming and expensive to collect. To mitigate the issue, we propose a denoising diffusion probabilistic model (DDPM) to generate training samples. This model conditions on a printed glyph image and creates mappings between printed characters and handwritten images, thus enabling the generation of photo-realistic handwritten samples with diverse styles and unseen text contents. However, the text contents in synthetic images are not always consistent with the glyph conditional images, leading to unreliable labels of synthetic samples. To address this issue, we further propose a progressive data filtering strategy to add those samples with a high confidence of correctness to the training set. Experimental results on IAM benchmark task show that OCR model trained with augmented DDPM-synthesized training samples can achieve about 45% relative word error rate reduction compared with the one trained on real data only. |[2305.19543v1](http://arxiv.org/abs/2305.19543v1)|null|
|**2023-05-31**|**Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels**|['cs.LG', 'cs.CV']|Learning from noisy labels is an important and long-standing problem in machine learning for real applications. One of the main research lines focuses on learning a label corrector to purify potential noisy labels. However, these methods typically rely on strict assumptions and are limited to certain types of label noise. In this paper, we reformulate the label-noise problem from a generative-model perspective, $\textit{i.e.}$, labels are generated by gradually refining an initial random guess. This new perspective immediately enables existing powerful diffusion models to seamlessly learn the stochastic generative process. Once the generative uncertainty is modeled, we can perform classification inference using maximum likelihood estimation of labels. To mitigate the impact of noisy labels, we propose the $\textbf{L}$abel-$\textbf{R}$etrieval-$\textbf{A}$ugmented (LRA) diffusion model, which leverages neighbor consistency to effectively construct pseudo-clean labels for diffusion training. Our model is flexible and general, allowing easy incorporation of different types of conditional information, $\textit{e.g.}$, use of pre-trained models, to further boost model performance. Extensive experiments are conducted for evaluation. Our model achieves new state-of-the-art (SOTA) results on all the standard real-world benchmark datasets. Remarkably, by incorporating conditional information from the powerful CLIP model, our method can boost the current SOTA accuracy by 10-20 absolute points in many cases. |[2305.19518v1](http://arxiv.org/abs/2305.19518v1)|**[link](https://github.com/puar-playground/lra-diffusion)**|
|**2023-05-31**|**Fine-grained Text Style Transfer with Diffusion-Based Language Models**|['cs.CL', 'cs.AI', 'cs.LG']|Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion-based language models have great potential under low-resource settings. |[2305.19512v1](http://arxiv.org/abs/2305.19512v1)|**[link](https://github.com/lvyiwei1/diffuseq_styleptb)**|
|**2023-05-31**|**Synthetic CT Generation from MRI using 3D Transformer-based Denoising Diffusion Model**|['eess.IV', 'cs.CV']|Magnetic resonance imaging (MRI)-based synthetic computed tomography (sCT) simplifies radiation therapy treatment planning by eliminating the need for CT simulation and error-prone image registration, ultimately reducing patient radiation dose and setup uncertainty. We propose an MRI-to-CT transformer-based denoising diffusion probabilistic model (MC-DDPM) to transform MRI into high-quality sCT to facilitate radiation treatment planning. MC-DDPM implements diffusion processes with a shifted-window transformer network to generate sCT from MRI. The proposed model consists of two processes: a forward process which adds Gaussian noise to real CT scans, and a reverse process in which a shifted-window transformer V-net (Swin-Vnet) denoises the noisy CT scans conditioned on the MRI from the same patient to produce noise-free CT scans. With an optimally trained Swin-Vnet, the reverse diffusion process was used to generate sCT scans matching MRI anatomy. We evaluated the proposed method by generating sCT from MRI on a brain dataset and a prostate dataset. Qualitative evaluation was performed using the mean absolute error (MAE) of Hounsfield unit (HU), peak signal to noise ratio (PSNR), multi-scale Structure Similarity index (MS-SSIM) and normalized cross correlation (NCC) indexes between ground truth CTs and sCTs. MC-DDPM generated brain sCTs with state-of-the-art quantitative results with MAE 43.317 HU, PSNR 27.046 dB, SSIM 0.965, and NCC 0.983. For the prostate dataset, MC-DDPM achieved MAE 59.953 HU, PSNR 26.920 dB, SSIM 0.849, and NCC 0.948. In conclusion, we have developed and validated a novel approach for generating CT images from routine MRIs using a transformer-based DDPM. This model effectively captures the complex relationship between CT and MRI images, allowing for robust and high-quality synthetic CT (sCT) images to be generated in minutes. |[2305.19467v1](http://arxiv.org/abs/2305.19467v1)|null|
|**2023-05-30**|**Diffusion-limited annihilating-coalescing systems**|['math.PR', '60K35']|We study a family of interacting particle systems with annihilating and coalescing reactions. Two types of particles are interspersed throughout a transitive unimodular graph. Both types diffuse as simple random walks with possibly different jump rates. Upon colliding, like particles coalesce up to some cap and unlike particles annihilate. We describe a phase transition as the initial particle density is varied and provide estimates for the expected occupation time of the root. For the symmetric setting with no cap on coalescence, we prove that the limiting occupation probability of the root is asymptotic to 2/3 the occupation probability for classical coalescing random walk. This addresses an open problem from Stephenson. |[2305.19333v1](http://arxiv.org/abs/2305.19333v1)|null|
|**2023-05-30**|**Ambient Diffusion: Learning Clean Distributions from Corrupted Data**|['cs.LG', 'cs.AI', 'cs.CV', 'cs.IT', 'math.IT']|We present the first diffusion-based framework that can learn an unknown distribution using only highly-corrupted samples. This problem arises in scientific applications where access to uncorrupted samples is impossible or expensive to acquire. Another benefit of our approach is the ability to train generative models that are less likely to memorize individual training samples since they never observe clean training data. Our main idea is to introduce additional measurement distortion during the diffusion process and require the model to predict the original corrupted image from the further corrupted image. We prove that our method leads to models that learn the conditional expectation of the full uncorrupted image given this additional measurement corruption. This holds for any corruption process that satisfies some technical conditions (and in particular includes inpainting and compressed sensing). We train models on standard benchmarks (CelebA, CIFAR-10 and AFHQ) and show that we can learn the distribution even when all the training samples have $90\%$ of their pixels missing. We also show that we can finetune foundation models on small corrupted datasets (e.g. MRI scans with block corruptions) and learn the clean distribution without memorizing the training set. |[2305.19256v1](http://arxiv.org/abs/2305.19256v1)|**[link](https://github.com/giannisdaras/ambient-diffusion)**|
|**2023-05-30**|**A boundary-oriented reduced Schwarz domain decomposition technique for parametric advection-diffusion problems**|['math.NA', 'cs.NA', '65, 76', 'G.1; I.6']|We present in this paper the results of a research motivated by the need of a very fast solution of thermal flow in solar receivers. These receivers are composed by a large number of parallel pipes with the same geometry. We have introduced a reduced Schwarz algorithm that skips the computation in a large part of the pipes. The computation of the temperature in the skep domain is replaced by a reduced mapping that provides the transmission conditions. This reduced mapping is computed in an off-line stage. We have performed an error analysis of the reduced Schwarz algorithm, proving that the error is bounded in terms of the linearly decreasing error of the standard Schwarz algorithm, plus the error stemming from the reduction of the trace mapping. The last error is asymptotically dominant in the Schwarz iterative process. We obtain $L^2$ errors below $2\%$ with relatively small overlapping lengths. |[2305.19199v1](http://arxiv.org/abs/2305.19199v1)|null|
|**2023-05-30**|**Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models**|['cs.CV']|In this study, we present an efficient and effective approach for achieving temporally consistent synthetic-to-real video translation in videos of varying lengths. Our method leverages off-the-shelf conditional image diffusion models, allowing us to perform multiple synthetic-to-real image generations in parallel. By utilizing the available optical flow information from the synthetic videos, our approach seamlessly enforces temporal consistency among corresponding pixels across frames. This is achieved through joint noise optimization, effectively minimizing spatial and temporal discrepancies. To the best of our knowledge, our proposed method is the first to accomplish diverse and temporally consistent synthetic-to-real video translation using conditional image diffusion models. Furthermore, our approach does not require any training or fine-tuning of the diffusion models. Extensive experiments conducted on various benchmarks for synthetic-to-real video translation demonstrate the effectiveness of our approach, both quantitatively and qualitatively. Finally, we show that our method outperforms other baseline methods in terms of both temporal consistency and visual quality. |[2305.19193v1](http://arxiv.org/abs/2305.19193v1)|null|
|**2023-05-28**|**Conditional score-based diffusion models for Bayesian inference in infinite dimensions**|['stat.ML', 'cs.LG', 'math.AP', 'math.PR', '62F15, 65N21, 68Q32, 60Hxx, 60Jxx']|Since their first introduction, score-based diffusion models (SDMs) have been successfully applied to solve a variety of linear inverse problems in finite-dimensional vector spaces due to their ability to efficiently approximate the posterior distribution. However, using SDMs for inverse problems in infinite-dimensional function spaces has only been addressed recently and by learning the unconditional score. While this approach has some advantages, depending on the specific inverse problem at hand, in order to sample from the conditional distribution it needs to incorporate the information from the observed data with a proximal optimization step, solving an optimization problem numerous times. This may not be feasible in inverse problems with computationally costly forward operators. To address these limitations, in this work we propose a method to learn the posterior distribution in infinite-dimensional Bayesian linear inverse problems using amortized conditional SDMs. In particular, we prove that the conditional denoising estimator is a consistent estimator of the conditional score in infinite dimensions. We show that the extension of SDMs to the conditional setting requires some care because the conditional score typically blows up for small times contrarily to the unconditional score. We also discuss the robustness of the learned distribution against perturbations of the observations. We conclude by presenting numerical examples that validate our approach and provide additional insights. |[2305.19147v1](http://arxiv.org/abs/2305.19147v1)|null|
|**2023-05-30**|**Calliffusion: Chinese Calligraphy Generation and Style Transfer with Diffusion Modeling**|['cs.CV']|In this paper, we propose Calliffusion, a system for generating high-quality Chinese calligraphy using diffusion models. Our model architecture is based on DDPM (Denoising Diffusion Probabilistic Models), and it is capable of generating common characters in five different scripts and mimicking the styles of famous calligraphers. Experiments demonstrate that our model can generate calligraphy that is difficult to distinguish from real artworks and that our controls for characters, scripts, and styles are effective. Moreover, we demonstrate one-shot transfer learning, using LoRA (Low-Rank Adaptation) to transfer Chinese calligraphy art styles to unseen characters and even out-of-domain symbols such as English letters and digits. |[2305.19124v1](http://arxiv.org/abs/2305.19124v1)|null|
|**2023-05-30**|**DiffMatch: Diffusion Model for Dense Matching**|['cs.CV']|The objective for establishing dense correspondence between paired images consists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, recent approaches have focused on learning the data term with deep neural networks without explicitly modeling the prior, assuming that the model itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, and large displacements. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms. Unlike previous approaches, this is accomplished by leveraging a conditional denoising diffusion model. DiffMatch consists of two main components: conditional denoising diffusion module and cost injection module. We stabilize the training process and reduce memory usage with a stage-wise training strategy. Furthermore, to boost performance, we introduce an inference technique that finds a better path to the accurate matching field. Our experimental results demonstrate significant performance improvements of our method over existing approaches, and the ablation studies validate our design choices along with the effectiveness of each component. Project page is available at https://ku-cvlab.github.io/DiffMatch/. |[2305.19094v1](http://arxiv.org/abs/2305.19094v1)|**[link](https://github.com/PruneTruong/DenseMatching)**|
|**2023-05-30**|**Nested Diffusion Processes for Anytime Image Generation**|['cs.CV']|Diffusion models are the current state-of-the-art in image generation, synthesizing high-quality images by breaking down the generation process into many fine-grained denoising steps. Despite their good performance, diffusion models are computationally expensive, requiring many neural function evaluations (NFEs). In this work, we propose an anytime diffusion-based method that can generate viable images when stopped at arbitrary times before completion. Using existing pretrained diffusion models, we show that the generation scheme can be recomposed as two nested diffusion processes, enabling fast iterative refinement of a generated image. We use this Nested Diffusion approach to peek into the generation process and enable flexible scheduling based on the instantaneous preference of the user. In experiments on ImageNet and Stable Diffusion-based text-to-image generation, we show, both qualitatively and quantitatively, that our method's intermediate generation quality greatly exceeds that of the original diffusion model, while the final slow generation result remains comparable. |[2305.19066v1](http://arxiv.org/abs/2305.19066v1)|**[link](https://github.com/noamelata/nesteddiffusion)**|
|**2023-05-30**|**A Heat Diffusion Perspective on Geodesic Preserving Dimensionality Reduction**|['cs.LG', 'q-bio.GN', 'q-bio.QM', 'stat.ML']|Diffusion-based manifold learning methods have proven useful in representation learning and dimensionality reduction of modern high dimensional, high throughput, noisy datasets. Such datasets are especially present in fields like biology and physics. While it is thought that these methods preserve underlying manifold structure of data by learning a proxy for geodesic distances, no specific theoretical links have been established. Here, we establish such a link via results in Riemannian geometry explicitly connecting heat diffusion to manifold distances. In this process, we also formulate a more general heat kernel based manifold embedding method that we call heat geodesic embeddings. This novel perspective makes clearer the choices available in manifold learning and denoising. Results show that our method outperforms existing state of the art in preserving ground truth manifold distances, and preserving cluster structure in toy datasets. We also showcase our method on single cell RNA-sequencing datasets with both continuum and cluster structure, where our method enables interpolation of withheld timepoints of data. Finally, we show that parameters of our more general method can be configured to give results similar to PHATE (a state-of-the-art diffusion based manifold learning method) as well as SNE (an attraction/repulsion neighborhood based method that forms the basis of t-SNE). |[2305.19043v1](http://arxiv.org/abs/2305.19043v1)|**[link](https://github.com/krishnaswamylab/heatgeo)**|
|**2023-05-30**|**A dynamical survey of the trans-Neptunian region II.: On the nature of chaotic diffusion**|['astro-ph.EP', 'nlin.CD']|On long enough timescales, chaotic diffusion has the potential to significantly alter the appearance of a dynamical system. The solar system is no exception: diffusive processes take part in the transportation of small bodies and provide dynamical pathways even for the distant trans-Neptunian objects to reach the inner solar system. In this Letter, we carry out a thorough investigation of the nature of chaotic diffusion. We analyze the temporal evolution of the mean squared displacement of ten thousand ensembles of test particles and quantify in each case the diffusion exponent (enabling the classification between normal, sub-, and super-diffusion), the generalized diffusion coefficient, and a characteristic diffusion timescale, too. This latter quantity is compared with an entropy-based timescale, and the two approaches are studied in light of direct computations as well. Our results are given in the context of two-dimensional maps, thereby facilitating the understanding of the relationship between the typical phase space structures and the properties of chaotic diffusion. |[2305.19018v1](http://arxiv.org/abs/2305.19018v1)|null|
|**2023-05-31**|**StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation**|['cs.CV', 'cs.AI']|The recent advancements in image-text diffusion models have stimulated research interest in large-scale 3D generative models. Nevertheless, the limited availability of diverse 3D resources presents significant challenges to learning. In this paper, we present a novel method for generating high-quality, stylized 3D avatars that utilizes pre-trained image-text diffusion models for data generation and a Generative Adversarial Network (GAN)-based 3D generation network for training. Our method leverages the comprehensive priors of appearance and geometry offered by image-text diffusion models to generate multi-view images of avatars in various styles. During data generation, we employ poses extracted from existing 3D models to guide the generation of multi-view images. To address the misalignment between poses and images in data, we investigate view-specific prompts and develop a coarse-to-fine discriminator for GAN training. We also delve into attribute-related prompts to increase the diversity of the generated avatars. Additionally, we develop a latent diffusion model within the style space of StyleGAN to enable the generation of avatars based on image inputs. Our approach demonstrates superior performance over current state-of-the-art methods in terms of visual quality and diversity of the produced avatars. |[2305.19012v2](http://arxiv.org/abs/2305.19012v2)|**[link](https://github.com/icoz69/styleavatar3d)**|
|**2023-05-30**|**Hyperbolic Diffusion Embedding and Distance for Hierarchical Representation Learning**|['cs.LG']|Finding meaningful representations and distances of hierarchical data is important in many fields. This paper presents a new method for hierarchical data embedding and distance. Our method relies on combining diffusion geometry, a central approach to manifold learning, and hyperbolic geometry. Specifically, using diffusion geometry, we build multi-scale densities on the data, aimed to reveal their hierarchical structure, and then embed them into a product of hyperbolic spaces. We show theoretically that our embedding and distance recover the underlying hierarchical structure. In addition, we demonstrate the efficacy of the proposed method and its advantages compared to existing methods on graph embedding benchmarks and hierarchical datasets. |[2305.18962v1](http://arxiv.org/abs/2305.18962v1)|**[link](https://github.com/ya-wei0/hyperbolicdiffusiondistance)**|
|**2023-05-30**|**The master equation for mean field game systems with fractional and nonlocal diffusions**|['math.AP', '35Q89, 35S10, 35A01, 35A08, 35K08, 49L12, 45K05, 35K61, 46E15, 46G05']|We prove existence and uniqueness of classical solutions of the master equation for mean field game (MFG) systems with fractional and nonlocal diffusions. We cover a large class of L\'evy diffusions of order greater than one, including purely nonlocal, local, and even mixed local-nonlocal operators. In the process we prove refined well-posedness results for the MFG systems, results that include the mixed local-nonlocal case. We also show various auxiliary results on viscous Hamilton-Jacobi equations, linear parabolic equations, and linear forward-backward systems that may be of independent interest. This includes a rigorous treatment of certain equations and systems with data and solutions in the duals of H\"older spaces $C^\gamma_b$ on the whole of $\mathbb{R}^d$. We do not assume existence of any moments for the initial distributions of players. In a future work we will use the results of this paper to prove the convergence of $N$-player games to mean field games as $N\to\infty$. |[2305.18867v1](http://arxiv.org/abs/2305.18867v1)|null|
|**2023-05-30**|**Identification of Novel Diagnostic Neuroimaging Biomarkers for Autism Spectrum Disorder Through Convolutional Neural Network-Based Analysis of Functional, Structural, and Diffusion Tensor Imaging Data Towards Enhanced Autism Diagnosis**|['q-bio.NC']|Autism Spectrum Disorder is one of the leading neurodevelopmental disorders in our world, present in over 1% of the population and rapidly increasing in prevalence, yet the condition lacks a robust, objective, and efficient diagnostic. Clinical diagnostic criteria rely on subjective behavioral assessments, which are prone to misdiagnosis as they face limitations in terms of their heterogeneity, specificity, and biases. This study proposes a novel convolutional-neural-network based classification tool that aims to identify the potential of different neuroimaging features as autism biomarkers. The model is constructed using a set of sequential layers specifically designed to extract relevant features from brain scans. Trained and tested on over 300,000 distinct features across three imaging types, the model shows promising results, achieving an accuracy of 95.4% and outperforming metrics of current gold standard diagnostics. 32 optimal features from the imaging data were identified and classified as candidate biomarkers using an independent samples t-test, in which functional features such as neural activity and connectivity in various brain regions exhibited the highest differences in the mean values between individuals with autism and typical control subjects. The p-values of these biomarkers were < 0.001, proving the statistical significance of the results and indicating that this research could pave the way towards the usage of neuroimaging in conjunction with behavioral criteria in clinics. Furthermore, the salient features discovered in the brain structure of individuals with autism could lead to a more profound understanding of the underlying neurobiological mechanisms of the disorder, which remains one of the most substantial enigmas in the field even today. |[2305.18841v1](http://arxiv.org/abs/2305.18841v1)|null|
|**2023-05-30**|**DiffSketching: Sketch Control Image Synthesis with Diffusion Models**|['cs.CV', 'cs.AI']|Creative sketch is a universal way of visual expression, but translating images from an abstract sketch is very challenging. Traditionally, creating a deep learning model for sketch-to-image synthesis needs to overcome the distorted input sketch without visual details, and requires to collect large-scale sketch-image datasets. We first study this task by using diffusion models. Our model matches sketches through the cross domain constraints, and uses a classifier to guide the image synthesis more accurately. Extensive experiments confirmed that our method can not only be faithful to user's input sketches, but also maintain the diversity and imagination of synthetic image results. Our model can beat GAN-based method in terms of generation quality and human evaluation, and does not rely on massive sketch-image datasets. Additionally, we present applications of our method in image editing and interpolation. |[2305.18812v1](http://arxiv.org/abs/2305.18812v1)|null|
|**2023-05-31**|**HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance**|['cs.CV', 'cs.AI', 'cs.LG']|Automatic text-to-3D synthesis has achieved remarkable advancements through the optimization of 3D models. Existing methods commonly rely on pre-trained text-to-image generative models, such as diffusion models, providing scores for 2D renderings of Neural Radiance Fields (NeRFs) and being utilized for optimizing NeRFs. However, these methods often encounter artifacts and inconsistencies across multiple views due to their limited understanding of 3D geometry. To address these limitations, we propose a reformulation of the optimization loss using the diffusion prior. Furthermore, we introduce a novel training approach that unlocks the potential of the diffusion prior. To improve 3D geometry representation, we apply auxiliary depth supervision for NeRF-rendered images and regularize the density field of NeRFs. Extensive experiments demonstrate the superiority of our method over prior works, resulting in advanced photo-realism and improved multi-view consistency. |[2305.18766v2](http://arxiv.org/abs/2305.18766v2)|null|
|**2023-05-30**|**Generating Behaviorally Diverse Policies with Latent Diffusion Models**|['cs.LG', 'cs.AI', 'cs.RO']|Recent progress in Quality Diversity Reinforcement Learning (QD-RL) has enabled learning a collection of behaviorally diverse, high performing policies. However, these methods typically involve storing thousands of policies, which results in high space-complexity and poor scaling to additional behaviors. Condensing the archive into a single model while retaining the performance and coverage of the original collection of policies has proved challenging. In this work, we propose using diffusion models to distill the archive into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98% of the original rewards and 89% of the original coverage. Further, the conditioning mechanism of diffusion models allows for flexibly selecting and sequencing behaviors, including using language. Project website: https://sites.google.com/view/policydiffusion/home |[2305.18738v1](http://arxiv.org/abs/2305.18738v1)|null|
|**2023-05-30**|**Real-World Image Variation by Aligning Diffusion Inversion Chain**|['cs.CV']|Recent diffusion model advancements have enabled high-fidelity images to be generated using text prompts. However, a domain gap exists between generated images and real-world images, which poses a challenge in generating high-quality variations of real-world images. Our investigation uncovers that this domain gap originates from a latents' distribution gap in different diffusion processes. To address this issue, we propose a novel inference pipeline called Real-world Image Variation by ALignment (RIVAL) that utilizes diffusion models to generate image variations from a single image exemplar. Our pipeline enhances the generation quality of image variations by aligning the image generation process to the source image's inversion chain. Specifically, we demonstrate that step-wise latent distribution alignment is essential for generating high-quality variations. To attain this, we design a cross-image self-attention injection for feature interaction and a step-wise distribution normalization to align the latent features. Incorporating these alignment processes into a diffusion model allows RIVAL to generate high-quality image variations without further parameter optimization. Our experimental results demonstrate that our proposed approach outperforms existing methods with respect to semantic-condition similarity and perceptual quality. Furthermore, this generalized inference pipeline can be easily applied to other diffusion-based generation tasks, such as image-conditioned text-to-image generation and example-based image inpainting. |[2305.18729v1](http://arxiv.org/abs/2305.18729v1)|**[link](https://github.com/julianjuaner/rival)**|
|**2023-05-30**|**Diffusion-Stego: Training-free Diffusion Generative Steganography via Message Projection**|['cs.CV']|Generative steganography is the process of hiding secret messages in generated images instead of cover images. Existing studies on generative steganography use GAN or Flow models to obtain high hiding message capacity and anti-detection ability over cover images. However, they create relatively unrealistic stego images because of the inherent limitations of generative models. We propose Diffusion-Stego, a generative steganography approach based on diffusion models which outperform other generative models in image generation. Diffusion-Stego projects secret messages into latent noise of diffusion models and generates stego images with an iterative denoising process. Since the naive hiding of secret messages into noise boosts visual degradation and decreases extracted message accuracy, we introduce message projection, which hides messages into noise space while addressing these issues. We suggest three options for message projection to adjust the trade-off between extracted message accuracy, anti-detection ability, and image quality. Diffusion-Stego is a training-free approach, so we can apply it to pre-trained diffusion models which generate high-quality images, or even large-scale text-to-image models, such as Stable diffusion. Diffusion-Stego achieved a high capacity of messages (3.0 bpp of binary messages with 98% accuracy, and 6.0 bpp with 90% accuracy) as well as high quality (with a FID score of 2.77 for 1.0 bpp on the FFHQ 64$\times$64 dataset) that makes it challenging to distinguish from real images in the PNG format. |[2305.18726v1](http://arxiv.org/abs/2305.18726v1)|null|
|**2023-05-30**|**Towards Accurate Data-free Quantization for Diffusion Models**|['cs.CV']|In this paper, we propose an accurate data-free post-training quantization framework of diffusion models (ADP-DM) for efficient image generation. Conventional data-free quantization methods learn shared quantization functions for tensor discretization regardless of the generation timesteps, while the activation distribution differs significantly across various timesteps. The calibration images are acquired in random timesteps which fail to provide sufficient information for generalizable quantization function learning. Both issues cause sizable quantization errors with obvious image generation performance degradation. On the contrary, we design group-wise quantization functions for activation discretization in different timesteps and sample the optimal timestep for informative calibration image generation, so that our quantized diffusion model can reduce the discretization errors with negligible computational overhead. Specifically, we partition the timesteps according to the importance weights of quantization functions in different groups, which are optimized by differentiable search algorithms. We also select the optimal timestep for calibration image generation by structural risk minimizing principle in order to enhance the generalization ability in the deployment of quantized diffusion model. Extensive experimental results show that our method outperforms the state-of-the-art post-training quantization of diffusion model by a sizable margin with similar computational cost. |[2305.18723v1](http://arxiv.org/abs/2305.18723v1)|null|
|**2023-05-30**|**A Mixed Finite Element Method for Singularly Perturbed Fourth Oder Convection-Reaction-Diffusion Problems on Shishkin Mesh**|['math.NA', 'cs.NA']|This paper introduces an approach to decoupling singularly perturbed boundary value problems for fourth-order ordinary differential equations that feature a small positive parameter $\epsilon$ multiplying the highest derivative. We specifically examine Lidstone boundary conditions and demonstrate how to break down fourth-order differential equations into a system of second-order problems, with one lacking the parameter and the other featuring $\epsilon$ multiplying the highest derivative. To solve this system, we propose a mixed finite element algorithm and incorporate the Shishkin mesh scheme to capture the solution near boundary layers. Our solver is both direct and of high accuracy, with computation time that scales linearly with the number of grid points. We present numerical results to validate the theoretical results and the accuracy of our method. |[2305.18711v1](http://arxiv.org/abs/2305.18711v1)|null|
|**2023-05-30**|**LayerDiffusion: Layered Controlled Image Editing with Diffusion Models**|['cs.CV']|Text-guided image editing has recently experienced rapid development. However, simultaneously performing multiple editing actions on a single image, such as background replacement and specific subject attribute changes, while maintaining consistency between the subject and the background remains challenging. In this paper, we propose LayerDiffusion, a semantic-based layered controlled image editing method. Our method enables non-rigid editing and attribute modification of specific subjects while preserving their unique characteristics and seamlessly integrating them into new backgrounds. We leverage a large-scale text-to-image model and employ a layered controlled optimization strategy combined with layered diffusion training. During the diffusion process, an iterative guidance strategy is used to generate a final image that aligns with the textual description. Experimental results demonstrate the effectiveness of our method in generating highly coherent images that closely align with the given textual description. The edited images maintain a high similarity to the features of the input image and surpass the performance of current leading image editing methods. LayerDiffusion opens up new possibilities for controllable image editing. |[2305.18676v1](http://arxiv.org/abs/2305.18676v1)|null|
|**2023-05-30**|**SAVE: Spectral-Shift-Aware Adaptation of Image Diffusion Models for Text-guided Video Editing**|['cs.CV']|Text-to-Image (T2I) diffusion models have achieved remarkable success in synthesizing high-quality images conditioned on text prompts. Recent methods have tried to replicate the success by either training text-to-video (T2V) models on a very large number of text-video pairs or adapting T2I models on text-video pairs independently. Although the latter is computationally less expensive, it still takes a significant amount of time for per-video adaption. To address this issue, we propose SAVE, a novel spectral-shift-aware adaptation framework, in which we fine-tune the spectral shift of the parameter space instead of the parameters themselves. Specifically, we take the spectral decomposition of the pre-trained T2I weights and only control the change in the corresponding singular values, i.e. spectral shift, while freezing the corresponding singular vectors. To avoid drastic drift from the original T2I weights, we introduce a spectral shift regularizer that confines the spectral shift to be more restricted for large singular values and more relaxed for small singular values. Since we are only dealing with spectral shifts, the proposed method reduces the adaptation time significantly (approx. 10 times) and has fewer resource constrains for training. Such attributes posit SAVE to be more suitable for real-world applications, e.g. editing undesirable content during video streaming. We validate the effectiveness of SAVE with an extensive experimental evaluation under different settings, e.g. style transfer, object replacement, privacy preservation, etc. |[2305.18670v1](http://arxiv.org/abs/2305.18670v1)|null|
|**2023-05-29**|**Symmetrization of laminar viscous fluid flow in a flat diffuser by periodic impaction on the inlet flow velocity**|['physics.flu-dyn']|This paper shows a method of symmetrization of an asymmetric flow of a vis-cous incompressible fluid in a flat diffuser using a weak periodic vibration ef-fect on the velocity input flow. The results are obtained for a viscous incom-pressible fluid by simulation based on numerical solving of the Navier-Stokes equations. The results of numerical simulation have shown one of the ways of symme-trization of asymmetric laminar flows of a viscous incompressible fluid in a flat diffuser with a weak periodic effect on the input flow velocity. It is shown that vibration effects, even at amplitudes less than 1% of the velocity can symmetrize the fluid flow in the diffuser. Richardson's "annular effect" for a harmonic oscillating fluid flow in the diffuser was shown. |[2305.18631v1](http://arxiv.org/abs/2305.18631v1)|null|
|**2023-05-30**|**Likelihood-Based Diffusion Language Models**|['cs.CL', 'cs.LG']|Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent samples in unconditional and zero-shot control settings. |[2305.18619v1](http://arxiv.org/abs/2305.18619v1)|**[link](https://github.com/igul222/plaid)**|
|**2023-05-29**|**On Diffusion Modeling for Anomaly Detection**|['cs.LG']|Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Probabilistic Model (DTPM). DTPM estimates the posterior distribution over diffusion time for a given input, enabling the identification of anomalies due to their higher posterior density at larger timesteps. We derive an analytical form for this posterior density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods perform competitively. Notably, DTPM achieves orders of magnitude faster inference time than DDPM, while outperforming it on this benchmark. These results establish diffusion-based anomaly detection as an interpretable and scalable alternative to traditional methods and recent deep-learning techniques. |[2305.18593v1](http://arxiv.org/abs/2305.18593v1)|null|
|**2023-05-29**|**Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation**|['cs.LG', 'cs.CE', 'cs.CV']|Generative models have had a profound impact on vision and language, paving the way for a new era of multimodal generative applications. While these successes have inspired researchers to explore using generative models in science and engineering to accelerate the design process and reduce the reliance on iterative optimization, challenges remain. Specifically, engineering optimization methods based on physics still outperform generative models when dealing with constrained environments where data is scarce and precision is paramount. To address these challenges, we introduce Diffusion Optimization Models (DOM) and Trajectory Alignment (TA), a learning framework that demonstrates the efficacy of aligning the sampling trajectory of diffusion models with the optimization trajectory derived from traditional physics-based methods. This alignment ensures that the sampling process remains grounded in the underlying physical principles. Our method allows for generating feasible and high-performance designs in as few as two steps without the need for expensive preprocessing, external surrogate models, or additional labeled data. We apply our framework to structural topology optimization, a fundamental problem in mechanical design, evaluating its performance on in- and out-of-distribution configurations. Our results demonstrate that TA outperforms state-of-the-art deep generative models on in-distribution configurations and halves the inference computational cost. When coupled with a few steps of optimization, it also improves manufacturability for out-of-distribution conditions. By significantly improving performance and inference efficiency, DOM enables us to generate high-quality designs in just a few steps and guide them toward regions of high performance and manufacturability, paving the way for the widespread application of generative models in large-scale data-driven design. |[2305.18470v1](http://arxiv.org/abs/2305.18470v1)|null|
|**2023-05-29**|**Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning**|['cs.LG', 'cs.AI']|Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, we find \textsc{MTDiff} outperforms state-of-the-art algorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data synthesis, \textsc{MTDiff} generates high-quality data for testing tasks given a single demonstration as a prompt, which enhances the low-quality datasets for even unseen tasks. |[2305.18459v1](http://arxiv.org/abs/2305.18459v1)|**[link](https://github.com/tinnerhrhe/MTDiff)**|
|**2023-05-29**|**Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models**|['cs.LG', 'cs.CV']|Due to the ease of training, ability to scale, and high sample quality, diffusion models (DMs) have become the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets. Containing intricate information about data distributions, pre-trained DMs are valuable assets for downstream applications. In this work, we consider learning from pre-trained DMs and transferring their knowledge to other generative models in a data-free fashion. Specifically, we propose a general framework called Diff-Instruct to instruct the training of arbitrary generative models as long as the generated samples are differentiable with respect to the model parameters. Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffusion process, which we show to be more robust in comparing distributions with misaligned supports. We also reveal non-trivial connections of our method to existing works such as DreamFusion, and generative adversarial training. To demonstrate the effectiveness and universality of Diff-Instruct, we consider two scenarios: distilling pre-trained diffusion models and refining existing GAN models. The experiments on distilling pre-trained diffusion models show that Diff-Instruct results in state-of-the-art single-step diffusion-based models. The experiments on refining GAN models show that the Diff-Instruct can consistently improve the pre-trained generators of GAN models across various settings. |[2305.18455v1](http://arxiv.org/abs/2305.18455v1)|null|
|**2023-05-29**|**Conditional Diffusion Models for Semantic 3D Medical Image Synthesis**|['eess.IV', 'cs.CV', 'cs.LG']|This paper introduces Med-DDPM, an innovative solution using diffusion models for semantic 3D medical image synthesis, addressing the prevalent issues in medical imaging such as data scarcity, inconsistent acquisition methods, and privacy concerns. Experimental evidence illustrates that diffusion models surpass Generative Adversarial Networks (GANs) in stability and performance, generating high-quality, realistic 3D medical images. The distinct feature of Med-DDPM is its use of semantic conditioning for the diffusion model in 3D image synthesis. By controlling the generation process through pixel-level mask labels, it facilitates the creation of realistic medical images. Empirical evaluations underscore the superior performance of Med-DDPM over GAN techniques in metrics such as accuracy, stability, and versatility. Furthermore, Med-DDPM outperforms traditional augmentation techniques and synthetic GAN images in enhancing the accuracy of segmentation models. It addresses challenges such as insufficient datasets, lack of annotated data, and class imbalance. Noting the limitations of the Frechet inception distance (FID) metric, we introduce a histogram-equalized FID metric for effective performance evaluation. In summary, Med-DDPM, by utilizing diffusion models, signifies a crucial step forward in the domain of high-resolution semantic 3D medical image synthesis, transcending the limitations of GANs and data constraints. This method paves the way for a promising solution in medical imaging, primarily for data augmentation and anonymization, thus contributing significantly to the field. |[2305.18453v1](http://arxiv.org/abs/2305.18453v1)|null|
|**2023-05-29**|**Generating Driving Scenes with Diffusion**|['cs.CV', 'cs.LG']|In this paper we describe a learned method of traffic scene generation designed to simulate the output of the perception system of a self-driving car. In our "Scene Diffusion" system, inspired by latent diffusion, we use a novel combination of diffusion and object detection to directly create realistic and physically plausible arrangements of discrete bounding boxes for agents. We show that our scene generation model is able to adapt to different regions in the US, producing scenarios that capture the intricacies of each region. |[2305.18452v1](http://arxiv.org/abs/2305.18452v1)|null|
|**2023-05-28**|**Cognitively Inspired Cross-Modal Data Generation Using Diffusion Models**|['cs.LG', 'cs.CV']|Most existing cross-modal generative methods based on diffusion models use guidance to provide control over the latent space to enable conditional generation across different modalities. Such methods focus on providing guidance through separately-trained models, each for one modality. As a result, these methods suffer from cross-modal information loss and are limited to unidirectional conditional generation. Inspired by how humans synchronously acquire multi-modal information and learn the correlation between modalities, we explore a multi-modal diffusion model training and sampling scheme that uses channel-wise image conditioning to learn cross-modality correlation during the training phase to better mimic the learning process in the brain. Our empirical results demonstrate that our approach can achieve data generation conditioned on all correlated modalities. |[2305.18433v1](http://arxiv.org/abs/2305.18433v1)|null|
|**2023-05-29**|**RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths**|['cs.CV']|Text-to-image generation has recently witnessed remarkable achievements. We introduce a text-conditional image diffusion model, termed RAPHAEL, to generate highly artistic images, which accurately portray the text prompts, encompassing multiple nouns, adjectives, and verbs. This is achieved by stacking tens of mixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling billions of diffusion paths (routes) from the network input to the output. Each path intuitively functions as a "painter" for depicting a particular textual concept onto a specified image region at a diffusion timestep. Comprehensive experiments reveal that RAPHAEL outperforms recent cutting-edge models, such as Stable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both image quality and aesthetic appeal. Firstly, RAPHAEL exhibits superior performance in switching images across diverse styles, such as Japanese comics, realism, cyberpunk, and ink illustration. Secondly, a single model with three billion parameters, trained on 1,000 A100 GPUs for two months, achieves a state-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore, RAPHAEL significantly surpasses its counterparts in human evaluation on the ViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel the frontiers of image generation research in both academia and industry, paving the way for future breakthroughs in this rapidly evolving field. More details can be found on a project webpage: https://raphael-painter.github.io/. |[2305.18295v1](http://arxiv.org/abs/2305.18295v1)|null|
|**2023-05-29**|**Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models**|['cs.CV']|Public large-scale text-to-image diffusion models, such as Stable Diffusion, have gained significant attention from the community. These models can be easily customized for new concepts using low-rank adaptations (LoRAs). However, the utilization of multiple concept LoRAs to jointly support multiple customized concepts presents a challenge. We refer to this scenario as decentralized multi-concept customization, which involves single-client concept tuning and center-node concept fusion. In this paper, we propose a new framework called Mix-of-Show that addresses the challenges of decentralized multi-concept customization, including concept conflicts resulting from existing single-client LoRA tuning and identity loss during model fusion. Mix-of-Show adopts an embedding-decomposed LoRA (ED-LoRA) for single-client tuning and gradient fusion for the center node to preserve the in-domain essence of single concepts and support theoretically limitless concept fusion. Additionally, we introduce regionally controllable sampling, which extends spatially controllable sampling (e.g., ControlNet and T2I-Adaptor) to address attribute binding and missing object problems in multi-concept sampling. Extensive experiments demonstrate that Mix-of-Show is capable of composing multiple customized concepts with high fidelity, including characters, objects, and scenes. |[2305.18292v1](http://arxiv.org/abs/2305.18292v1)|null|
|**2023-05-29**|**Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors**|['cs.CV', 'cs.AI', 'q-bio.NC']|We present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct viewed images from brain activity. Our model comprises two parallel submodules that are specialized for retrieval (using contrastive learning) and reconstruction (using a diffusion prior). MindEye can map fMRI brain activity to any high dimensional multimodal latent space, like CLIP image space, enabling image reconstruction using generative models that accept embeddings from this latent space. We comprehensively compare our approach with other existing methods, using both qualitative side-by-side comparisons and quantitative evaluations, and show that MindEye achieves state-of-the-art performance in both reconstruction and retrieval tasks. In particular, MindEye can retrieve the exact original image even among highly similar candidates indicating that its brain embeddings retain fine-grained image-specific information. This allows us to accurately retrieve images even from large-scale databases like LAION-5B. We demonstrate through ablations that MindEye's performance improvements over previous methods result from specialized submodules for retrieval and reconstruction, improved training techniques, and training models with orders of magnitude more parameters. Furthermore, we show that MindEye can better preserve low-level image features in the reconstructions by using img2img, with outputs from a separate autoencoder. All code is available on GitHub. |[2305.18274v1](http://arxiv.org/abs/2305.18274v1)|**[link](https://github.com/medarc-ai/fmri-reconstruction-nsd)**|
|**2023-05-30**|**Accurate Estimation of Diffusion Coefficients and their Uncertainties from Computer Simulation**|['cond-mat.stat-mech', 'cond-mat.mtrl-sci']|Self-diffusion coefficients, $D^*$, are routinely estimated from molecular dynamics simulations by fitting a linear model to the observed mean-squared displacements (MSDs) of mobile species. MSDs derived from simulation suffer from statistical noise, which introduces uncertainty in the resulting estimate of $D^*$. An optimal scheme for estimating $D^*$ will minimise this uncertainty, i.e., will have high statistical efficiency, and will give an accurate estimate of the uncertainty itself. We present a scheme for estimating $D^*$ from a single simulation trajectory with high statistical efficiency and accurately estimating the uncertainty in the predicted value. The statistical distribution of MSDs observable from a given simulation is modelled as a multivariate normal distribution using an analytical covariance matrix for an equivalent system of freely diffusing particles, which we parameterise from the available simulation data. We then perform Bayesian regression to sample the distribution of linear models that are compatible with this model multivariate normal distribution, to obtain a statistically efficient estimate of $D^*$ and an accurate estimate of the associated statistical uncertainty. |[2305.18244v2](http://arxiv.org/abs/2305.18244v2)|**[link](https://github.com/arm61/msd-errors)**|
|**2023-05-29**|**Diffusion enhancement and Taylor dispersion for rotationally symmetric flows in discs and pipes**|['math.AP', 'physics.flu-dyn', '35Q35, 47B44, 76F25']|In this note, we study the long-time dynamics of passive scalars driven by rotationally symmetric flows. We focus on identifying precise conditions on the velocity field in order to prove enhanced dissipation and Taylor dispersion in three-dimensional infinite pipes. As a byproduct of our analysis, we obtain an enhanced decay for circular flows on a disc of arbitrary radius. |[2305.18162v1](http://arxiv.org/abs/2305.18162v1)|null|
|**2023-05-29**|**InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions**|['cs.CV']|Recent works have explored text-guided image editing using diffusion models and generated edited images based on text prompts. However, the models struggle to accurately locate the regions to be edited and faithfully perform precise edits. In this work, we propose a framework termed InstructEdit that can do fine-grained editing based on user instructions. Our proposed framework has three components: language processor, segmenter, and image editor. The first component, the language processor, processes the user instruction using a large language model. The goal of this processing is to parse the user instruction and output prompts for the segmenter and captions for the image editor. We adopt ChatGPT and optionally BLIP2 for this step. The second component, the segmenter, uses the segmentation prompt provided by the language processor. We employ a state-of-the-art segmentation framework Grounded Segment Anything to automatically generate a high-quality mask based on the segmentation prompt. The third component, the image editor, uses the captions from the language processor and the masks from the segmenter to compute the edited image. We adopt Stable Diffusion and the mask-guided generation from DiffEdit for this purpose. Experiments show that our method outperforms previous editing methods in fine-grained editing applications where the input image contains a complex object or multiple objects. We improve the mask quality over DiffEdit and thus improve the quality of edited images. We also show that our framework can accept multiple forms of user instructions as input. We provide the code at https://github.com/QianWangX/InstructEdit. |[2305.18047v1](http://arxiv.org/abs/2305.18047v1)|**[link](https://github.com/qianwangx/instructedit)**|
|**2023-05-29**|**A reduced conjugate gradient basis method for fractional diffusion**|['math.NA', 'cs.NA', '65N30, 65N55']|This work is on a fast and accurate reduced basis method for solving discretized fractional elliptic partial differential equations (PDEs) of the form $\mathcal{A}^su=f$ by rational approximation. A direct computation of the action of such an approximation would require solving multiple (20$\sim$30) large-scale sparse linear systems. Our method constructs the reduced basis using the first few directions obtained from the preconditioned conjugate gradient method applied to one of the linear systems. As shown in the theory and experiments, only a small number of directions (5$\sim$10) are needed to approximately solve all large-scale systems on the reduced basis subspace. This reduces the computational cost dramatically because: (1) We only use one of the large-scale problems to construct the basis; and (2) all large-scale problems restricted to the subspace have much smaller sizes. We test our algorithms for fractional PDEs on a 3d Euclidean domain, a 2d surface, and random combinatorial graphs. We also use a novel approach to construct the rational approximation for the fractional power function by the orthogonal greedy algorithm (OGA). |[2305.18038v1](http://arxiv.org/abs/2305.18038v1)|**[link](https://github.com/yuwenli925/rcgbm)**|
|**2023-05-29**|**CamoDiffusion: Camouflaged Object Detection via Conditional Diffusion Models**|['cs.CV', 'cs.AI']|Camouflaged Object Detection (COD) is a challenging task in computer vision due to the high similarity between camouflaged objects and their surroundings. Existing COD methods primarily employ semantic segmentation, which suffers from overconfident incorrect predictions. In this paper, we propose a new paradigm that treats COD as a conditional mask-generation task leveraging diffusion models. Our method, dubbed CamoDiffusion, employs the denoising process of diffusion models to iteratively reduce the noise of the mask. Due to the stochastic sampling process of diffusion, our model is capable of sampling multiple possible predictions from the mask distribution, avoiding the problem of overconfident point estimation. Moreover, we develop specialized learning strategies that include an innovative ensemble approach for generating robust predictions and tailored forward diffusion methods for efficient training, specifically for the COD task. Extensive experiments on three COD datasets attest the superior performance of our model compared to existing state-of-the-art methods, particularly on the most challenging COD10K dataset, where our approach achieves 0.019 in terms of MAE. |[2305.17932v1](http://arxiv.org/abs/2305.17932v1)|**[link](https://github.com/rapisurazurite/camodiffusion)**|
|**2023-05-28**|**Diffusive limits of the steady state radiative heat transfer system: Curvature effects**|['math.AP']|This paper is devoted to the diffusive limit of the nonlinear radiative heat transfer system with curved boundary domain (\textit{two dimensional disk}). The solution constructed in \cite{ghattassi2022convergence} by the leading order interior solution and the boundary layer corrections fails here to approximate the solutions in $L^\infty$ sense for the diffusive limit. The present paper aims to construct a geometric correction to the boundary layer problem and obtain a valid approximate solution in $L^\infty$ sense. The main tools to overcome the convergence problem, are to use matched asymptotic expansion techniques, fixed-point theorems, linear and nonlinear stability analysis of the boundary layer problem. In particular, the spectral assumption on the leading order interior solution, which was proposed for the flat case in \cite{Bounadrylayer2019GHM2}, is shown to be still valid which guarantee the stability of the boundary layer expansion with geometric corrections. Moreover, the convergence result established in \cite[Lemma 10]{ghattassi2022convergence} remain applicable for the approximate solution with geometric corrections. |[2305.17661v1](http://arxiv.org/abs/2305.17661v1)|null|
|**2023-05-27**|**A Diffusion Model for Event Skeleton Generation**|['cs.CL']|Event skeleton generation, aiming to induce an event schema skeleton graph with abstracted event nodes and their temporal relations from a set of event instance graphs, is a critical step in the temporal complex event schema induction task. Existing methods effectively address this task from a graph generation perspective but suffer from noise-sensitive and error accumulation, e.g., the inability to correct errors while generating schema. We, therefore, propose a novel Diffusion Event Graph Model~(DEGM) to address these issues. Our DEGM is the first workable diffusion model for event skeleton generation, where the embedding and rounding techniques with a custom edge-based loss are introduced to transform a discrete event graph into learnable latent representation. Furthermore, we propose a denoising training process to maintain the model's robustness. Consequently, DEGM derives the final schema, where error correction is guaranteed by iteratively refining the latent representation during the schema generation process. Experimental results on three IED bombing datasets demonstrate that our DEGM achieves better results than other state-of-the-art baselines. Our code and data are available at https://github.com/zhufq00/EventSkeletonGeneration. |[2305.17458v1](http://arxiv.org/abs/2305.17458v1)|null|
|**2023-05-27**|**Creating Personalized Synthetic Voices from Post-Glossectomy Speech with Guided Diffusion Models**|['eess.AS']|This paper is about developing personalized speech synthesis systems with recordings of mildly impaired speech. In particular, we consider consonant and vowel alterations resulted from partial glossectomy, the surgical removal of part of the tongue. The aim is to restore articulation in the synthesized speech and maximally preserve the target speaker's individuality. We propose to tackle the problem with guided diffusion models. Specifically, a diffusion-based speech synthesis model is trained on original recordings, to capture and preserve the target speaker's original articulation style. When using the model for inference, a separately trained phone classifier will guide the synthesis process towards proper articulation. Objective and subjective evaluation results show that the proposed method substantially improves articulation in the synthesized speech over original recordings, and preserves more of the target speaker's individuality than a voice conversion baseline. |[2305.17436v1](http://arxiv.org/abs/2305.17436v1)|null|
|**2023-05-27**|**Towards Consistent Video Editing with Text-to-Image Diffusion Models**|['cs.CV', 'cs.AI']|Existing works have advanced Text-to-Image (TTI) diffusion models for video editing in a one-shot learning manner. Despite their low requirements of data and computation, these methods might produce results of unsatisfied consistency with text prompt as well as temporal sequence, limiting their applications in the real world. In this paper, we propose to address the above issues with a novel EI$^2$ model towards \textbf{E}nhancing v\textbf{I}deo \textbf{E}diting cons\textbf{I}stency of TTI-based frameworks. Specifically, we analyze and find that the inconsistent problem is caused by newly added modules into TTI models for learning temporal information. These modules lead to covariate shift in the feature space, which harms the editing capability. Thus, we design EI$^2$ to tackle the above drawbacks with two classical modules: Shift-restricted Temporal Attention Module (STAM) and Fine-coarse Frame Attention Module (FFAM). First, through theoretical analysis, we demonstrate that covariate shift is highly related to Layer Normalization, thus STAM employs a \textit{Instance Centering} layer replacing it to preserve the distribution of temporal features. In addition, {STAM} employs an attention layer with normalized mapping to transform temporal features while constraining the variance shift. As the second part, we incorporate {STAM} with a novel {FFAM}, which efficiently leverages fine-coarse spatial information of overall frames to further enhance temporal consistency. Extensive experiments demonstrate the superiority of the proposed EI$^2$ model for text-driven video editing. |[2305.17431v1](http://arxiv.org/abs/2305.17431v1)|null|

<p align=right>(<a href=#Updated-on-20230603>back to top</a>)</p>

## text generation

|Publish Date|Title|Categories|Abstract|PDF|Code|
|---|---|---|---|---|
|**2023-06-01**|**Finite Entanglement Entropy in String Theory**|['hep-th', 'gr-qc', 'math-ph', 'math.MP', 'quant-ph']|We analyze the one-loop quantum entanglement entropy in ten-dimensional Type-II string theory using the orbifold method by analytically continuing in $N$ the genus-one partition function for string orbifolds on $\mathbb{R}^2/\mathbb{Z}_N$ conical spaces known for all odd integers $N > 1$. We show that the tachyonic contributions to the orbifold partition function can be appropriately summed and analytically continued to an expression that is finite in the physical region $0 < N \leq 1$ resulting in a finite and calculable answer for the entanglement entropy. We discuss the implications of the finiteness of the entanglement entropy for the information paradox, quantum gravity, and holography. |[2306.00990v1](http://arxiv.org/abs/2306.00990v1)|null|
|**2023-06-01**|**StyleGAN knows Normal, Depth, Albedo, and More**|['cs.CV', 'cs.GR', 'cs.LG']|Intrinsic images, in the original sense, are image-like maps of scene properties like depth, normal, albedo or shading. This paper demonstrates that StyleGAN can easily be induced to produce intrinsic images. The procedure is straightforward. We show that, if StyleGAN produces $G({w})$ from latents ${w}$, then for each type of intrinsic image, there is a fixed offset ${d}_c$ so that $G({w}+{d}_c)$ is that type of intrinsic image for $G({w})$. Here ${d}_c$ is {\em independent of ${w}$}. The StyleGAN we used was pretrained by others, so this property is not some accident of our training regime. We show that there are image transformations StyleGAN will {\em not} produce in this fashion, so StyleGAN is not a generic image regression engine.   It is conceptually exciting that an image generator should ``know'' and represent intrinsic images. There may also be practical advantages to using a generative model to produce intrinsic images. The intrinsic images obtained from StyleGAN compare well both qualitatively and quantitatively with those obtained by using SOTA image regression techniques; but StyleGAN's intrinsic images are robust to relighting effects, unlike SOTA methods. |[2306.00987v1](http://arxiv.org/abs/2306.00987v1)|null|
|**2023-06-01**|**Diffusion Self-Guidance for Controllable Image Generation**|['cs.CV', 'cs.LG', 'stat.ML']|Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images. For results and an interactive demo, see our project page at https://dave.ml/selfguidance/ |[2306.00986v1](http://arxiv.org/abs/2306.00986v1)|null|
|**2023-06-01**|**Using generative AI to investigate medical imagery models and datasets**|['eess.IV', 'cs.CV', 'cs.LG']|AI models have shown promise in many medical imaging tasks. However, our ability to explain what signals these models have learned is severely lacking. Explanations are needed in order to increase the trust in AI-based models, and could enable novel scientific discovery by uncovering signals in the data that are not yet known to experts. In this paper, we present a method for automatic visual explanations leveraging team-based expertise by generating hypotheses of what visual signals in the images are correlated with the task. We propose the following 4 steps: (i) Train a classifier to perform a given task (ii) Train a classifier guided StyleGAN-based image generator (StylEx) (iii) Automatically detect and visualize the top visual attributes that the classifier is sensitive towards (iv) Formulate hypotheses for the underlying mechanisms, to stimulate future research. Specifically, we present the discovered attributes to an interdisciplinary panel of experts so that hypotheses can account for social and structural determinants of health. We demonstrate results on eight prediction tasks across three medical imaging modalities: retinal fundus photographs, external eye photographs, and chest radiographs. We showcase examples of attributes that capture clinically known features, confounders that arise from factors beyond physiological mechanisms, and reveal a number of physiologically plausible novel attributes. Our approach has the potential to enable researchers to better understand, improve their assessment, and extract new knowledge from AI-based models. Importantly, we highlight that attributes generated by our framework can capture phenomena beyond physiology or pathophysiology, reflecting the real world nature of healthcare delivery and socio-cultural factors. Finally, we intend to release code to enable researchers to train their own StylEx models and analyze their predictive tasks. |[2306.00985v1](http://arxiv.org/abs/2306.00985v1)|null|
|**2023-06-01**|**StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners**|['cs.CV']|We investigate the potential of learning visual representations using synthetic images generated by text-to-image models. This is a natural question in the light of the excellent performance of such models in generating high-quality images. We consider specifically the Stable Diffusion, one of the leading open source text-to-image models. We show that (1) when the generative model is configured with proper classifier-free guidance scale, training self-supervised methods on synthetic images can match or beat the real image counterpart; (2) by treating the multiple images generated from the same text prompt as positives for each other, we develop a multi-positive contrastive learning method, which we call StableRep. With solely synthetic images, the representations learned by StableRep surpass the performance of representations learned by SimCLR and CLIP using the same set of text prompts and corresponding real images, on large scale datasets. When we further add language supervision, StableRep trained with 20M synthetic images achieves better accuracy than CLIP trained with 50M real images. |[2306.00984v1](http://arxiv.org/abs/2306.00984v1)|null|
|**2023-06-01**|**StyleDrop: Text-to-Image Generation in Any Style**|['cs.CV', 'cs.AI']|Pre-trained large text-to-image models synthesize impressive images with an appropriate use of text prompts. However, ambiguities inherent in natural language and out-of-distribution effects make it hard to synthesize image styles, that leverage a specific design pattern, texture or material. In this paper, we introduce StyleDrop, a method that enables the synthesis of images that faithfully follow a specific style using a text-to-image model. The proposed method is extremely versatile and captures nuances and details of a user-provided style, such as color schemes, shading, design patterns, and local and global effects. It efficiently learns a new style by fine-tuning very few trainable parameters (less than $1\%$ of total model parameters) and improving the quality via iterative training with either human or automated feedback. Better yet, StyleDrop is able to deliver impressive results even when the user supplies only a single image that specifies the desired style. An extensive study shows that, for the task of style tuning text-to-image models, StyleDrop implemented on Muse convincingly outperforms other methods, including DreamBooth and textual inversion on Imagen or Stable Diffusion. More results are available at our project website: https://styledrop.github.io |[2306.00983v1](http://arxiv.org/abs/2306.00983v1)|null|
|**2023-06-01**|**SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds**|['cs.CV', 'cs.AI', 'cs.LG']|Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers. However, these models are large, with complex network architectures and tens of denoising iterations, making them computationally expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run diffusion models at scale. This is costly and has privacy implications, especially when user data is sent to a third party. To overcome these challenges, we present a generic approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in less than $2$ seconds. We achieve so by introducing efficient network architecture and improving step distillation. Specifically, we propose an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, we enhance the step distillation by exploring training strategies and introducing regularization from classifier-free guidance. Our extensive experiments on MS-COCO show that our model with $8$ denoising steps achieves better FID and CLIP scores than Stable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creation by bringing powerful text-to-image diffusion models to the hands of users. |[2306.00980v1](http://arxiv.org/abs/2306.00980v1)|**[link](https://github.com/huggingface/diffusers)**|
|**2023-06-01**|**AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration**|['cs.CL']|Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth). In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights. AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set; it also does not rely on any data layout reordering, maintaining the hardware efficiency. AWQ outperforms existing work on various language modeling, common sense QA, and domain-specific benchmarks. Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. We also implement efficient tensor core kernels with reorder-free online dequantization to accelerate AWQ, achieving a 1.45x speedup over GPTQ and is 1.85x faster than the cuBLAS FP16 implementation. Our method provides a turn-key solution to compress LLMs to 3/4 bits for efficient deployment. |[2306.00978v1](http://arxiv.org/abs/2306.00978v1)|**[link](https://github.com/mit-han-lab/llm-awq)**|
|**2023-06-01**|**Intriguing Properties of Text-guided Diffusion Models**|['cs.CV']|Text-guided diffusion models (TDMs) are widely applied but can fail unexpectedly. Common failures include: (i) natural-looking text prompts generating images with the wrong content, or (ii) different random samples of the latent variables that generate vastly different, and even unrelated, outputs despite being conditioned on the same text prompt. In this work, we aim to study and understand the failure modes of TDMs in more detail. To achieve this, we propose SAGE, an adversarial attack on TDMs that uses image classifiers as surrogate loss functions, to search over the discrete prompt space and the high-dimensional latent space of TDMs to automatically discover unexpected behaviors and failure cases in the image generation. We make several technical contributions to ensure that SAGE finds failure cases of the diffusion model, rather than the classifier, and verify this in a human study. Our study reveals four intriguing properties of TDMs that have not been systematically studied before: (1) We find a variety of natural text prompts producing images that fail to capture the semantics of input texts. We categorize these failures into ten distinct types based on the underlying causes. (2) We find samples in the latent space (which are not outliers) that lead to distorted images independent of the text prompt, suggesting that parts of the latent space are not well-structured. (3) We also find latent samples that lead to natural-looking images which are unrelated to the text prompt, implying a potential misalignment between the latent and prompt spaces. (4) By appending a single adversarial token embedding to an input prompt we can generate a variety of specified target objects, while only minimally affecting the CLIP score. This demonstrates the fragility of language representations and raises potential safety concerns. |[2306.00974v1](http://arxiv.org/abs/2306.00974v1)|**[link](https://github.com/qihao067/sage)**|
|**2023-06-01**|**Intelligent Grimm -- Open-ended Visual Storytelling via Latent Diffusion Models**|['cs.CV']|Generative models have recently exhibited exceptional capabilities in various scenarios, for example, image generation based on text description. In this work, we focus on the task of generating a series of coherent image sequence based on a given storyline, denoted as open-ended visual storytelling. We make the following three contributions: (i) to fulfill the task of visual storytelling, we introduce two modules into a pre-trained stable diffusion model, and construct an auto-regressive image generator, termed as StoryGen, that enables to generate the current frame by conditioning on both a text prompt and a preceding frame; (ii) to train our proposed model, we collect paired image and text samples by sourcing from various online sources, such as videos, E-books, and establish a data processing pipeline for constructing a diverse dataset, named StorySalon, with a far larger vocabulary than existing animation-specific datasets; (iii) we adopt a three-stage curriculum training strategy, that enables style transfer, visual context conditioning, and human feedback alignment, respectively. Quantitative experiments and human evaluation have validated the superiority of our proposed model, in terms of image quality, style consistency, content consistency, and visual-language alignment. We will make the code, model, and dataset publicly available to the research community. |[2306.00973v1](http://arxiv.org/abs/2306.00973v1)|**[link](https://github.com/haoningwu3639/StoryGen)**|
|**2023-06-01**|**ViCo: Detail-Preserving Visual Condition for Personalized Text-to-Image Generation**|['cs.CV', 'cs.AI']|Personalized text-to-image generation using diffusion models has recently been proposed and attracted lots of attention. Given a handful of images containing a novel concept (e.g., a unique toy), we aim to tune the generative model to capture fine visual details of the novel concept and generate photorealistic images following a text condition. We present a plug-in method, named ViCo, for fast and lightweight personalized generation. Specifically, we propose an image attention module to condition the diffusion process on the patch-wise visual semantics. We introduce an attention-based object mask that comes almost at no cost from the attention module. In addition, we design a simple regularization based on the intrinsic properties of text-image attention maps to alleviate the common overfitting degradation. Unlike many existing models, our method does not finetune any parameters of the original diffusion model. This allows more flexible and transferable model deployment. With only light parameter training (~6% of the diffusion U-Net), our method achieves comparable or even better performance than all state-of-the-art models both qualitatively and quantitatively. |[2306.00971v1](http://arxiv.org/abs/2306.00971v1)|**[link](https://github.com/haoosz/vico)**|
|**2023-06-01**|**Quadratic shape biases in three-dimensional halo intrinsic alignments**|['astro-ph.CO']|Understanding the nonlinear relation between the shapes of halos or galaxies and the surrounding matter distribution is essential in accurate modeling of their intrinsic alignments. In the perturbative treatment, such nonlinear relation of the intrinsic alignments appears as higher-order shape bias parameters. In this paper, we present accurate measurements of the quadratic shape bias parameters by combining the $\textit{full three-dimensional}$ power spectrum of the intrinsic alignments (i.e., without any projection) with the quadratic field method. In order to benefit from the full three-dimensional power spectrum we employ the spherical tensor decomposition of the three-dimensional shape field and measure their power spectra for the first time. In particular, we detect the vector and tensor power spectra in this basis, which cannot be explained by the widely-used nonlinear alignment model. Further, by cross-correlating the three-dimensional halo shape field with the quadratic shape bias operators from the initial condition of the same simulation to cancel cosmic variance, we effectively extract bispectrum information and detect quadratic shape bias parameters in the intrinsic alignments with high significance for the first time. We also compare these measurements with the prediction where quadratic shape biases are dynamically generated from the linear Lagrandian shape bias through the large-scale bulk flow. We find general agreement for all three biases with small deviations, which in practice could be negligible for the current photometric surveys. This implies that the advection prediction for the higher-order shape biases can be used as a prior in the cosmological analyses of intrinsic alignments. |[2306.00969v1](http://arxiv.org/abs/2306.00969v1)|null|
|**2023-06-01**|**GRES: Generalized Referring Expression Segmentation**|['cs.CV']|Referring Expression Segmentation (RES) aims to generate a segmentation mask for the object described by a given language expression. Existing classic RES datasets and methods commonly support single-target expressions only, i.e., one expression refers to one target object. Multi-target and no-target expressions are not considered. This limits the usage of RES in practice. In this paper, we introduce a new benchmark called Generalized Referring Expression Segmentation (GRES), which extends the classic RES to allow expressions to refer to an arbitrary number of target objects. Towards this, we construct the first large-scale GRES dataset called gRefCOCO that contains multi-target, no-target, and single-target expressions. GRES and gRefCOCO are designed to be well-compatible with RES, facilitating extensive experiments to study the performance gap of the existing RES methods on the GRES task. In the experimental study, we find that one of the big challenges of GRES is complex relationship modeling. Based on this, we propose a region-based GRES baseline ReLA that adaptively divides the image into regions with sub-instance clues, and explicitly models the region-region and region-language dependencies. The proposed approach ReLA achieves new state-of-the-art performance on the both newly proposed GRES and classic RES tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GRES. |[2306.00968v1](http://arxiv.org/abs/2306.00968v1)|**[link](https://github.com/henghuiding/ReLA)**|
|**2023-06-01**|**Free Stein kernels, free moment maps, and higher order derivatives**|['math.PR', 'math.OA']|In this work, we describe new constructions of free Stein kernels. Firstly, in dimension one, we propose a free analog to the construction of Stein kernels using moment maps as the one proposed by Fathi. This will be possible for a class of measures called the free moment measures via the notion of free moment map (convex functions), introduced in the free case by Bahr and Boschert. In a second time, we introduce the notion of higher-order free Stein kernels relative to a potential, which can be thought as the free counterpart of a recent and powerful idea introduced in the classical case by Fathi, and which generalize the notion of free Stein kernels by introducing higher-order derivatives of test functions (in our context noncommutative polynomials). We then focus our attention to the case of homothetic semicircular potentials. We prove as in the classical case, that their existence implies moments constraints. Finally, we relate these discrepancies to various metrics: the free (quadratic) Wasserstein distance, the relative free Fisher information along the Ornstein-Uhlenbeck flow or the relative non-microstates free entropy. Finally, as an important application, we provide new rates of convergences in the entropic free CLT under higher moments constraints. |[2306.00967v1](http://arxiv.org/abs/2306.00967v1)|null|
|**2023-06-01**|**The Hidden Language of Diffusion Models**|['cs.CV']|Text-to-image diffusion models have demonstrated an unparalleled ability to generate high-quality, diverse images from a textual concept (e.g., "a doctor", "love"). However, the internal process of mapping text to a rich visual representation remains an enigma. In this work, we tackle the challenge of understanding concept representations in text-to-image models by decomposing an input text prompt into a small set of interpretable elements. This is achieved by learning a pseudo-token that is a sparse weighted combination of tokens from the model's vocabulary, with the objective of reconstructing the images generated for the given concept. Applied over the state-of-the-art Stable Diffusion model, this decomposition reveals non-trivial and surprising structures in the representations of concepts. For example, we find that some concepts such as "a president" or "a composer" are dominated by specific instances (e.g., "Obama", "Biden") and their interpolations. Other concepts, such as "happiness" combine associated terms that can be concrete ("family", "laughter") or abstract ("friendship", "emotion"). In addition to peering into the inner workings of Stable Diffusion, our method also enables applications such as single-image decomposition to tokens, bias detection and mitigation, and semantic image manipulation. Our code will be available at: https://hila-chefer.github.io/Conceptor/ |[2306.00966v1](http://arxiv.org/abs/2306.00966v1)|**[link](https://github.com/hila-chefer/Conceptor)**|
|**2023-06-01**|**Cocktail: Mixing Multi-Modality Controls for Text-Conditional Image Generation**|['cs.CV']|Text-conditional diffusion models are able to generate high-fidelity images with diverse contents. However, linguistic representations frequently exhibit ambiguous descriptions of the envisioned objective imagery, requiring the incorporation of additional control signals to bolster the efficacy of text-guided diffusion models. In this work, we propose Cocktail, a pipeline to mix various modalities into one embedding, amalgamated with a generalized ControlNet (gControlNet), a controllable normalisation (ControlNorm), and a spatial guidance sampling method, to actualize multi-modal and spatially-refined control for text-conditional diffusion models. Specifically, we introduce a hyper-network gControlNet, dedicated to the alignment and infusion of the control signals from disparate modalities into the pre-trained diffusion model. gControlNet is capable of accepting flexible modality signals, encompassing the simultaneous reception of any combination of modality signals, or the supplementary fusion of multiple modality signals. The control signals are then fused and injected into the backbone model according to our proposed ControlNorm. Furthermore, our advanced spatial guidance sampling methodology proficiently incorporates the control signal into the designated region, thereby circumventing the manifestation of undesired objects within the generated image. We demonstrate the results of our method in controlling various modalities, proving high-quality synthesis and fidelity to multiple external signals. |[2306.00964v1](http://arxiv.org/abs/2306.00964v1)|null|
|**2023-06-01**|**Cosmology in the Lorentz gauge theory**|['gr-qc']|This proceeding is an introduction to cosmological applications of the Lorentz gauge theory. It provides the ingredients for a unique, though yet tentative $\Lambda$CDM theory cosmology. The emergence of spacetime is described by the spontaneous symmetry breaking called here the khronogenesis. Space is then associated with the field strength of the antiself-dual gauge potential, and gravity is associated with the self-dual field strength. In the cosmological setting, khronogenesis seems to predict inflation. It is shown that the Lorentz gauge theory allows the consistent description of spin currents which could have important roles in cosmological phenomenology. |[2306.00963v1](http://arxiv.org/abs/2306.00963v1)|null|
|**2023-06-01**|**JWST-JADES. Possible Population III signatures at z=10.6 in the halo of GN-z11**|['astro-ph.GA', 'astro-ph.CO']|Finding the first generation of stars formed out of pristine gas in the early Universe, known as Population III (PopIII) stars, is one of the most important goals of modern astrophysics. Recent models suggest that PopIII stars may form in pockets of pristine gas in the halo of more evolved galaxies. Here we present NIRSpec-IFU and NIRSpec-MSA observations of the region around GN-z11, an exceptionally luminous galaxy at $z=10.6$, which reveal a $>$5$\sigma$ detection of a feature consistent with being HeII$\lambda$1640 emission at the redshift of GN-z11. The very high equivalent width of the putative HeII emission in this clump (170 A), and the lack of metal lines, can be explained in terms of photoionisation by PopIII stars, while photoionisation by PopII stars is inconsistent with the data. It would also indicate that the putative PopIII stars likely have a top-heavy initial mass function (IMF), with an upper cutoff reaching at least 500 M$_\odot$. The PopIII bolometric luminosity inferred from the HeII line would be $\sim 2\times 10^{10}~L_\odot$, which (with a top-heavy IMF) would imply a total stellar mass formed in the burst of $\sim 6\times 10^{5}~M_\odot$. We find that photoionisation by the Active Galactic Nucleus (AGN) in GN-z11 cannot account for the HeII luminosity observed in the clump, but can potentially be responsible for additional HeII emission observed closer to GN-z11. We also consider the possibility of in-situ photoionisation by an accreting Direct Collapse Black Hole (DCBH) hosted by the HeII clump; we find that this scenario is less favoured, but it remains a possible alternative interpretation. We also report the detection of a Ly$\alpha$ halo stemming out of GN-z11 and extending out to $\sim$2 kpc, as well as resolved, funnel-shaped CIII] emission, likely tracing the ionisation cone of the AGN. |[2306.00953v1](http://arxiv.org/abs/2306.00953v1)|null|
|**2023-06-01**|**Speaker-specific Thresholding for Robust Imposter Identification in Unseen Speaker Recognition**|['eess.AS', 'cs.LG', 'cs.SD']|Speaker identification systems are deployed in diverse environments, often different from the lab conditions on which they are trained and tested. In this paper, first, we show the problem of generalization using fixed thresholds computed using the equal error rate metric. Secondly, we introduce a novel and generalizable speaker-specific thresholding technique for robust imposter identification in unseen speaker identification. We propose a speaker-specific adaptive threshold, which can be computed using the enrollment audio samples, for identifying imposters in unseen speaker identification. Furthermore, we show the efficacy of the proposed technique on VoxCeleb1, VCTK and the FFSVC 2022 datasets, beating the baseline fixed thresholding by up to 25%. Finally, we exhibit that the proposed algorithm is also generalizable, demonstrating its performance on ResNet50, ECAPA-TDNN and RawNet3 speaker encoders. |[2306.00952v1](http://arxiv.org/abs/2306.00952v1)|null|
|**2023-06-01**|**The ABJM Amplituhedron**|['hep-th']|In this paper we take a major step towards the construction and applications of an all-loop, all-multiplicity amplituhedron for three-dimensional planar $\mathcal{N}{=}6$ Chern-Simons matter theory, or the {\it ABJM amplituhedron}. We show that by simply changing the overall sign of the positive region of the original amplituhedron for four-dimensional planar $\mathcal{N}{=}4$ super-Yang-Mills (sYM) and performing a symplectic reduction, only three-dimensional kinematics in the middle sector of even-multiplicity survive. The resulting form of the geometry, combined with its parity images, gives the full loop integrand. This simple modification geometrically enforces the vanishing of odd-multiplicity cuts, and manifests the correct soft and two-particle unitarity cuts. Furthermore, the so-called ``bipartite structures" of four-point all-loop negative geometries also directly generalize to all multiplicities. We introduce a novel triangulation of the loop amplituhedron based on the kinematics of the tree region, resulting in local integrands tailored to ``prescriptive unitarity". This construction sheds a fascinating new light on the interplay between loop and tree amplituhedra for both ABJM and $\mathcal{N}{=}4$ sYM: the loop-geometry demands that the tree region must be dissected into \textit{chambers}, defined by the simultaneous positivity of maximal cuts. The loop geometry is then ``fibration" of the tree region. Using the new construction, we give explicit results of one-loop integrands up to ten points and two-loop integrands up to eight points by computing the canonical form of ABJM loop amplituhedron. |[2306.00951v1](http://arxiv.org/abs/2306.00951v1)|null|
|**2023-06-01**|**EEL: Efficiently Encoding Lattices for Reranking**|['cs.CL', 'cs.AI', 'cs.LG']|Standard decoding approaches for conditional text generation tasks typically search for an output hypothesis with high model probability, but this may not yield the best hypothesis according to human judgments of quality. Reranking to optimize for "downstream" metrics can better optimize for quality, but many metrics of interest are computed with pre-trained language models, which are slow to apply to large numbers of hypotheses. We explore an approach for reranking hypotheses by using Transformers to efficiently encode lattices of generated outputs, a method we call EEL. With a single Transformer pass over the entire lattice, we can approximately compute a contextualized representation of each token as if it were only part of a single hypothesis in isolation. We combine this approach with a new class of token-factored rerankers (TFRs) that allow for efficient extraction of high reranker-scoring hypotheses from the lattice. Empirically, our approach incurs minimal degradation error compared to the exponentially slower approach of encoding each hypothesis individually. When applying EEL with TFRs across three text generation tasks, our results show both substantial speedup compared to naive reranking and often better performance on downstream metrics than comparable approaches. |[2306.00947v1](http://arxiv.org/abs/2306.00947v1)|**[link](https://github.com/prasanns/eel-reranking)**|
|**2023-06-01**|**Exposing Attention Glitches with Flip-Flop Language Modeling**|['cs.LG', 'cs.CL']|Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs. |[2306.00946v1](http://arxiv.org/abs/2306.00946v1)|null|
|**2023-06-01**|**CS4ML: A general framework for active learning with arbitrary data based on Christoffel functions**|['cs.LG', 'cs.NA', 'math.NA']|We introduce a general framework for active learning in regression problems. Our framework extends the standard setup by allowing for general types of data, rather than merely pointwise samples of the target function. This generalization covers many cases of practical interest, such as data acquired in transform domains (e.g., Fourier data), vector-valued data (e.g., gradient-augmented data), data acquired along continuous curves, and, multimodal data (i.e., combinations of different types of measurements). Our framework considers random sampling according to a finite number of sampling measures and arbitrary nonlinear approximation spaces (model classes). We introduce the concept of generalized Christoffel functions and show how these can be used to optimize the sampling measures. We prove that this leads to near-optimal sample complexity in various important cases. This paper focuses on applications in scientific computing, where active learning is often desirable, since it is usually expensive to generate data. We demonstrate the efficacy of our framework for gradient-augmented learning with polynomials, Magnetic Resonance Imaging (MRI) using generative models and adaptive sampling for solving PDEs using Physics-Informed Neural Networks (PINNs). |[2306.00945v1](http://arxiv.org/abs/2306.00945v1)|null|
|**2023-06-01**|**Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance**|['cs.CV']|Creating a vivid video from the event or scenario in our imagination is a truly fascinating experience. Recent advancements in text-to-video synthesis have unveiled the potential to achieve this with prompts only. While text is convenient in conveying the overall scene context, it may be insufficient to control precisely. In this paper, we explore customized video generation by utilizing text as context description and motion structure (e.g. frame-wise depth) as concrete guidance. Our method, dubbed Make-Your-Video, involves joint-conditional video generation using a Latent Diffusion Model that is pre-trained for still image synthesis and then promoted for video generation with the introduction of temporal modules. This two-stage learning scheme not only reduces the computing resources required, but also improves the performance by transferring the rich concepts available in image datasets solely into video generation. Moreover, we use a simple yet effective causal attention mask strategy to enable longer video synthesis, which mitigates the potential quality degradation effectively. Experimental results show the superiority of our method over existing baselines, particularly in terms of temporal coherence and fidelity to users' guidance. In addition, our model enables several intriguing applications that demonstrate potential for practical usage. |[2306.00943v1](http://arxiv.org/abs/2306.00943v1)|null|
|**2023-06-01**|**Train Offline, Test Online: A Real Robot Learning Benchmark**|['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG']|Three challenges limit the progress of robot learning research: robots are expensive (few labs can participate), everyone uses different robots (findings do not generalize across labs), and we lack internet-scale robotics data. We take on these challenges via a new benchmark: Train Offline, Test Online (TOTO). TOTO provides remote users with access to shared robotic hardware for evaluating methods on common tasks and an open-source dataset of these tasks for offline training. Its manipulation task suite requires challenging generalization to unseen objects, positions, and lighting. We present initial results on TOTO comparing five pretrained visual representations and four offline policy learning baselines, remotely contributed by five institutions. The real promise of TOTO, however, lies in the future: we release the benchmark for additional submissions from any user, enabling easy, direct comparison to several methods without the need to obtain hardware or collect data. |[2306.00942v1](http://arxiv.org/abs/2306.00942v1)|**[link](https://github.com/AGI-Labs/toto_benchmark)**|
|**2023-06-01**|**STEVE-1: A Generative Model for Text-to-Behavior in Minecraft**|['cs.AI', 'cs.LG']|Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces an instruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1, demonstrating that the unCLIP approach, utilized in DALL-E 2, is also effective for creating instruction-following sequential decision-making agents. STEVE-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, bypassing the need for costly human text annotations. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 costs just $60 to train and can follow a wide range of short-horizon open-ended text and visual instructions in Minecraft. STEVE-1 sets a new bar for open-ended instruction following in Minecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming previous baselines. We provide experimental evidence highlighting key factors for downstream performance, including pretraining, classifier-free guidance, and data scaling. All resources, including our model weights, training scripts, and evaluation tools are made available for further research. |[2306.00937v1](http://arxiv.org/abs/2306.00937v1)|null|
|**2023-06-01**|**Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features**|['cs.CR', 'cs.LG']|The black-box nature of complex Neural Network (NN)-based models has hindered their widespread adoption in security domains due to the lack of logical explanations and actionable follow-ups for their predictions. To enhance the transparency and accountability of Graph Neural Network (GNN) security models used in system provenance analysis, we propose PROVEXPLAINER, a framework for projecting abstract GNN decision boundaries onto interpretable feature spaces.   We first replicate the decision-making process of GNNbased security models using simpler and explainable models such as Decision Trees (DTs). To maximize the accuracy and fidelity of the surrogate models, we propose novel graph structural features founded on classical graph theory and enhanced by extensive data study with security domain knowledge. Our graph structural features are closely tied to problem-space actions in the system provenance domain, which allows the detection results to be explained in descriptive, human language. PROVEXPLAINER allowed simple DT models to achieve 95% fidelity to the GNN on program classification tasks with general graph structural features, and 99% fidelity on malware detection tasks with a task-specific feature package tailored for direct interpretation. The explanations for malware classification are demonstrated with case studies of five real-world malware samples across three malware families. |[2306.00934v1](http://arxiv.org/abs/2306.00934v1)|null|
|**2023-06-01**|**"Let's not Quote out of Context": Unified Vision-Language Pretraining for Context Assisted Image Captioning**|['cs.CV', 'cs.CL']|Well-formed context aware image captions and tags in enterprise content such as marketing material are critical to ensure their brand presence and content recall. Manual creation and updates to ensure the same is non trivial given the scale and the tedium towards this task. We propose a new unified Vision-Language (VL) model based on the One For All (OFA) model, with a focus on context-assisted image captioning where the caption is generated based on both the image and its context. Our approach aims to overcome the context-independent (image and text are treated independently) nature of the existing approaches. We exploit context by pretraining our model with datasets of three tasks: news image captioning where the news article is the context, contextual visual entailment, and keyword extraction from the context. The second pretraining task is a new VL task, and we construct and release two datasets for the task with 1.1M and 2.2K data instances. Our system achieves state-of-the-art results with an improvement of up to 8.34 CIDEr score on the benchmark news image captioning datasets. To the best of our knowledge, ours is the first effort at incorporating contextual information in pretraining the models for the VL tasks. |[2306.00931v1](http://arxiv.org/abs/2306.00931v1)|null|
|**2023-06-01**|**Large Charge 't Hooft Limit of $\mathcal{N}=4$ Super-Yang-Mills**|['hep-th']|The planar integrability of $\mathcal{N}=4$ super-Yang-Mills (SYM) is the cornerstone for numerous exact observables. We show that the large charge sector of the ${\rm SU}(2)$ $\mathcal{N}=4$ SYM provides another interesting solvable corner which exhibits striking similarities despite being far from the planar limit. We study non-BPS operators obtained by small deformations of half-BPS operators with $R$-charge $J$ in the limit $J\to\infty$ with $\lambda_{J}\equiv g_{\rm YM}^2 J/2$ fixed. The dynamics in this {\it large charge 't Hooft limit} is constrained by a centrally-extended $\mathfrak{psu}(2|2)^2$ symmetry that played a crucial role for the planar integrability. To the leading order in $1/J$, the spectrum is fully fixed by this symmetry, manifesting the magnon dispersion relation familiar from the planar limit, while it is constrained up to a few constants at the next order. We also determine the structure constant of two large charge operators and the Konishi operator, revealing a rich structure interpolating between the perturbative series at weak coupling and the worldline instantons at strong coupling. In addition we compute heavy-heavy-light-light (HHLL) four-point functions of half-BPS operators in terms of resummed conformal integrals and recast them into an integral form reminiscent of the hexagon formalism in the planar limit. For general ${\rm SU}(N)$ gauge groups, we study integrated HHLL correlators by supersymmetric localization and identify a dual matrix model of size $J/2$ that reproduces our large charge result at $N=2$. Finally we discuss a relation to the physics on the Coulomb branch and explain how the dilaton Ward identity emerges from a limit of the conformal block expansion. We comment on generalizations including the large spin 't Hooft limit, the combined large $N$-large $J$ limits, and applications to general $\mathcal{N}=2$ superconformal field theories. |[2306.00929v1](http://arxiv.org/abs/2306.00929v1)|null|
|**2023-06-01**|**ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER**|['cs.CL', 'cs.AI', 'cs.IR']|Complex Named Entity Recognition (NER) is the task of detecting linguistically complex named entities in low-context text. In this paper, we present ACLM Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach based on conditional generation to address the data scarcity problem in low-resource complex NER. ACLM alleviates the context-entity mismatch issue, a problem existing NER data augmentation techniques suffer from and often generates incoherent augmentations by placing complex named entities in the wrong context. ACLM builds on BART and is optimized on a novel text reconstruction or denoising task - we use selective masking (aided by attention maps) to retain the named entities and certain keywords in the input sentence that provide contextually relevant additional knowledge or hints about the named entities. Compared with other data augmentation strategies, ACLM can generate more diverse and coherent augmentations preserving the true word sense of complex entities in the sentence. We demonstrate the effectiveness of ACLM both qualitatively and quantitatively on monolingual, cross-lingual, and multilingual complex NER across various low-resource settings. ACLM outperforms all our neural baselines by a significant margin (1%-36%). In addition, we demonstrate the application of ACLM to other domains that suffer from data scarcity (e.g., biomedical). In practice, ACLM generates more effective and factual augmentations for these domains than prior methods. Code: https://github.com/Sreyan88/ACLM |[2306.00928v1](http://arxiv.org/abs/2306.00928v1)|null|
|**2023-06-01**|**Second Sight: Using brain-optimized encoding models to align image distributions with human brain activity**|['q-bio.NC', 'cs.CV', 'cs.LG']|Two recent developments have accelerated progress in image reconstruction from human brain activity: large datasets that offer samples of brain activity in response to many thousands of natural scenes, and the open-sourcing of powerful stochastic image-generators that accept both low- and high-level guidance. Most work in this space has focused on obtaining point estimates of the target image, with the ultimate goal of approximating literal pixel-wise reconstructions of target images from the brain activity patterns they evoke. This emphasis belies the fact that there is always a family of images that are equally compatible with any evoked brain activity pattern, and the fact that many image-generators are inherently stochastic and do not by themselves offer a method for selecting the single best reconstruction from among the samples they generate. We introduce a novel reconstruction procedure (Second Sight) that iteratively refines an image distribution to explicitly maximize the alignment between the predictions of a voxel-wise encoding model and the brain activity patterns evoked by any target image. We show that our process converges on a distribution of high-quality reconstructions by refining both semantic content and low-level image details across iterations. Images sampled from these converged image distributions are competitive with state-of-the-art reconstruction algorithms. Interestingly, the time-to-convergence varies systematically across visual cortex, with earlier visual areas generally taking longer and converging on narrower image distributions, relative to higher-level brain areas. Second Sight thus offers a succinct and novel method for exploring the diversity of representations across visual brain areas. |[2306.00927v1](http://arxiv.org/abs/2306.00927v1)|null|
|**2023-06-01**|**Inserting Anybody in Diffusion Models via Celeb Basis**|['cs.CV']|Exquisite demand exists for customizing the pretrained large text-to-image model, $\textit{e.g.}$, Stable Diffusion, to generate innovative concepts, such as the users themselves. However, the newly-added concept from previous customization methods often shows weaker combination abilities than the original ones even given several images during training. We thus propose a new personalization method that allows for the seamless integration of a unique individual into the pre-trained diffusion model using just $\textbf{one facial photograph}$ and only $\textbf{1024 learnable parameters}$ under $\textbf{3 minutes}$. So as we can effortlessly generate stunning images of this person in any pose or position, interacting with anyone and doing anything imaginable from text prompts. To achieve this, we first analyze and build a well-defined celeb basis from the embedding space of the pre-trained large text encoder. Then, given one facial photo as the target identity, we generate its own embedding by optimizing the weight of this basis and locking all other parameters. Empowered by the proposed celeb basis, the new identity in our customized model showcases a better concept combination ability than previous personalization methods. Besides, our model can also learn several new identities at once and interact with each other where the previous customization model fails to. The code will be released. |[2306.00926v1](http://arxiv.org/abs/2306.00926v1)|**[link](https://github.com/ygtxr1997/celebbasis)**|
|**2023-06-01**|**Expert covariational reasoning resources in physics graphing tasks**|['physics.ed-ph']|Developing and making sense of quantitative models is a core practice of physics. Covariational reasoning -- considering how the changes in one quantity affect changes in another, related quantity -- is an essential part of modeling quantitatively. Covariational reasoning has been studied widely in mathematics education research, but has only begun to be used in physics education research. We present evidence from three studies of 25 individual interviews with physics experts, in which the experts were asked to reason out loud while generating graphical models. We analyze the studies through the lens of covariational reasoning frameworks from mathematics education research, and determine that the frameworks are useful but do not completely describe the covariational reasoning of the physics experts we interviewed. From our data, we identified reasoning patterns that are not described in the mathematics education research that, together with the mathematics covariational reasoning frameworks, begin to characterize physics covariational reasoning. |[2306.00921v1](http://arxiv.org/abs/2306.00921v1)|null|
|**2023-06-01**|**Understanding Social Context from Smartphone Sensing: Generalization Across Countries and Daily Life Moments**|['cs.HC', 'cs.CY']|Understanding and longitudinally tracking the social context of people help in understanding their behavior and mental well-being better. Hence, instead of burdensome questionnaires, some studies used passive smartphone sensors to infer social context with machine learning models. However, the few studies that have been done up to date have focused on unique, situated contexts (i.e., when eating or drinking) in one or two countries, hence limiting the understanding of the inference in terms of generalization to (i) everyday life occasions and (ii) different countries. In this paper, we used a novel, large-scale, and multimodal smartphone sensing dataset with over 216K self-reports collected from over 580 participants in five countries (Mongolia, Italy, Denmark, UK, Paraguay), first to understand whether social context inference (i.e., alone or not) is feasible with sensor data, and then, to know how behavioral and country-level diversity affects the inference. We found that (i) sensor features from modalities such as activity, location, app usage, Bluetooth, and WiFi could be informative of social context; (ii) partially personalized multi-country models (trained and tested with data from all countries) and country-specific models (trained and tested within countries) achieved similar accuracies in the range of 80%-90%; and (iii) models do not generalize well to unseen countries regardless of geographic similarity. |[2306.00919v1](http://arxiv.org/abs/2306.00919v1)|null|
|**2023-06-01**|**Novel CMB constraints on the $$ parameter in alpha-attractor models**|['astro-ph.CO', 'gr-qc', 'hep-ph', 'hep-th']|Cosmological $\alpha$-attractors are a compelling class of inflationary models. They lead to universal predictions for large-scale observables, broadly independent from the functional form of the inflaton potential. In this work we derive improved analytical predictions for the large-scale observables, whose dependence on the duration of reheating and the parameter $\alpha$ is made explicit. We compare these with Planck and BICEP/Keck 2018 data in the framework of a Bayesian study, employing uniform logarithmic and linear priors for $\alpha$. Our improved universal predictions allow direct constraints on the duration of reheating. Furthermore, while it is well-known that CMB constraints on the tensor-to-scalar ratio can be used to place an upper bound on the $\alpha$ parameter, we demonstrate that including the $\alpha$-dependence of the scalar spectral tilt yields novel constraints on $\alpha$. In particular, for small $\alpha$, the scalar spectral tilt scales with $\log_{10}\alpha$, regardless of the specific potential shape. For decreasing $\alpha$, this eventually puts the models in tension with CMB measurements, bounding the magnitude of $\alpha$ from below. Therefore, in addition to the upper bound from the tensor-to-scalar ratio, we derive the first lower bound on the magnitude of $\alpha$ for $\alpha$-attractor T-models, $\log_{10}{\alpha} = -4.2^{+5.4}_{-8.6}$ at $95\%$ C.L. . |[2306.00918v1](http://arxiv.org/abs/2306.00918v1)|null|
|**2023-06-01**|**Conditioning Diffusion Models via Attributes and Semantic Masks for Face Generation**|['cs.CV']|Deep generative models have shown impressive results in generating realistic images of faces. GANs managed to generate high-quality, high-fidelity images when conditioned on semantic masks, but they still lack the ability to diversify their output. Diffusion models partially solve this problem and are able to generate diverse samples given the same condition. In this paper, we propose a multi-conditioning approach for diffusion models via cross-attention exploiting both attributes and semantic masks to generate high-quality and controllable face images. We also studied the impact of applying perceptual-focused loss weighting into the latent space instead of the pixel space. Our method extends the previous approaches by introducing conditioning on more than one set of features, guaranteeing a more fine-grained control over the generated face images. We evaluate our approach on the CelebA-HQ dataset, and we show that it can generate realistic and diverse samples while allowing for fine-grained control over multiple attributes and semantic regions. Additionally, we perform an ablation study to evaluate the impact of different conditioning strategies on the quality and diversity of the generated images. |[2306.00914v1](http://arxiv.org/abs/2306.00914v1)|null|
|**2023-06-01**|**Introduction to Generalized Global Symmetries in QFT and Particle Physics**|['hep-ph']|Generalized symmetries (also known as categorical symmetries) is a newly developing technique for studying quantum field theories. It has given us new insights into the structure of QFT and many new powerful tools that can be applied to the study of particle phenomenology. In these notes we give an exposition to the topic of generalized/categorical symmetries for high energy phenomenologists although the topics covered may be useful to the broader physics community. Here we describe generalized symmetries without the use of category theory and pay particular attention to the introduction of discrete symmetries and their gauging. |[2306.00912v1](http://arxiv.org/abs/2306.00912v1)|null|
|**2023-06-01**|**Dissipation in Hydrodynamics from Micro- to Macroscale: Wisdom from Boltzmann and Stochastic Thermodynamics**|['cond-mat.stat-mech', 'physics.flu-dyn']|We show that macroscopic irreversible thermodynamics for viscous fluids can be derived from exact information-theoretic thermodynamic identities valid at the microscale. Entropy production in particular is a measure of the loss of many-particle correlations in the same way in which it measures the loss of system-reservoirs correlations in stochastic thermodynamics (ST). More specifically, we first show that boundary conditions at the macroscopic level define a natural decomposition of the entropy production rate (EPR) in terms of thermodynamic forces multiplying their conjugated currents, as well as a change in suitable nonequilibrium potential that acts as a Lyapunov function in absence of forces. At the microscale, for isolated Hamiltonian systems, we identify the exact identities at the origin of these dissipative contributions. Indeed, the molecular chaos hypothesis, which gives rise to the Boltzmann equation at the mesoscale, leads to a positive rate of loss of many-particle correlations which we identify with the Boltzmann entropy production rate. By generalizing the Boltzmann equation to account for boundaries with nonuniform temperature and nonzero velocity, and resorting to the Chapman--Enskog expansion, we reproduce the macroscopic theory we started from. We also show that an equivalent mesoscale description in terms of a linearized Boltzmann equation obeys local detailed balance (LDB) and thus reproduces a ST theory. Our work unambiguously demonstrates the information-theoretical origin of thermodynamic notions of entropy and dissipation in macroscale irreversible thermodynamics. |[2306.00911v1](http://arxiv.org/abs/2306.00911v1)|null|
|**2023-06-01**|**A General Framework for Regression with Mismatched Data Based on Mixture Modeling**|['stat.ME']|Data sets obtained from linking multiple files are frequently affected by mismatch error, as a result of non-unique or noisy identifiers used during record linkage. Accounting for such mismatch error in downstream analysis performed on the linked file is critical to ensure valid statistical inference. In this paper, we present a general framework to enable valid post-linkage inference in the challenging secondary analysis setting in which only the linked file is given. The proposed framework covers a wide selection of statistical models and can flexibly incorporate additional information about the underlying record linkage process. Specifically, we propose a mixture model for pairs of linked records whose two components reflect distributions conditional on match status, i.e., correct match or mismatch. Regarding inference, we develop a method based on composite likelihood and the EM algorithm as well as an extension towards a fully Bayesian approach. Extensive simulations and several case studies involving contemporary record linkage applications corroborate the effectiveness of our framework. |[2306.00909v1](http://arxiv.org/abs/2306.00909v1)|null|
|**2023-06-01**|**MOSAIC: Masked Optimisation with Selective Attention for Image Reconstruction**|['cs.CV', 'eess.IV']|Compressive sensing (CS) reconstructs images from sub-Nyquist measurements by solving a sparsity-regularized inverse problem. Traditional CS solvers use iterative optimizers with hand crafted sparsifiers, while early data-driven methods directly learn an inverse mapping from the low-dimensional measurement space to the original image space. The latter outperforms the former, but is restrictive to a pre-defined measurement domain. More recent, deep unrolling methods combine traditional proximal gradient methods and data-driven approaches to iteratively refine an image approximation. To achieve higher accuracy, it has also been suggested to learn both the sampling matrix, and the choice of measurement vectors adaptively. Contrary to the current trend, in this work we hypothesize that a general inverse mapping from a random set of compressed measurements to the image domain exists for a given measurement basis, and can be learned. Such a model is single-shot, non-restrictive and does not parametrize the sampling process. To this end, we propose MOSAIC, a novel compressive sensing framework to reconstruct images given any random selection of measurements, sampled using a fixed basis. Motivated by the uneven distribution of information across measurements, MOSAIC incorporates an embedding technique to efficiently apply attention mechanisms on an encoded sequence of measurements, while dispensing the need to use unrolled deep networks. A range of experiments validate our proposed architecture as a promising alternative for existing CS reconstruction methods, by achieving the state-of-the-art for metrics of reconstruction accuracy on standard datasets. |[2306.00906v1](http://arxiv.org/abs/2306.00906v1)|null|
|**2023-06-01**|**T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation**|['cs.CL', 'cs.AI', 'cs.CV', 'I.2.6']|Warning: This paper contains several contents that may be toxic, harmful, or offensive.   In the last few years, text-to-image generative models have gained remarkable success in generating images with unprecedented quality accompanied by a breakthrough of inference speed. Despite their rapid progress, human biases that manifest in the training examples, particularly with regard to common stereotypical biases, like gender and skin tone, still have been found in these generative models. In this work, we seek to measure more complex human biases exist in the task of text-to-image generations. Inspired by the well-known Implicit Association Test (IAT) from social psychology, we propose a novel Text-to-Image Association Test (T2IAT) framework that quantifies the implicit stereotypes between concepts and valence, and those in the images. We replicate the previously documented bias tests on generative models, including morally neutral tests on flowers and insects as well as demographic stereotypical tests on diverse social attributes. The results of these experiments demonstrate the presence of complex stereotypical behaviors in image generations. |[2306.00905v1](http://arxiv.org/abs/2306.00905v1)|null|
|**2023-06-01**|**Periodicity of ideals of minors in free resolutions**|['math.AC', '13D02']|We study the asymptotic behavior of the ideals of minors in minimal free resolutions over local rings. In particular, we prove that such ideals are eventually 2-periodic over complete intersections and Golod rings. We also establish general results on the stable behavior of ideals of minors in any infinite minimal free resolution. These ideals have intimate connections to trace ideals and cohomology annihilators. Constraints on the stable values attained by the ideals of minors in many situations are obtained, and they can be explicitly computed in certain cases. |[2306.00903v1](http://arxiv.org/abs/2306.00903v1)|null|
|**2023-06-01**|**Suppression of chaos in a partially driven recurrent neural network**|['q-bio.NC', 'cond-mat.dis-nn', 'nlin.CD']|The dynamics of recurrent neural networks (RNNs), and particularly their response to inputs, play a critical role in information processing. In many applications of RNNs, only a specific subset of the neurons generally receive inputs. However, it remains to be theoretically clarified how the restriction of the input to a specific subset of neurons affects the network dynamics. Considering recurrent neural networks with such restricted input, we investigate how the proportion, $p$, of the neurons receiving inputs (the "inputs neurons") and a quantity, $\xi$, representing the strength of the input signals affect the dynamics by analytically deriving the conditional maximum Lyapunov exponent. Our results show that for sufficiently large $p$, the maximum Lyapunov exponent decreases monotonically as a function of $\xi$, indicating the suppression of chaos, but if $p$ is smaller than a critical threshold, $p_c$, even significantly amplified inputs cannot suppress spontaneous chaotic dynamics. Furthermore, although the value of $p_c$ is seemingly dependent on several model parameters, such as the sparseness and strength of recurrent connections, it is proved to be intrinsically determined solely by the strength of chaos in spontaneous activity of the RNN. This is to say, despite changes in these model parameters, it is possible to represent the value of $p_c$ as a common invariant function by appropriately scaling these parameters to yield the same strength of spontaneous chaos. Our study suggests that if $p$ is above $p_c$, we can bring the neural network to the edge of chaos, thereby maximizing its information processing capacity, by adjusting $\xi$. |[2306.00900v1](http://arxiv.org/abs/2306.00900v1)|null|
|**2023-06-01**|**Boundary conditions and universal finite-size scaling for the hierarchical $|\varphi|^4$ model in dimensions 4 and higher**|['math-ph', 'math.MP', 'math.PR', '82B27, 82B28, 60K35']|We analyse and clarify the finite-size scaling of the weakly-coupled hierarchical $n$-component $|\varphi|^4$ model for all integers $n \ge 1$ in all dimensions $d\ge 4$, for both free and periodic boundary conditions. For $d>4$, we prove that for a volume of size $R^{d}$ with periodic boundary conditions the infinite-volume critical point is an effective finite-volume critical point, whereas for free boundary conditions the effective critical point is shifted smaller by an amount of order $R^{-2}$. For both boundary conditions, the average field has the same non-Gaussian limit within a critical window of width $R^{-d/2}$ around the effective critical point, and in that window we compute the universal scaling profile for the susceptibility. In contrast, and again for both boundary conditions, the average field has a massive Gaussian limit when above the effective critical point by an amount $R^{-2}$. In particular, at the infinite-volume critical point the susceptibility scales as $R^{d/2}$ for periodic boundary conditions and as $R^{2}$ for free boundary conditions. We identify a mass generation mechanism for free boundary conditions that is responsible for this distinction and which we believe has wider validity, in particular to Euclidean (non-hierarchical) models on $\mathbb{Z}^d$ in dimensions $d \ge 4$. For $d=4$ we prove a similar picture with logarithmic corrections. Our analysis is based on the rigorous renormalisation group method of Bauerschmidt, Brydges and Slade, which we improve and extend. |[2306.00896v1](http://arxiv.org/abs/2306.00896v1)|null|
|**2023-06-01**|**Efficient Temporal Butterfly Counting and Enumeration on Temporal Bipartite Graphs**|['cs.DS']|Bipartite graphs model relationships between two different sets of entities, like actor-movie, user-item, and author-paper. The butterfly, a 4-vertices 4-edges $2\times 2$ bi-clique, is the simplest cohesive motif in a bipartite graph and is the fundamental component of higher-order substructures. Counting and enumerating the butterflies offer significant benefits across various applications, including fraud detection, graph embedding, and community search. While the corresponding motif, the triangle, in the unipartite graphs has been widely studied in both static and temporal settings, the extension of butterfly to temporal bipartite graphs remains unexplored. In this paper, we investigate the temporal butterfly counting and enumeration problem: count and enumerate the butterflies whose edges establish following a certain order within a given duration. Towards efficient computation, we devise a non-trivial baseline rooted in the state-of-the-art butterfly counting algorithm on static graphs, further, explore the intrinsic property of the temporal butterfly, and optimize the process with a compact data structure and smart pruning strategies. The time complexity is proved to be significantly reduced without compromising on space efficiency. In addition, we generalize our algorithms to practical streaming settings and multi-core computing architectures. Our extensive experiments on 11 large-scale real-world datasets demonstrate the efficiency and scalability of our solutions. |[2306.00893v1](http://arxiv.org/abs/2306.00893v1)|null|
|**2023-06-01**|**LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day**|['cs.CV', 'cs.CL']|Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model. |[2306.00890v1](http://arxiv.org/abs/2306.00890v1)|null|
|**2023-06-01**|**OpenPI-C: A Better Benchmark and Stronger Baseline for Open-Vocabulary State Tracking**|['cs.CL']|Open-vocabulary state tracking is a more practical version of state tracking that aims to track state changes of entities throughout a process without restricting the state space and entity space. OpenPI is to date the only dataset annotated for open-vocabulary state tracking. However, we identify issues with the dataset quality and evaluation metric. For the dataset, we categorize 3 types of problems on the procedure level, step level and state change level respectively, and build a clean dataset OpenPI-C using multiple rounds of human judgment. For the evaluation metric, we propose a cluster-based metric to fix the original metric's preference for repetition.   Model-wise, we enhance the seq2seq generation baseline by reinstating two key properties for state tracking: temporal dependency and entity awareness. The state of the world after an action is inherently dependent on the previous state. We model this dependency through a dynamic memory bank and allow the model to attend to the memory slots during decoding. On the other hand, the state of the world is naturally a union of the states of involved entities. Since the entities are unknown in the open-vocabulary setting, we propose a two-stage model that refines the state change prediction conditioned on entities predicted from the first stage. Empirical results show the effectiveness of our proposed model especially on the cluster-based metric. The code and data are released at https://github.com/shirley-wu/openpi-c |[2306.00887v1](http://arxiv.org/abs/2306.00887v1)|**[link](https://github.com/shirley-wu/openpi-c)**|
|**2023-06-01**|**A quantum-enhanced support vector machine for galaxy classification**|['astro-ph.GA']|Galaxy morphology, a key tracer of the evolution of a galaxy's physical structure, has motivated extensive research on machine learning techniques for efficient and accurate galaxy classification. The emergence of quantum computers has generated optimism about the potential for significantly improving the accuracy of such classifications by leveraging the large dimensionality of quantum Hilbert space. This paper presents a quantum-enhanced support vector machine algorithm for classifying galaxies based on their morphology. The algorithm requires the computation of a kernel matrix, a task that is performed on a simulated quantum computer using a quantum circuit conjectured to be intractable on classical computers. The result shows similar performance between classical and quantum-enhanced support vector machine algorithms. For a training size of $40$k, the receiver operating characteristic curve for differentiating ellipticals and spirals has an under-curve area (ROC AUC) of $0.946\pm 0.005$ for both classical and quantum-enhanced algorithms. This investigation is among the very first applications of quantum machine learning in astronomy and highlights their potential for further application in this field. |[2306.00881v1](http://arxiv.org/abs/2306.00881v1)|null|
|**2023-06-01**|**Covariance in Non-Commutative Algebra**|['math.GM']|Consider vector space over non-commutative division algebra. Set of automorphisms of this vector space is group $GL$. Group $GL$ acts on the set of bases of vector space (basis manifold) single transitive and generates active representation. Twin representation on basis manifold is called passive representation. There is no automorphism associated with passive transformation. However passive transformation generates transformation of coordinates of vector with respect to basis. If we consider homomorphism of vector space $V$ into vector space $W$, then we can learn how passive transformation in vector space $V$ generates transformation of coordinates of vector in vector space $W$. Vector in vector space $W$ is called geometric object in vector space $V$. Covariance principle states that geometric object does not depend on the choice of basis. I considered transformation of coordinates of vector and polylinear map. |[2306.00880v1](http://arxiv.org/abs/2306.00880v1)|null|
|**2023-06-01**|**Domain Generalization for Domain-Linked Classes**|['cs.LG', 'cs.AI']|Domain generalization (DG) focuses on transferring domain-invariant knowledge from multiple source domains (available at train time) to an, a priori, unseen target domain(s). This requires a class to be expressed in multiple domains for the learning algorithm to break the spurious correlations between domain and class. However, in the real-world, classes may often be domain-linked, i.e. expressed only in a specific domain, which leads to extremely poor generalization performance for these classes. In this work, we aim to learn generalizable representations for these domain-linked classes by transferring domain-invariant knowledge from classes expressed in multiple source domains (domain-shared classes). To this end, we introduce this task to the community and propose a Fair and cONtrastive feature-space regularization algorithm for Domain-linked DG, FOND. Rigorous and reproducible experiments with baselines across popular DG tasks demonstrate our method and its variants' ability to accomplish state-of-the-art DG results for domain-linked classes. We also provide practical insights on data conditions that increase domain-linked class generalizability to tackle real-world data scarcity. |[2306.00879v1](http://arxiv.org/abs/2306.00879v1)|null|
|**2023-06-01**|**Fusions of the Tensor Square of a Strongly Regular Graph**|['math.CO', 'math.AC', 'Primary 05E30, Secondary 05C25']|In this paper we determine all fusions of the association scheme $\mathcal{A} \otimes \mathcal{A}$, where $\mathcal{A}$ is the symmetric rank $3$ association scheme corresponding to a strongly regular graph. This includes both guaranteed fusions, which are fusions for all symmetric rank $3$ association schemes $\mathcal{A}$, and specific case fusions, which only exist under restrictions on the parameters of the association scheme. Along the way we will determine the fusions of wreath products of strongly regular graphs and the fusions of the tensor square of a symmetric rank $3$ table algebra. This extends recent work of the authors and Meagher, which solved the same problem for the generalized Hamming scheme $H(2,\mathcal{A})$ of the association scheme obtained from a strongly regular graph. The main results of this article show (1) the families of strongly regular graphs for which $\mathcal{A} \otimes \mathcal{A}$ has a special case fusion are the same families for which $H(2,\mathcal{A})$ has a special case fusion; and (2) the imprimitive strongly regular graphs are the only family of strongly regular graphs for which the wreath product $\mathcal{A} \wr \mathcal{A}$ has a special case fusion. |[2306.00878v1](http://arxiv.org/abs/2306.00878v1)|null|
|**2023-06-01**|**Complex Arnol'd-Liouville maps**|['math.DS', '37J05, 37J35, 37J40, 70H05, 70H08, 70H15']|We discuss the holomorphic properties of the complex continuation of the classical Arnol'd-Liouville action-angle variables for real analytic 1 degree--of--freedom Hamiltonian systems depending on external parameters in suitable `generic standard form', with particular regard to the behaviour near separatrices. |[2306.00875v1](http://arxiv.org/abs/2306.00875v1)|null|
|**2023-06-01**|**A general-purpose single-photon-based quantum computing platform**|['quant-ph']|Quantum computing aims at exploiting quantum phenomena to efficiently perform computations that are unfeasible even for the most powerful classical supercomputers. Among the promising technological approaches, photonic quantum computing offers the advantages of low decoherence, information processing with modest cryogenic requirements, and native integration with classical and quantum networks. To date, quantum computing demonstrations with light have implemented specific tasks with specialized hardware, notably Gaussian Boson Sampling which permitted quantum computational advantage to be reached. Here we report a first user-ready general-purpose quantum computing prototype based on single photons. The device comprises a high-efficiency quantum-dot single-photon source feeding a universal linear optical network on a reconfigurable chip for which hardware errors are compensated by a machine-learned transpilation process. Our full software stack allows remote control of the device to perform computations via logic gates or direct photonic operations. For gate-based computation we benchmark one-, two- and three-qubit gates with state-of-the art fidelities of $99.6\pm0.1 \%$, $93.8\pm0.6 \%$ and $86\pm1.2 \%$ respectively. We also implement a variational quantum eigensolver, which we use to calculate the energy levels of the hydrogen molecule with high accuracy. For photon native computation, we implement a classifier algorithm using a $3$-photon-based quantum neural network and report a first $6$-photon Boson Sampling demonstration on a universal reconfigurable integrated circuit. Finally, we report on a first heralded 3-photon entanglement generation, a key milestone toward measurement-based quantum computing. |[2306.00874v1](http://arxiv.org/abs/2306.00874v1)|null|
|**2023-06-01**|**Constraining Palatini gravity with GR-independent equations of state**|['gr-qc', 'astro-ph.HE', 'hep-ph', 'nucl-th']|We demonstrate how to construct GR-independent equations of state. We emphasize the importance of using theory-based principles instead of relying solely on astrophysical observables and General Relativity (GR). We build a set of equations of state based on first principles, including chiral perturbation theory and perturbation theory in quantum chromodynamics. Interpolation methods are employed to assume thermodynamic stability and causality in the intermediate region. These equations of state are then used to constrain quadratic Palatini $f(\mathcal R)$ gravity, indicating that the parameter lies within the range $-6.47 \lesssim \beta \lesssim 1.99$ km$^2$. Additionally, we briefly discuss the problem of phase transitions and twin stars. |[2306.00870v1](http://arxiv.org/abs/2306.00870v1)|null|
|**2023-06-01**|**Blockchain-based Decentralized Co-governance: Innovations and Solutions for Sustainable Crowdfunding**|['cs.CY', 'econ.GN', 'q-fin.EC', 'K.4.0; H.4.0']|This thesis provides an in-depth exploration of the Decentralized Co-governance Crowdfunding (DCC) Ecosystem, a novel solution addressing prevailing challenges in conventional crowdfunding methods faced by MSMEs and innovative projects. Among the problems it seeks to mitigate are high transaction costs, lack of transparency, fraud, and inefficient resource allocation. Leveraging a comprehensive review of the existing literature on crowdfunding economic activities and blockchain's impact on organizational governance, we propose a transformative socio-economic model based on digital tokens and decentralized co-governance. This ecosystem is marked by a tripartite community structure - the Labor, Capital, and Governance communities - each contributing uniquely to the ecosystem's operation. Our research unfolds the evolution of the DCC ecosystem through distinct phases, offering a novel understanding of socioeconomic dynamics in a decentralized digital world. It also delves into the intricate governance mechanism of the ecosystem, ensuring integrity, fairness, and a balanced distribution of value and wealth. |[2306.00869v1](http://arxiv.org/abs/2306.00869v1)|null|
|**2023-06-01**|**Stochastic Mean-field Theory for Conditional Spin Squeezing by Homodyne Probing of Atom-Cavity Photon Dressed States**|['quant-ph']|A projective measurement on a quantum system prepares an eigenstate of the observable measured. Measurements of collective observables can thus be employed to herald the preparation of entangled states of quantum systems with no mutual interactions. For large quantum systems numerical handling of the conditional quantum state by the density matrix becomes prohibitively complicated, but they may be treated by effective approximate methods. In this article, we present a stochastic variant of cumulant mean-field theory to simulate the effect of continuous optical probing of an atomic ensemble, which can be readily generalized to describe more complex systems, such as ensembles of multi-level systems and hybrid atomic and mechanical systems, and protocols that include adaptive measurements and feedback. We apply the theory to a system with tens of thousands of rubidium-87 atom in an optical cavity, and we study the spin squeezing occurring solely due to homodyne detection of a transmitted light signal near an atom-photon dressed state resonance, cf., a similar application of heterodyne detection to this system [Nat. Photonics, 8(9), 731-736 (2014)]. |[2306.00868v1](http://arxiv.org/abs/2306.00868v1)|null|
|**2023-06-01**|**IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control**|['cs.LG', 'cs.AI']|Model-based reinforcement learning (RL) has shown great promise due to its sample efficiency, but still struggles with long-horizon sparse-reward tasks, especially in offline settings where the agent learns from a fixed dataset. We hypothesize that model-based RL agents struggle in these environments due to a lack of long-term planning capabilities, and that planning in a temporally abstract model of the environment can alleviate this issue. In this paper, we make two key contributions: 1) we introduce an offline model-based RL algorithm, IQL-TD-MPC, that extends the state-of-the-art Temporal Difference Learning for Model Predictive Control (TD-MPC) with Implicit Q-Learning (IQL); 2) we propose to use IQL-TD-MPC as a Manager in a hierarchical setting with any off-the-shelf offline RL algorithm as a Worker. More specifically, we pre-train a temporally abstract IQL-TD-MPC Manager to predict "intent embeddings", which roughly correspond to subgoals, via planning. We empirically show that augmenting state representations with intent embeddings generated by an IQL-TD-MPC manager significantly improves off-the-shelf offline RL agents' performance on some of the most challenging D4RL benchmark tasks. For instance, the offline RL algorithms AWAC, TD3-BC, DT, and CQL all get zero or near-zero normalized evaluation scores on the medium and large antmaze tasks, while our modification gives an average score over 40. |[2306.00867v1](http://arxiv.org/abs/2306.00867v1)|null|
|**2023-06-01**|**Flat F-theory and friends**|['hep-th']|We discuss F-theory backgrounds associated to flat torus bundles over Ricci-flat manifolds. In this setting the F-theory background can be understood as a IIB orientifold with a large radius limit described by a supersymmetric compactification of IIB supergravity on a smooth, Ricci flat, but in general non-spin geometry. When compactified on an additional circle these backgrounds are T-dual to IIA compactifications on smooth non-orientable manifolds with a Pin-minus structure. |[2306.00865v1](http://arxiv.org/abs/2306.00865v1)|null|
|**2023-06-01**|**DeepFake-Adapter: Dual-Level Adapter for DeepFake Detection**|['cs.CV']|Existing deepfake detection methods fail to generalize well to unseen or degraded samples, which can be attributed to the over-fitting of low-level forgery patterns. Here we argue that high-level semantics are also indispensable recipes for generalizable forgery detection. Recently, large pre-trained Vision Transformers (ViTs) have shown promising generalization capability. In this paper, we propose the first parameter-efficient tuning approach for deepfake detection, namely DeepFake-Adapter, to effectively and efficiently adapt the generalizable high-level semantics from large pre-trained ViTs to aid deepfake detection. Given large pre-trained models but limited deepfake data, DeepFake-Adapter introduces lightweight yet dedicated dual-level adapter modules to a ViT while keeping the model backbone frozen. Specifically, to guide the adaptation process to be aware of both global and local forgery cues of deepfake data, 1) we not only insert Globally-aware Bottleneck Adapters in parallel to MLP layers of ViT, 2) but also actively cross-attend Locally-aware Spatial Adapters with features from ViT. Unlike existing deepfake detection methods merely focusing on low-level forgery patterns, the forgery detection process of our model can be regularized by generalizable high-level semantics from a pre-trained ViT and adapted by global and local low-level forgeries of deepfake data. Extensive experiments on several standard deepfake detection benchmarks validate the effectiveness of our approach. Notably, DeepFake-Adapter demonstrates a convincing advantage under cross-dataset and cross-manipulation settings. The source code is released at https://github.com/rshaojimmy/DeepFake-Adapter |[2306.00863v1](http://arxiv.org/abs/2306.00863v1)|**[link](https://github.com/rshaojimmy/deepfake-adapter)**|
|**2023-06-01**|**Non-stationary Reinforcement Learning under General Function Approximation**|['cs.LG', 'stat.ML']|General function approximation is a powerful tool to handle large state and action spaces in a broad range of reinforcement learning (RL) scenarios. However, theoretical understanding of non-stationary MDPs with general function approximation is still limited. In this paper, we make the first such an attempt. We first propose a new complexity metric called dynamic Bellman Eluder (DBE) dimension for non-stationary MDPs, which subsumes majority of existing tractable RL problems in static MDPs as well as non-stationary MDPs. Based on the proposed complexity metric, we propose a novel confidence-set based model-free algorithm called SW-OPEA, which features a sliding window mechanism and a new confidence set design for non-stationary MDPs. We then establish an upper bound on the dynamic regret for the proposed algorithm, and show that SW-OPEA is provably efficient as long as the variation budget is not significantly large. We further demonstrate via examples of non-stationary linear and tabular MDPs that our algorithm performs better in small variation budget scenario than the existing UCB-type algorithms. To the best of our knowledge, this is the first dynamic regret analysis in non-stationary MDPs with general function approximation. |[2306.00861v1](http://arxiv.org/abs/2306.00861v1)|null|
|**2023-06-01**|**Adversarial learning of neural user simulators for dialogue policy optimisation**|['cs.CL']|Reinforcement learning based dialogue policies are typically trained in interaction with a user simulator. To obtain an effective and robust policy, this simulator should generate user behaviour that is both realistic and varied. Current data-driven simulators are trained to accurately model the user behaviour in a dialogue corpus. We propose an alternative method using adversarial learning, with the aim to simulate realistic user behaviour with more variation. We train and evaluate several simulators on a corpus of restaurant search dialogues, and then use them to train dialogue system policies. In policy cross-evaluation experiments we demonstrate that an adversarially trained simulator produces policies with 8.3% higher success rate than those trained with a maximum likelihood simulator. Subjective results from a crowd-sourced dialogue system user evaluation confirm the effectiveness of adversarially training user simulators. |[2306.00858v1](http://arxiv.org/abs/2306.00858v1)|null|
|**2023-06-01**|**Loss-Optimal Classification Trees: A Generalized Framework and the Logistic Case**|['stat.ML', 'cs.LG']|The Classification Tree (CT) is one of the most common models in interpretable machine learning. Although such models are usually built with greedy strategies, in recent years, thanks to remarkable advances in Mixer-Integer Programming (MIP) solvers, several exact formulations of the learning problem have been developed. In this paper, we argue that some of the most relevant ones among these training models can be encapsulated within a general framework, whose instances are shaped by the specification of loss functions and regularizers. Next, we introduce a novel realization of this framework: specifically, we consider the logistic loss, handled in the MIP setting by a linear piece-wise approximation, and couple it with $\ell_1$-regularization terms. The resulting Optimal Logistic Tree model numerically proves to be able to induce trees with enhanced interpretability features and competitive generalization capabilities, compared to the state-of-the-art MIP-based approaches. |[2306.00857v1](http://arxiv.org/abs/2306.00857v1)|null|
|**2023-06-01**|**A deep-learning approach to early identification of suggested sexual harassment from videos**|['cs.CV', 'cs.LG']|Sexual harassment, sexual abuse, and sexual violence are prevalent problems in this day and age. Women's safety is an important issue that needs to be highlighted and addressed. Given this issue, we have studied each of these concerns and the factors that affect it based on images generated from movies. We have classified the three terms (harassment, abuse, and violence) based on the visual attributes present in images depicting these situations. We identified that factors such as facial expression of the victim and perpetrator and unwanted touching had a direct link to identifying the scenes containing sexual harassment, abuse and violence. We also studied and outlined how state-of-the-art explicit content detectors such as Google Cloud Vision API and Clarifai API fail to identify and categorise these images. Based on these definitions and characteristics, we have developed a first-of-its-kind dataset from various Indian movie scenes. These scenes are classified as sexual harassment, sexual abuse, or sexual violence and exported in the PASCAL VOC 1.1 format. Our dataset is annotated on the identified relevant features and can be used to develop and train a deep-learning computer vision model to identify these issues. The dataset is publicly available for research and development. |[2306.00856v1](http://arxiv.org/abs/2306.00856v1)|null|
|**2023-06-01**|**Learning Sampling Dictionaries for Efficient and Generalizable Robot Motion Planning with Transformers**|['cs.RO', 'cs.AI']|Motion planning is integral to robotics applications such as autonomous driving, surgical robots, and industrial manipulators. Existing planning methods lack scalability to higher-dimensional spaces, while recent learning based planners have shown promise in accelerating sampling-based motion planners (SMP) but lack generalizability to out-of-distribution environments. To address this, we present a novel approach, Vector Quantized-Motion Planning Transformers (VQ-MPT) that overcomes the key generalization and scaling drawbacks of previous learning-based methods. VQ-MPT consists of two stages. Stage 1 is a Vector Quantized-Variational AutoEncoder model that learns to represent the planning space using a finite number of sampling distributions, and stage 2 is an Auto-Regressive model that constructs a sampling region for SMPs by selecting from the learned sampling distribution sets. By splitting large planning spaces into discrete sets and selectively choosing the sampling regions, our planner pairs well with out-of-the-box SMPs, generating near-optimal paths faster than without VQ-MPT's aid. It is generalizable in that it can be applied to systems of varying complexities, from 2D planar to 14D bi-manual robots with diverse environment representations, including costmaps and point clouds. Trained VQ-MPT models generalize to environments unseen during training and achieve higher success rates than previous methods. |[2306.00851v1](http://arxiv.org/abs/2306.00851v1)|null|
|**2023-06-01**|**Efficient Approximation of Molecular Kinetics using Random Fourier Features**|['physics.comp-ph', 'math.DS', 'physics.data-an']|Slow kinetic processes of molecular systems can be characterized by computing dominant eigenpairs of the Koopman operator or its generator. In terms of the numerical approximation, the Variational Approach to Markov Processes (VAMP) provides a rigorous way of discerning the quality of different approximate models. Kernel methods have been shown to provide accurate and robust estimates for slow kinetic processes, but are sensitive to hyper-parameter selection, and require the solution of large-scale generalized eigenvalue problems, which can easily become computationally demanding for large data sizes. In this contribution, we employ a stochastic approximation of the kernel based on random Fourier features (RFFs), to derive a small-scale dual eigenvalue problem which can easily be solved. We provide an interpretation of this procedure in terms of a finite randomly generated basis set. By combining the RFF approach and model selection by means of the VAMP score, we show that kernel parameters can be efficiently tuned, and accurate estimates of slow molecular kinetics can be obtained for several benchmarking systems, such as deca alanine and the NTL9 protein. |[2306.00849v1](http://arxiv.org/abs/2306.00849v1)|null|
|**2023-06-01**|**What model does MuZero learn?**|['cs.LG', 'cs.AI']|Model-based reinforcement learning has drawn considerable interest in recent years, given its promise to improve sample efficiency. Moreover, when using deep-learned models, it is potentially possible to learn compact models from complex sensor data. However, the effectiveness of these learned models, particularly their capacity to plan, i.e., to improve the current policy, remains unclear. In this work, we study MuZero, a well-known deep model-based reinforcement learning algorithm, and explore how far it achieves its learning objective of a value-equivalent model and how useful the learned models are for policy improvement. Amongst various other insights, we conclude that the model learned by MuZero cannot effectively generalize to evaluate unseen policies, which limits the extent to which we can additionally improve the current policy by planning with the model. |[2306.00840v1](http://arxiv.org/abs/2306.00840v1)|null|
|**2023-06-01**|**Geometrothermodynamic cosmology**|['gr-qc', 'astro-ph.CO']|We review the main aspects of geometrothermodynamics, a formalism that uses contact geometry and Riemannian geometry to describe the properties of thermodynamic systems. We show how to handle in a geometric way the invariance of classical thermodynamics with respect to Legendre transformations, which means that the properties of the systems do not depend on the choice of the thermodynamic potential. Moreover, we show that in geometrothermodynamics it is possible to apply a variational principle to generate thermodynamic fundamental equations, which can be used in the context of relativistic cosmology to generate cosmological models. As a particular example, we consider a fundamental equation that relates the entropy with the internal energy and the volume of the Universe, and construct cosmological models with arbitrary parameters, which can be fixed to reproduce the main aspects of the inflationary era and the standard cosmological paradigm. |[2306.00839v1](http://arxiv.org/abs/2306.00839v1)|null|
|**2023-06-01**|**Collective modes and quantum effects in two-dimensional nanofluidic channels**|['cond-mat.mes-hall', 'cond-mat.mtrl-sci', 'cond-mat.soft', 'cond-mat.stat-mech']|Nanoscale fluid transport is typically pictured in terms of atomic-scale dynamics, as is natural in the real-space framework of molecular simulations. An alternative Fourier-space picture, that involves the collective charge fluctuation modes of both the liquid and the confining wall, has recently been successful at predicting new nanofluidic phenomena such as quantum friction and near-field heat transfer, that rely on the coupling of those fluctuations. Here, we study the charge fluctuation modes of a two-dimensional (planar) nanofluidic channel. Introducing confined response functions that generalize the notion of surface response function, we show that the channel walls exhibit coupled plasmon modes as soon as the confinement is comparable to the plasmon wavelength. Conversely, the water fluctuations remain remarkably bulk-like, with significant confinement effects arising only when the wall spacing is reduced to 7 A. We apply the confined response formalism to predict the dependence of the solid-water quantum friction and thermal boundary conductance on channel width for model channel wall materials. Our results provide a general framework for Coulomb interactions of fluctuating matter in nanoscale confinement. |[2306.00837v1](http://arxiv.org/abs/2306.00837v1)|null|
|**2023-06-01**|**The Journey Towards 6G: A Digital and Societal Revolution in the Making**|['cs.NI']|While the fifth generation (5G) is bringing an innovative fabric of breakthrough technologies, enabling smart factories, cities, and Internet-of-Things (IoT), the unprecedented strain on communication networks put by these applications, in terms of highly cognitive, agile architectures and the support of massive connectivity, energy efficiency, and extreme ultralow latency, is pushing 5G to their limits. As such, the focus of academic and industrial efforts has shifted toward beyond 5G (B5G) and the conceptualization of sixth generation (6G) systems. This article discusses four main digital and societal use cases (UCs) that will drive the need to reconcile a new breed of network requirements. Based on this, we provide our vision of the fundamental architectural ingredients that will enable the promise of 6G networks of bringing the unification of experiences across the digital, physical, and human worlds. We outline key disruptive technological paradigms that will support 6G materialize a bouquet of unique expectations and redefine how we live and protect our planet. Finally, we adopt the recently envisaged ecosystem of the Internet-of-Musical Things (IoMusT) to depict how the discussed UCs and technological paradigms may be exploited to realize this ecosystem. |[2306.00832v1](http://arxiv.org/abs/2306.00832v1)|null|
|**2023-06-01**|**Jamming pair of general run-and-tumble particles: Exact results and universality classes**|['cond-mat.stat-mech', 'math.PR']|We consider a general system of two run-and-tumble particles interacting by hardcore jamming on the unidimensional torus. RTP are a paradigmatic active matter model, typically modeling the evolution of bacteria. By directly modeling the system at the continuous-space and -time level thanks to piecewise deterministic Markov processes (PDMP), we derive the conservation conditions which sets the invariant distribution and explicitly construct the two universality classes for the steady state, the detailed-jamming and the global-jamming classes. They respectively identify with the preservation or not of a detailed symmetry at the level of the dynamical internal states between probability flows entering and exiting jamming configurations. Thanks to a spectral analysis of the tumble kernel, we give explicit expressions for the invariant measure in the general case. The non-equilibrium features exhibited by the steady state includes positive mass for the jammed configurations and, for the global-jamming class, exponential decay and growth terms, potentially modulated by polynomial terms. Interestingly, we find that the invariant measure follows, away from jamming configurations, a catenary-like constraint, which results from the interplay between probability conservation and the dynamical skewness introduced by the jamming interactions, seen now as a boundary constraint. This work shows the powerful analytical approach PDMP provide for the study of the stationary behaviors of RTP sytems and motivates their future applications to larger systems, with the goal to derive microscopic conditions for motility-induced phase transitions. |[2306.00831v1](http://arxiv.org/abs/2306.00831v1)|null|
|**2023-06-01**|**Zero and Few-shot Semantic Parsing with Ambiguous Inputs**|['cs.CL']|Despite the ubiquity of ambiguity in natural language, it is often ignored or deliberately removed in semantic parsing tasks, which generally assume that a given surface form has only one correct logical form. We attempt to address this shortcoming by introducing AmP, a framework, dataset, and challenge for parsing with linguistic ambiguity. We define templates and generate data for five well-documented linguistic ambiguities. Using AmP, we investigate how several few-shot semantic parsing systems handle ambiguity, introducing three new metrics. We find that large pre-trained models perform poorly at capturing the distribution of possible meanings without deliberate instruction. However, models are able to capture distribution well when ambiguity is attested in their inputs. These results motivate a call for ambiguity to be explicitly included in semantic parsing, and promotes considering the distribution of possible outputs when evaluating semantic parsing systems. |[2306.00824v1](http://arxiv.org/abs/2306.00824v1)|**[link](https://github.com/esteng/ambiguous_parsing)**|
|**2023-06-01**|**Geo-Tiles for Semantic Segmentation of Earth Observation Imagery**|['cs.CV', 'eess.IV']|To cope with the high requirements during the computation of semantic segmentations of earth observation imagery, current state-of-the-art pipelines divide the corresponding data into smaller images. Existing methods and benchmark datasets oftentimes rely on pixel-based tiling schemes or on geo-tiling schemes employed by web mapping applications. The selection of the subimages (comprising size, location and orientation) is crucial since it affects the available context information of each pixel, defines the number of tiles during training, and influences the degree of information degradation while down- and up-sampling the tile contents to the size required by the segmentation model. In this paper we propose a new segmentation pipeline for earth observation imagery relying on a tiling scheme that creates geo-tiles based on the geo-information of the raster data. This approach exhibits several beneficial properties compared to pixel-based or common web mapping approaches. For instance, the proposed tiling scheme shows flexible customization properties regarding tile granularity, tile stride and image boundary alignment, which allows us to perform a tile specific data augmentation during training and a substitution of pixel predictions with limited context information using data of overlapping tiles during inference. Furthermore, the generated tiles show a consistent spatial tile extent w.r.t. heterogeneous sensors, varying recording distances and different latitudes. In our experiments we demonstrate how the proposed tiling system allows to improve the results of current state-of-the-art semantic segmentation models. To foster future research we make the source code publicly available. |[2306.00823v1](http://arxiv.org/abs/2306.00823v1)|**[link](https://github.com/SBCV/EarthObservationTiles)**|
|**2023-06-01**|**Far infrared to terahertz widely tunable narrow linewidth light source via surface-emitting periodically poled thin film lithium niobate waveguides**|['physics.optics']|Generating widely tunable, continuous wave light at long wavelengths via difference frequency generation (DFG) remains challenging due to high absorption and dispersion. The relatively new platform of thin film lithium niobate enables high-confinement nonlinear waveguides, which could improve efficiency. We simulated DFG in thin film lithium niobate waveguides that are periodically poled for surface emission at 30 THz. Maximum efficiency for a 1 cm device is 9.16 $\times$ 10$^{-6}$ W$^{-1}$ assuming $d_{33}$ = 30 pm/V. The tuning range within 50$\%$ of efficiency at 30 THz is as wide as 25 THz, from 25 THz to 50 THz. |[2306.00819v1](http://arxiv.org/abs/2306.00819v1)|null|
|**2023-06-01**|**Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers**|['cs.CV', 'cs.CR']|Deep neural networks (DNNs) can be manipulated to exhibit specific behaviors when exposed to specific trigger patterns, without affecting their performance on normal samples. This type of attack is known as a backdoor attack. Recent research has focused on designing invisible triggers for backdoor attacks to ensure visual stealthiness. These triggers have demonstrated strong attack performance even under backdoor defense, which aims to eliminate or suppress the backdoor effect in the model. However, through experimental observations, we have noticed that these carefully designed invisible triggers are often susceptible to visual distortion during inference, such as Gaussian blurring or environmental variations in real-world scenarios. This phenomenon significantly undermines the effectiveness of attacks in practical applications. Unfortunately, this issue has not received sufficient attention and has not been thoroughly investigated. To address this limitation, we propose a novel approach called the Visible, Semantic, Sample-Specific, and Compatible trigger (VSSC-trigger), which leverages a recent powerful image method known as the stable diffusion model. In this approach, a text trigger is utilized as a prompt and combined with a benign image. The resulting combination is then processed by a pre-trained stable diffusion model, generating a corresponding semantic object. This object is seamlessly integrated with the original image, resulting in a new realistic image, referred to as the poisoned image. Extensive experimental results and analysis validate the effectiveness and robustness of our proposed attack method, even in the presence of visual distortion. We believe that the new trigger proposed in this work, along with the proposed idea to address the aforementioned issues, will have significant prospective implications for further advancements in this direction. |[2306.00816v1](http://arxiv.org/abs/2306.00816v1)|null|
|**2023-06-01**|**Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis**|['cs.SD', 'cs.LG', 'eess.AS']|Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that addresses the key challenges of modeling spectral coefficients. Vocos demonstrates improved computational efficiency, achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches. As shown by objective evaluation, Vocos not only matches state-of-the-art audio quality, but thanks to frequency-aware generator, also effectively mitigates the periodicity issues frequently associated with time-domain GANs. The source code and model weights have been open-sourced at https://github.com/charactr-platform/vocos. |[2306.00814v1](http://arxiv.org/abs/2306.00814v1)|**[link](https://github.com/charactr-platform/vocos)**|
|**2023-06-01**|**UniDiff: Advancing Vision-Language Models with Generative and Discriminative Learning**|['cs.CV']|Recent advances in vision-language pre-training have enabled machines to perform better in multimodal object discrimination (e.g., image-text semantic alignment) and image synthesis (e.g., text-to-image generation). On the other hand, fine-tuning pre-trained models with discriminative or generative capabilities such as CLIP and Stable Diffusion on domain-specific datasets has shown to be effective in various tasks by adapting to specific domains. However, few studies have explored the possibility of learning both discriminative and generative capabilities and leveraging their synergistic effects to create a powerful and personalized multimodal model during fine-tuning. This paper presents UniDiff, a unified multi-modal model that integrates image-text contrastive learning (ITC), text-conditioned image synthesis learning (IS), and reciprocal semantic consistency modeling (RSC). UniDiff effectively learns aligned semantics and mitigates the issue of semantic collapse during fine-tuning on small datasets by leveraging RSC on visual features from CLIP and diffusion models, without altering the pre-trained model's basic architecture. UniDiff demonstrates versatility in both multi-modal understanding and generative tasks. Experimental results on three datasets (Fashion-man, Fashion-woman, and E-commercial Product) showcase substantial enhancements in vision-language retrieval and text-to-image generation, illustrating the advantages of combining discriminative and generative fine-tuning. The proposed UniDiff model establishes a robust pipeline for personalized modeling and serves as a benchmark for future comparisons in the field. |[2306.00813v1](http://arxiv.org/abs/2306.00813v1)|null|
|**2023-06-01**|**The observed power spectrum & frequency-angular power spectrum**|['astro-ph.CO']|The two-point summary statistics is one of the most commonly used tools in the study of cosmological structure. Starting from the theoretical power spectrum defined in the 3D volume and obtained via the process of ensemble averaging, we establish the construction of the observed 3D power spectrum, folding the unequal-time information around the average position into the wave modes along the line of sight. We show how these unequal-time cross-correlation effects give rise to scale-dependent corrections in the observable 3D power spectrum. We also introduce a new dimensionless observable, the frequency-angular power spectrum, which is a function of dimensionless and directly observable quantities corresponding to Fourier counterparts of angles and redshifts. While inheriting many useful characteristics of the canonical observed power spectrum, this newly introduced statistic does not depend on physical distances and is hence free of so-called Alcock-Paczynski effects. Such observable thus presents a clear advantage and simplification over the traditional power spectrum. Moreover, relying on linear theory calculations, we estimate that unequal-time corrections, while generally small, can amount to a few percent on large scales and high redshifts. Interestingly, such corrections depend on the bias of the tracers, the growth rate, but also their time derivatives, opening up the possibility of new tests of cosmological models. These radial mode effects also introduce anisotropies in the observed power spectrum, in addition to the ones arising from redshift-space distortions, generating non-vanishing odd multiples and imaginary contributions. Lastly, we investigate the effects of unequal-time corrections in resumming long displacements (IR-resummation) of the observed power spectrum. |[2306.00808v1](http://arxiv.org/abs/2306.00808v1)|null|
|**2023-06-01**|**Gravitational collapse of matter in the presence of Quintessence and Phantom-like scalar fields**|['gr-qc']|In this work, we propose a model of the gravitational collapse of dark matter in the presence of quintessence or phantom-like scalar fields. Our treatment is based on the principles of general relativity up to virialization. We have chosen a spherical patch that starts to collapse gravitationally as it happens in top-hat collapse. It is seen that although the dark matter sector collapses the dark energy sector does keep a profile that is almost similar to the dark energy profile for the background expanding Friedmann-Lemaitre-Robertson-Walker (FLRW) universe for suitable model parameters. It is observed that in order to formulate the problem in the general relativistic setting one has to abandon the idea of a closed FLRW isolated collapsing patch. General relativity requires an external generalized Vaidya spacetime to be matched with the internal spherical patch whose dynamics is guided by the FLRW metric. It is shown that almost all collapses are accompanied by some flux of matter and radiation in the generalized Vaidya spacetime. Some of the spherical regions of the universe are seen not to collapse but expand eternally, producing void-like structures. Whether a spherical region will collapse or expand depends upon the initial values of the system and other model parameters. As this work shows that collapsing structures must emit some form of radiation, this may be taken as an observational signature of our proposal. |[2306.00805v1](http://arxiv.org/abs/2306.00805v1)|null|
|**2023-06-01**|**Birth of a Transformer: A Memory Viewpoint**|['stat.ML', 'cs.CL', 'cs.LG']|Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an "induction head" mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties. |[2306.00802v1](http://arxiv.org/abs/2306.00802v1)|null|
|**2023-06-01**|**FigGen: Text to Scientific Figure Generation**|['cs.CV', 'cs.AI']|The generative modeling landscape has experienced tremendous growth in recent years, particularly in generating natural images and art. Recent techniques have shown impressive potential in creating complex visual compositions while delivering impressive realism and quality. However, state-of-the-art methods have been focusing on the narrow domain of natural images, while other distributions remain unexplored. In this paper, we introduce the problem of text-to-figure generation, that is creating scientific figures of papers from text descriptions. We present FigGen, a diffusion-based approach for text-to-figure as well as the main challenges of the proposed task. Code and models are available at https://github.com/joanrod/figure-diffusion |[2306.00800v1](http://arxiv.org/abs/2306.00800v1)|null|
|**2023-06-01**|**Microstructure quality control of steels using deep learning**|['cond-mat.mtrl-sci', 'cs.AI']|In quality control, microstructures are investigated rigorously to ensure structural integrity, exclude the presence of critical volume defects, and validate the formation of the target microstructure. For quenched, hierarchically-structured steels, the morphology of the bainitic and martensitic microstructures are of major concern to guarantee the reliability of the material under service conditions. Therefore, industries conduct small sample-size inspections of materials cross-sections through metallographers to validate the needle morphology of such microstructures. We demonstrate round-robin test results revealing that this visual grading is afflicted by pronounced subjectivity despite the thorough training of personnel. Instead, we propose a deep learning image classification approach that distinguishes steels based on their microstructure type and classifies their needle length alluding to the ISO 643 grain size assessment standard. This classification approach facilitates the reliable, objective, and automated classification of hierarchically structured steels. Specifically, an accuracy of 96% and roughly 91% is attained for the distinction of martensite/bainite subtypes and needle length, respectively. This is achieved on an image dataset that contains significant variance and labeling noise as it is acquired over more than ten years from multiple plants, alloys, etchant applications, and light optical microscopes by many metallographers (raters). Interpretability analysis gives insights into the decision-making of these models and allows for estimating their generalization capability. |[2306.00797v1](http://arxiv.org/abs/2306.00797v1)|null|
|**2023-06-01**|**Numerical Simulations of the Two-phase flow and Fluid-Structure Interaction Problems with Adaptive Mesh Refinement**|['physics.flu-dyn']|Numerical simulations of two-phase flow and fluid structure interaction problems are of great interest in many environmental problems and engineering applications. To capture the complex physical processes involved in these problems, a high grid resolution is usually needed. However, one does not need or maybe cannot afford a fine grid of uniformly high resolution across the whole domain. The need to resolve local fine features can be addressed by the adaptive mesh refinement (AMR) method, which increases the grid resolution in regions of interest as needed during the simulation while leaving general estimates in other regions.   In this work, we propose a block-structured adaptive mesh refinement (BSAMR) framework to simulate two-phase flows using the level set (LS) function with both the subcycling and non-subcycling methods on a collocated grid. To the best of our knowledge, this is the first framework that unifies the subcycling and non-subcycling methods to simulate two-phase flows. The use of the collocated grid is also the first among the two-phase BSAMR framework, which significantly simplifies the implementation of multi-level differential operators and interpolation schemes. We design the synchronization operations, including the averaging, refluxing, and synchronization projection, which ensures that the flow field is divergence-free on the multi-level grid. It is shown that the present multi-level scheme can accurately resolve the interfaces of the two-phase flows with gravitational and surface tension effects while having good momentum and energy conservation. |[2306.00796v1](http://arxiv.org/abs/2306.00796v1)|null|
|**2023-06-01**|**Fermonic anyons: entanglement and quantum computation from a resource-theoretic perspective**|['quant-ph']|Often quantum computational models can be understood via the lens of resource theories, where a computational advantage is achieved by consuming specific forms of quantum resources and, conversely, resource-free computations are classically simulable. For example, circuits of nearest-neighbor matchgates can be mapped to free-fermion dynamics, which can be simulated classically. Supplementing these circuits with nonmatchgate operations or non-gaussian fermionic states, respectively, makes them quantum universal. Can we similarly identify quantum computational resources in the setting of more general quasi-particle statistics, such as that of fermionic anyons? In this work, we develop a resource-theoretic framework to define and investigate the separability of fermionic anyons. We build the notion of separability through a fractional Jordan-Wigner transformation, leading to a Schmidt decomposition for fermionic-anyon states. We show that this notion of fermionic-anyon separability, and the unitary operations that preserve it, can be mapped to the free resources of matchgate circuits. We also identify how entanglement between two qubits encoded in a dual-rail manner, as standard for matchgate circuits, corresponds to the notion of entanglement between fermionic anyons. Though this does not coincide with the usual definition of qubit entanglement, it provides new insight into the limited capabilities of matchgate circuits. |[2306.00795v1](http://arxiv.org/abs/2306.00795v1)|null|
|**2023-06-01**|**SlothSpeech: Denial-of-service Attack Against Speech Recognition Models**|['cs.SD', 'cs.CR', 'cs.LG', 'eess.AS']|Deep Learning (DL) models have been popular nowadays to execute different speech-related tasks, including automatic speech recognition (ASR). As ASR is being used in different real-time scenarios, it is important that the ASR model remains efficient against minor perturbations to the input. Hence, evaluating efficiency robustness of the ASR model is the need of the hour. We show that popular ASR models like Speech2Text model and Whisper model have dynamic computation based on different inputs, causing dynamic efficiency. In this work, we propose SlothSpeech, a denial-of-service attack against ASR models, which exploits the dynamic behaviour of the model. SlothSpeech uses the probability distribution of the output text tokens to generate perturbations to the audio such that efficiency of the ASR model is decreased. We find that SlothSpeech generated inputs can increase the latency up to 40X times the latency induced by benign input. |[2306.00794v1](http://arxiv.org/abs/2306.00794v1)|**[link](https://github.com/0xrutvij/slothspeech)**|
|**2023-06-01**|**Inverse problems of identifying the time-dependent source coefficient for subelliptic heat equations**|['math.AP']|We discuss inverse problems of determining the time-dependent source coefficient for a general class of subelliptic heat equations. We show that a single data at an observation point guarantees the existence of a (smooth) solution pair for the inverse problem. Moreover, additional data at the observation point implies an explicit formula for the time-dependent source coefficient. We also explore an inverse problem with nonlocal additional data, which seems a new approach even in the Laplacian case. |[2306.00786v1](http://arxiv.org/abs/2306.00786v1)|null|
|**2023-06-01**|**Data Interpolants -- That's What Discriminators in Higher-order Gradient-regularized GANs Are**|['stat.ML', 'cs.LG']|We consider the problem of optimizing the discriminator in generative adversarial networks (GANs) subject to higher-order gradient regularization. We show analytically, via the least-squares (LSGAN) and Wasserstein (WGAN) GAN variants, that the discriminator optimization problem is one of interpolation in $n$-dimensions. The optimal discriminator, derived using variational Calculus, turns out to be the solution to a partial differential equation involving the iterated Laplacian or the polyharmonic operator. The solution is implementable in closed-form via polyharmonic radial basis function (RBF) interpolation. In view of the polyharmonic connection, we refer to the corresponding GANs as Poly-LSGAN and Poly-WGAN. Through experimental validation on multivariate Gaussians, we show that implementing the optimal RBF discriminator in closed-form, with penalty orders $m \approx\lceil \frac{n}{2} \rceil $, results in superior performance, compared to training GAN with arbitrarily chosen discriminator architectures. We employ the Poly-WGAN discriminator to model the latent space distribution of the data with encoder-decoder-based GAN flavors such as Wasserstein autoencoders. |[2306.00785v1](http://arxiv.org/abs/2306.00785v1)|null|
|**2023-06-01**|**Interpretable Math Word Problem Solution Generation Via Step-by-step Planning**|['cs.CL', 'cs.AI']|Solutions to math word problems (MWPs) with step-by-step explanations are valuable, especially in education, to help students better comprehend problem-solving strategies. Most existing approaches only focus on obtaining the final correct answer. A few recent approaches leverage intermediate solution steps to improve final answer correctness but often cannot generate coherent steps with a clear solution strategy. Contrary to existing work, we focus on improving the correctness and coherence of the intermediate solutions steps. We propose a step-by-step planning approach for intermediate solution generation, which strategically plans the generation of the next solution step based on the MWP and the previous solution steps. Our approach first plans the next step by predicting the necessary math operation needed to proceed, given history steps, then generates the next step, token-by-token, by prompting a language model with the predicted math operation. Experiments on the GSM8K dataset demonstrate that our approach improves the accuracy and interpretability of the solution on both automatic metrics and human evaluation. |[2306.00784v1](http://arxiv.org/abs/2306.00784v1)|null|
|**2023-06-01**|**FDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models**|['cs.CV']|The ability to create high-quality 3D faces from a single image has become increasingly important with wide applications in video conferencing, AR/VR, and advanced video editing in movie industries. In this paper, we propose Face Diffusion NeRF (FDNeRF), a new generative method to reconstruct high-quality Face NeRFs from single images, complete with semantic editing and relighting capabilities. FDNeRF utilizes high-resolution 3D GAN inversion and expertly trained 2D latent-diffusion model, allowing users to manipulate and construct Face NeRFs in zero-shot learning without the need for explicit 3D data. With carefully designed illumination and identity preserving loss, as well as multi-modal pre-training, FD-NeRF offers users unparalleled control over the editing process enabling them to create and edit face NeRFs using just single-view images, text prompts, and explicit target lighting. The advanced features of FDNeRF have been designed to produce more impressive results than existing 2D editing approaches that rely on 2D segmentation maps for editable attributes. Experiments show that our FDNeRF achieves exceptionally realistic results and unprecedented flexibility in editing compared with state-of-the-art 3D face reconstruction and editing methods. Our code will be available at https://github.com/BillyXYB/FDNeRF. |[2306.00783v1](http://arxiv.org/abs/2306.00783v1)|**[link](https://github.com/billyxyb/fdnerf)**|
|**2023-06-01**|**Non-perturbative theory of spontaneous parametric down-conversion in open and dispersive optical systems**|['quant-ph']|We develop a non-perturbative formulation based on the Green-function quantization method, that can describe spontaneous parametric down-conversion in the high-gain regime in nonlinear optical structures with arbitrary amount of loss and dispersion. This formalism opens the way for description and design of arbitrary complex and/or open nanostructured nonlinear optical systems in quantum technology applications, such as squeezed-light generation, nonlinearity-based quantum sensing, and hybrid quantum systems mediated by nonlinear interactions. As an example case, we numerically investigate the scenario of integrated quantum spectroscopy with undetected photons, in the high-gain regime, and uncover novel gain-dependent effects in the performance of the system. |[2306.00781v1](http://arxiv.org/abs/2306.00781v1)|null|
|**2023-06-01**|**Percolation and topological properties of temporal higher-order networks**|['physics.soc-ph']|Many time-varying networks exhibit non-pairwise interactions that cannot be effectively captured by traditional graph models. Here, we propose a hidden variables formalism to analytically characterize higher-order temporal networks. We apply our framework to a higher-order activity-driven model, providing analytical expressions for the main topological properties of the time-integrated hypergraphs, depending on the integration time and the activity distributions characterizing the model. Furthermore, we provide analytical estimates for the percolation times of general classes of uncorrelated and correlated hypergraphs. Finally, we quantify the extent to which the percolation threshold of empirical social interactions is underestimated when their higher-order nature is neglected. |[2306.00779v1](http://arxiv.org/abs/2306.00779v1)|null|
|**2023-06-01**|**Object pop-up: Can we infer 3D objects and their poses from human interactions alone?**|['cs.CV']|The intimate entanglement between objects affordances and human poses is of large interest, among others, for behavioural sciences, cognitive psychology, and Computer Vision communities. In recent years, the latter has developed several object-centric approaches: starting from items, learning pipelines synthesizing human poses and dynamics in a realistic way, satisfying both geometrical and functional expectations. However, the inverse perspective is significantly less explored: Can we infer 3D objects and their poses from human interactions alone? Our investigation follows this direction, showing that a generic 3D human point cloud is enough to pop up an unobserved object, even when the user is just imitating a functionality (e.g., looking through a binocular) without involving a tangible counterpart. We validate our method qualitatively and quantitatively, with synthetic data and sequences acquired for the task, showing applicability for XR/VR. The code is available at https://github.com/ptrvilya/object-popup. |[2306.00777v1](http://arxiv.org/abs/2306.00777v1)|**[link](https://github.com/ptrvilya/object-popup)**|
|**2023-06-01**|**In-Context Learning User Simulators for Task-Oriented Dialog Systems**|['cs.CL', 'cs.LG']|This paper presents a novel application of large language models in user simulation for task-oriented dialog systems, specifically focusing on an in-context learning approach. By harnessing the power of these models, the proposed approach generates diverse utterances based on user goals and limited dialog examples. Unlike traditional simulators, this method eliminates the need for labor-intensive rule definition or extensive annotated data, making it more efficient and accessible. Additionally, an error analysis of the interaction between the user simulator and dialog system uncovers common mistakes, providing valuable insights into areas that require improvement. Our implementation is available at https://github.com/telepathylabsai/prompt-based-user-simulator. |[2306.00774v1](http://arxiv.org/abs/2306.00774v1)|null|
|**2023-06-01**|**Neutrino mass and nature through its mediation in atomic clock interference**|['physics.atom-ph', 'hep-ph']|The absolute mass of neutrinos and their nature are presently unknown. Aggregate matter has a coherent weak charge leading to a repulsive interaction mediated by a neutrino pair. Near its range at micron distances the virtual neutrinos are non-relativistic, giving a distinct behavior for Dirac versus Majorana mass terms. The magnitude and the distance dependence of the effective potential disentangle these fundamental properties of neutrinos. We propose an experiment to search for this potential based on the concept that the density dependent interaction of an atomic probe with a material source in one arm of an atomic clock interferometer generates a differential phase. The appropriate geometry of the device is selected using the saturation of the weak potential as a guide. The proposed experiment has the added benefit of being sensitive to gravity at micron distances. A strategy to suppress the competing Casimir-Polder interaction, depending on the electronic structure of the material source, as well as a way to compensate the gravitational interaction in the two arms of the interferometer is discussed. |[2306.00767v1](http://arxiv.org/abs/2306.00767v1)|null|
|**2023-06-01**|**New and improved software for data processing at HartRAO**|['astro-ph.IM']|The Hartebeesthoek Radio Astronomy Observatory (HartRAO) has been processing its data using LINES, a Fortran-based program developed in 1989. However, due to the lack of adequate software updates over recent years, the program has become difficult to work with, sighting problems ranging from compatibility issues with newer operating systems to maintenance issues from using a generally unfamiliar programming language. This work presents DRAN, a new software package for the reduction and analysis of HartRAO single-dish continuum data. DRANs main functionality is based on LINES, however, it was developed using Python and offers a variety of advanced features that include automated data flagging, outlier detection, flux calibration, and time-series analysis which were not previously available in LINES. The objective of this project was to produce a standard user friendly software package for the observatory that produces timeously calibrated data, drift scan images, and supporting documentation for users of HartRAO continuum data. |[2306.00764v1](http://arxiv.org/abs/2306.00764v1)|null|
|**2023-06-01**|**Learning Disentangled Prompts for Compositional Image Synthesis**|['cs.CV', 'cs.AI']|We study domain-adaptive image synthesis, the problem of teaching pretrained image generative models a new style or concept from as few as one image to synthesize novel images, to better understand the compositional image synthesis. We present a framework that leverages a pretrained class-conditional generation model and visual prompt tuning. Specifically, we propose a novel source class distilled visual prompt that learns disentangled prompts of semantic (e.g., class) and domain (e.g., style) from a few images. Learned domain prompt is then used to synthesize images of any classes in the style of target domain. We conduct studies on various target domains with the number of images ranging from one to a few to many, and show qualitative results which show the compositional generalization of our method. Moreover, we show that our method can help improve zero-shot domain adaptation classification accuracy. |[2306.00763v1](http://arxiv.org/abs/2306.00763v1)|null|
|**2023-06-01**|**Inference and Sampling of Point Processes from Diffusion Excursions**|['stat.CO', 'stat.ME', 'stat.ML']|Point processes often have a natural interpretation with respect to a continuous process. We propose a point process construction that describes arrival time observations in terms of the state of a latent diffusion process. In this framework, we relate the return times of a diffusion in a continuous path space to new arrivals of the point process. This leads to a continuous sample path that is used to describe the underlying mechanism generating the arrival distribution. These models arise in many disciplines, such as financial settings where actions in a market are determined by a hidden continuous price or in neuroscience where a latent stimulus generates spike trains. Based on the developments in It\^o's excursion theory, we propose methods for inferring and sampling from the point process derived from the latent diffusion process. We illustrate the approach with numerical examples using both simulated and real data. The proposed methods and framework provide a basis for interpreting point processes through the lens of diffusions. |[2306.00762v1](http://arxiv.org/abs/2306.00762v1)|null|
|**2023-06-01**|**Efficient Failure Pattern Identification of Predictive Algorithms**|['cs.LG']|Given a (machine learning) classifier and a collection of unlabeled data, how can we efficiently identify misclassification patterns presented in this dataset? To address this problem, we propose a human-machine collaborative framework that consists of a team of human annotators and a sequential recommendation algorithm. The recommendation algorithm is conceptualized as a stochastic sampler that, in each round, queries the annotators a subset of samples for their true labels and obtains the feedback information on whether the samples are misclassified. The sampling mechanism needs to balance between discovering new patterns of misclassification (exploration) and confirming the potential patterns of classification (exploitation). We construct a determinantal point process, whose intensity balances the exploration-exploitation trade-off through the weighted update of the posterior at each round to form the generator of the stochastic sampler. The numerical results empirically demonstrate the competitive performance of our framework on multiple datasets at various signal-to-noise ratios. |[2306.00760v1](http://arxiv.org/abs/2306.00760v1)|**[link](https://github.com/nguyenngocbaocmt02/fpd)**|
|**2023-06-01**|**AI Chain on Large Language Model for Unsupervised Control Flow Graph Generation for Statically-Typed Partial Code**|['cs.SE']|Control Flow Graphs (CFGs) are essential for visualizing, understanding and analyzing program behavior. For statically-typed programming language like Java, developers obtain CFGs by using bytecode-based methods for compilable code and Abstract Syntax Tree (AST)-based methods for partially uncompilable code. However, explicit syntax errors during AST construction and implicit semantic errors caused by bad coding practices can lead to behavioral loss and deviation of CFGs.To address the issue, we propose a novel approach that leverages the error-tolerant and understanding ability of pre-trained Large Language Models (LLMs) to generate CFGs. Our approach involves a Chain of Thought (CoT) with four steps: structure hierarchy extraction, nested code block extraction, CFG generation of nested code blocks, and fusion of all nested code blocks' CFGs. To address the limitations of the original CoT's single-prompt approach (i.e., completing all steps in a single generative pass), which can result in an ``epic'' prompt with hard-to-control behavior and error accumulation, we break down the CoT into an AI chain with explicit sub-steps. Each sub-step corresponds to a separate AI-unit, with an effective prompt assigned to each unit for interacting with LLMs to accomplish a specific purpose.Our experiments confirmed that our method outperforms existing CFG tools in terms of node and edge coverage, especially for incomplete or erroneous code. We also conducted an ablation experiment and confirmed the effectiveness of AI chain design principles: Hierarchical Task Breakdown, Unit Composition, and Mix of AI Units and Non-AI Units.Our work opens up new possibilities for building foundational software engineering tools based on LLMs, as opposed to traditional program analysis methods. |[2306.00757v1](http://arxiv.org/abs/2306.00757v1)|null|
|**2023-06-01**|**Differentiable Tree Operations Promote Compositional Generalization**|['cs.CL', 'cs.LG']|In the context of structure-to-structure transformation tasks, learning sequences of discrete symbolic operations poses significant challenges due to their non-differentiability. To facilitate the learning of these symbolic sequences, we introduce a differentiable tree interpreter that compiles high-level symbolic tree operations into subsymbolic matrix operations on tensors. We present a novel Differentiable Tree Machine (DTM) architecture that integrates our interpreter with an external memory and an agent that learns to sequentially select tree operations to execute the target transformation in an end-to-end manner. With respect to out-of-distribution compositional generalization on synthetic semantic parsing and language generation tasks, DTM achieves 100% while existing baselines such as Transformer, Tree Transformer, LSTM, and Tree2Tree LSTM achieve less than 30%. DTM remains highly interpretable in addition to its perfect performance. |[2306.00751v1](http://arxiv.org/abs/2306.00751v1)|null|
|**2023-06-01**|**Low-complexity approximations for sets defined by generalizations of affine conditions**|['math.CO', 'math.GR', 'math.NT']|Let $p$ be a prime, let $S$ be a non-empty subset of $\mathbb{F}_p$ and let $0<\epsilon\leq 1$. We show that there exists a constant $C=C(p, \epsilon)$ such that for every positive integer $k$, whenever $\phi_1, \dots, \phi_k: \mathbb{F}_p^n \rightarrow \mathbb{F}_p$ are linear forms and $E_1, \dots, E_k$ are subsets of $\mathbb{F}_p$, there exist linear forms $\psi_1, \dots, \psi_C: \mathbb{F}_p^n \rightarrow \mathbb{F}_p$ and subsets $F_1, \dots, F_C$ of $\mathbb{F}_p$ such that the set $U=\{x \in S^n: \psi_1(x) \in F_1, \dots, \psi_C(x) \in F_C\}$ is contained inside the set $V=\{x \in S^n: \phi_1(x) \in E_1, \dots, \phi_k(x) \in E_k\}$, and the difference $V \setminus U$ has density at most $\epsilon$ inside $S^n$. We then generalize this result to one where $\phi_1, \dots, \phi_k$ are replaced by homomorphisms $G^n \to H$ for some pair of finite Abelian groups $G$ and $H$, and to another where they are replaced by polynomial maps $\mathbb{F}_p^n \to \mathbb{F}_p$ of small degree. |[2306.00747v1](http://arxiv.org/abs/2306.00747v1)|null|

<p align=right>(<a href=#Updated-on-20230603>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/bansky-cl/diffusion-nlp-paper-arxiv.svg?style=for-the-badge
[contributors-url]: https://github.com/bansky-cl/diffusion-nlp-paper-arxiv/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/bansky-cl/diffusion-nlp-paper-arxiv.svg?style=for-the-badge
[forks-url]: https://github.com/bansky-cl/diffusion-nlp-paper-arxiv/network/members
[stars-shield]: https://img.shields.io/github/stars/bansky-cl/diffusion-nlp-paper-arxiv.svg?style=for-the-badge
[stars-url]: https://github.com/bansky-cl/diffusion-nlp-paper-arxiv/stargazers
[issues-shield]: https://img.shields.io/github/issues/bansky-cl/diffusion-nlp-paper-arxiv.svg?style=for-the-badge
[issues-url]: https://github.com/bansky-cl/diffusion-nlp-paper-arxiv/issues

